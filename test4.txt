OPERATING SYSTEMS 
THREE EASY PIECES 
REMZI H. ARPACI-DUSSEAU ANDREA C. ARPACI-DUSSEAU UNIVERSITY OF WISCONSIN–MADISON 

To Vedat S. Arpaci, a lifelong inspiration 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 


Preface 
To Everyone 
Welcome to this book! We hope you’ll enjoy reading it as much aswe enjoyed writing it. The book is called Operating Systems: Three Easy Pieces, and the title is obviously an homage to one of the greatest setsof lecture notes ever created, by one Richard Feynman on the topic of Physics [F96]. While this book will undoubtedly fall short of the high standard setby thatfamous physicist, perhaps it will be good enough for you in your questto understand what operating systems (and more generally, systems) are allabout. 
The three easy pieces refer to the three major thematic elements the book is organized around: virtualization, concurrency,and persistence.In dis­cussing these concepts, we’ll end up discussing most of the important things an operating system does; hopefully, you’ll also have some fun along the way. Learning new things is fun, right? At least, it should be. 
Each major concept is divided into a set of chapters, most of which present aparticular problem and then show how to solve it. The chapters are short, and try (as best as possible) to reference the source materialwhere the ideas really came from. One of our goals in writing this book is to make the paths of history as clear as possible, as we think that helps a student understand what is, what was, and what will be more clearly. In this case, seeing how the sausage was made is nearly as important as understanding whatthe sausage is good for1. 
There are a couple devices we use throughout the book which areproba­bly worth introducing here. The .rst is the crux of the problem. Anytime we are trying to solve a problem, we .rst try to state what the mostimportant issue is; such a crux of the problem is explicitly called out in the text, and hopefully solved via the techniques, algorithms, and ideas presented in the rest of the text. 
1
Hint: eating! Or if you’re a vegetarian, running away from. 
iii 
We also use one of the oldest didactic methods, the dialogue,throughout the book, as a way of presenting some of the material in a different light. These are used to introduce the major thematic concepts (in a peachy way, as we will see), as well as to review material every now and then. They are also achance to write in amore humorous style, which we greatly enjoy. Whether you enjoy it, well, that’s another matter entirely. 
At the beginning of each major section, we’ll .rst present an abstraction that an operating system provides, and then work in subsequent chapters on the mechanisms, policies, and other support needed to provide the abstrac­tion. Abstractions are fundamental to all aspects of Computer Science, so it is perhaps no surprise that they are also essential in operatingsystems. 
Throughout the chapters, we try to use real code (not pseudocode)where possible, so for virtually all examples, you should be able totype them up yourself and run them. Running real code on real systems is thebest way to learn about operating systems, so we encourage you to do as much of this as possible. 
In various parts of the text, we have sprinkled in a few homeworks to en­sure that you are understanding what is going on. These homeworks are usu­ally little simulations of various pieces of an operating system; you should download the homeworks, and run them to quiz yourself. The homework simulators have the following feature: by giving them a different random seed, you can generate a virtually in.nite set of problems; the simulators can also be told to solve the problems for you. Thus, you can test and re-test yourself until you have achieved a good level of understanding. 
The most important addendum to this book is a set of projects in which you learn about how real systems work by designing, implementing, and testing your own code. All projects (as well as the code examples, mentioned above) are in the Cprogramming language [KR88]; C is a simple and power­ful language that underlies most operating systems, and is thus worth adding to your tool-chest of languages with which you are familiar. Two types of projects are available (ideas for which are provided in the appendix). The .rst type is based on what you would call systems programming;these projects are great for those who are new to C and Unix and want to learn howto do low-level C programming on such systems. The second type is based on a real operating system kernel developed at MIT called xv6 [CK+08];these projects are great for students that already have some C background andreally want to get their hands dirty inside the operating system. At Wisconsin, we’ve run the course in three different ways: either all systems programming, all xv6 programming, or a mix of both. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

To Educators 
If you are an instructor or professor who wishes to use these notes, please feel free to do so. As you may have noticed, they are free and available on-line from the following web page: 
http://www.cs.wisc.edu/˜remzi/OSTEP 
At some point, you will be able to purchase a printed copy from aself­publishing site such as lulu.com.Look for it soon! Details will also be on the web page above. 
The proper reference to the book is as follows: 
Operating Systems: Three Easy Pieces 
Remzi H. Arpaci-Dusseau and Andrea C. Arpaci-Dusseau Version 0.5, June 2012 
http://www.cs.wisc.edu/˜remzi/OSTEP 
The course divides fairly well across a 15-week semester, in which you can cover most of the topics within at a reasonable level of depth.Cramming the course into a quarter probably requires dropping some detailfrom each of the pieces. There are also a few chapters on virtual machine monitors, which we usually squeeze in sometime during the semester, either right at end of the large section on virtualization, or near the end as an aside. 
One slightly unusual aspect of the book is that concurrency, atopic at the front of many OS books, is pushed off herein until the student has built an understanding of virtualization of the CPU and of memory. In our experi­ence, students have a hard time understanding how the concurrency problem arises, or why they are trying to solve it, if they don’t yet understand what an address space is, what a process is, or why context switchescan occur at arbitrary points in time. Once they do understand these concepts, however, introducing the notion of threads and the problems that arisedue to them becomes rather easy, or at least, easier. 
You may have noticed there are no slides that go hand-in-hand with the notes. The major reason for this omission is that we believe inthe most old-fashioned of teaching methods: chalk and a blackboard2.Thus, when we teach the course, we come to class with a few major ideas in mindanduse the board to present them. In our experience, using slides encourages students to “check out” of lecture (and log into facebook.com), as they know the material is there (in powerpoint form) for them to digest later; using the blackboard makes lecture a “live” viewing experience and thus (hopefully) more interac­tive, dynamic, and enjoyable for the students in your class. 
2
We could say markers and a whiteboard, but that is less satisfying. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
If you’d like a copy of the notes we use in preparation for class, please feel 
free to drop us an email. We hope to make a version of those class-preparation 
notes available online sometime. 
OPERATING SYSTEMS ARPACI-DUSSEAU 


To Students 
If you are a student reading this book, thank you! It is a real honor for us to provide some material to help you in your pursuit of knowledge about operating systems. We both think back fondly towards some textbooks of our undergraduate days (e.g., Hennessy and Patterson [HP90]) and hope this book will become one of those good memories for you. 
You may have noticed this book is free and available online. There is one major reason for this: textbooks are generally too expensive. This book, we hope, represents a new wave of free materials to help those in pursuit of their education, regardless of which part of the world they come from or how much they are willing to spend for a book. 
We also hope, where possible, to point you to the original sources of much of the material in the book: the great papers and persons who have shaped the .eld of operating systems over the years. Ideas are not pulled out of the air; they come from smart and hard-working people (including numer­ous Turing-award winners3), and thus we should strive to celebrate those ideas and people where possible. In doing so, we hopefully canbetter under­stand the revolutions that have taken place, instead of writing texts as if those thoughts have been ever present [K62]. Further, perhaps suchreferences will encourage you to dig deeper on your own. 
3
The Turing Award is the highest award in Computer Science; it is like the Nobel Prize, except that you have never heard of it. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

Acknowledgments 
This section will contain thanks to those who helped us put thebook to­gether. The important thing for now: your name could go here! But, you have to help. So send us some feedback and help debug this book.And you could be famous! Or, at least, have your name in some book. 
Those who have helped so far include: Abhirami Senthilkumaran*, Adam 
Eggum, Ahmed Fikri*, Alex Wyler, Benita Bose, Brennan Payne,Cara Lau­
ritzen, Charlotte Kissinger, Cody Hanson, Dan Soendergaard(U. Aarhus), 
Dustin Metzler, Dustin Passofaro, Emily Jacobson, Finn Kuusisto*, Guilherme 
Baptista, Henry Abbey, Huanchen Zhang*, Jake Gillberg, James Perry (U. 
Michigan-Dearborn)*, Jay Lim, Karl Wallinger, Kevin Liu*, Lihao Wang, Martha 
Ferris, Matt Reichoff, Meng Huang, Mike Griepentrog, Murugan Kandaswamy, 
Natasha Eilbert, Nathan Sullivan, Radford Smith, RipudamanSingh, Ross 
Aiken, Ryland Herrick, Seth Pollen, Sharad Punuganti, Shreevatsa R., Sivara­
man Sivaraman*, Srinivasan Thirunarayanan*, Suriyhaprakhas Balaram Sankari, 
Sy Jin Cheah, Tony Adkins, Tuo Wang, Xiang Peng, and Zef RosnBrick. Spe­
cial thanks to those marked with an asterisk above,who have gone above and 
beyond in their suggestions for improvement. 
Also, many thanks to the hundreds students who have taken 537 over the years. In particular, the Fall ’08 class who encouraged the .rst written form of these notes (they were sick of not having any kind of textbook to read – pushy students!). 
Agreatdebt of thanks is also owed to the brave few who took the xv6 project lab course, much of which is now incorporated into themain 537 course. From Spring ’09: Justin Cherniak, Patrick Deline, Matt Czech, Tony Gregerson, Michael Griepentrog, Tyler Harter, Ryan Kroiss,Eric Radzikowski, Wesley Reardan, Rajiv Vaidyanathan, and Christopher Waclawik. From Fall ’09: Nick Bearson, Aaron Brown, Alex Bird, David Capel, KeithGould, Tom Grim, Jeffrey Hugo, Brandon Johnson, John Kjell, Boyan Li, James Loethen, Will McCardell, Ryan Szaroletta, Simon Tso, and Ben Yule. From Spring ’10: Patrick Blesi, Aidan Dennis-Oehling, Paras Doshi, Jake Friedman, Benjamin Frisch, Evan Hanson, Pikkili Hemanth, Michael Jeung, Alex Langenfeld, Scott Rick, Mike Treffert, Garret Staus, Brennan Wall, Hans Werner, Soo-Young Yang, and Carlos Grif.n (almost). 
A.nal debtof gratitude is also owed to Aaron Brown, who .rst took this course many years ago (Spring ’09), then took the xv6 lab course (Fall ’09), and .nally was a graduate teaching assistant for the course for two years or so (Fall ’10 through Spring ’12). His tireless work has vastly improved the state of the projects (particularly those in xv6 land) and thus has helped better the learning experience for countless undergraduates and graduates here at Wisconsin. As Aaron would say (in his usual succinct manner):“Thanks.” 
OPERATING SYSTEMS ARPACI-DUSSEAU 

Final Words 
Yeats famously said “Education is not the .lling of a pail but the lighting of a .re.” He was right but wrong at the same time4.You do have to “.ll the pail” a bit, and these notes are certainly here to help with that part of your education; after all, when you go to interview at Google, and they ask you atrick question about how to use semaphores, it might be good to actually know what a semaphore is, right? 
But Yeats’s larger point is obviously on the mark: the real point of educa­tion is to get you interested in something, to learn somethingmore about the subject matter on your own and not just what you have to digest to get an “A” in some class. As one of our fathers (Remzi’s) used to say, “Learn beyond the classroom”. 
We created these notes to spark your interest in operating systems, to read more about the topic on your own, to talk to your professor about all the exciting research that is going on in the .eld, and even to get involved with that research. It is a great .eld(!), full of exciting and wonderful ideas that have shaped computing history in profound and important ways. And while we understand this .re won’t light for all of you, we hope it does for many, or even a few. Because once that .re is lit, well, that is when you truly become capable of doing something great. And thus the real point of the educational process: to go forth, to study many topics, to learn and to mature, and most importantly, to .nd something that lights a .re for you. 
Andrea and Remzi Married couple Professors of Computer Science at the University of Wisconsin Chief Lighters of Fires, hopefully 5 
4If, that is, he actually said this; as with many famous quotes,the history of this gem is murky. 
5
If this sounds like we are admitting some past history as arsonists, you are probably missing the point. Probably. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

References 
[CK+08] “The xv6 Operating System” Russ Cox, Frans Kaashoek, Robert Morris, Nickolai Zeldovich From: http://pdos.csail.mit.edu/6.828/2008/index.html xv6 was developed as a port of the original UNIX version 6 and represents a beautiful, clean, and simple way to understand a modern operating system. 
[F96] “Six Easy Pieces: Essentials Of Physics Explained By Its Most Brilliant Teacher” Richard P. Feynman Basic Books, 1996 
This book reprints the six easiest chapters of Feynman’s Lectures on Physics, from 1963. If you like Physics, it is a fantastic read. 
[HP90] “Computer Architecture a Quantitative Approach” (1st ed.) David A. Patterson and John L. Hennessy Morgan-Kaufman, 1990 
Abook that encouraged each of us at our undergraduate institutions to pursue graduate studies; we later both had the pleasure of working with Patterson, who greatly shaped the foundations of our research careers. 
[KR88] “The C Programming Language” 
Brian Kernighan and Dennis Ritchie 
Prentice-Hall, April 1988 

The C programming reference that everyone should have, by thepeoplewho invented the lan­guage. 
[K62] “The Structure of Scienti.c Revolutions” 
Thomas S. Kuhn 
University of Chicago Press, 1962 

Agreat and famous read about the fundamentals of the scienti.c process. Mop-up work, anomaly, crisis, and revolution. We are mostly destined to do mop-up work, alas. 
OPERATING SYSTEMS ARPACI-DUSSEAU 


Contents 

ToEveryone ........................... iii 
ToEducators ........................... v 
ToStudents ............................ vii 
Acknowledgments........................ viii 
FinalWords ........................... ix 
References ............................ x 

1ADialogueontheBook 1 
2IntroductiontoOperatingSystems 5 
2.1 VirtualizingtheCPU ................... 7 

2.2 VirtualizingMemory ................... 9 

2.3 Concurrency ........................ 11 

2.4 Persistence ......................... 14 

2.5 DesignGoals ........................ 16 

2.6 SomeHistory ....................... 18 

2.7 Summary .......................... 22 
References ............................ 23 

IVirtualization 25 

3ADialogueonVirtualization 27 
4Abstraction:TheProcess 29 
4.1 TheAbstraction:AProcess ................ 30 

xi 

4.2 ProcessAPI ........................ 31 

4.3 ProcessStates ....................... 32 

4.4 DataStructures ...................... 33 

4.5 Summary .......................... 35 
References ............................ 36 


5Interlude:ProcessAPI 37 
5.1 The fork() SystemCall ................. 37 

5.2 Adding wait() SystemCall ............... 40 

5.3 Finally, the exec() SystemCall ............. 41 

5.4 Why?MotivatingtheAPI ................ 42 

5.5 OtherPartsoftheAPI................... 43 

5.6 Summary .......................... 44 
References ............................ 45 


6Mechanism:LimitedDirectExecution 47 
6.1 Basic Technique: Limited Direct Execution . . . . . . . 48 
6.2 Problem #1: Restricted Operations . . . . . . . . . . . . 48 
6.3 Problem #2: Switching Between Processes . . . . . . . 51 
6.4 Summary .......................... 55 
References ............................ 57 
Homework ............................ 59 


7Scheduling:Introduction 61 
7.1 WorkloadAssumptions.................. 61 

7.2 SchedulingMetrics .................... 62 

7.3 FirstIn,FirstOut(FIFO) ................. 63 

7.4 ShortestJobFirst(SJF) .................. 64 

7.5 Shortest Time-to-Completion First (STCF) . . . . . . . 66 
7.6 RoundRobin........................ 68 

7.7 IncorporatingI/O ..................... 70 

7.8 NoMoreOracle ...................... 72 

7.9 Summary .......................... 72 
References ............................ 73 
Homework ............................ 74 

8Scheduling: TheMulti-LevelFeedbackQueue 79 
8.1 MLFQ:BasicRules .................... 80 

8.2 Attempt#1:HowtoChangePriority . . . . . . . . . . 81 
OPERATING SYSTEMS ARPACI-DUSSEAU 
8.3 Attempt#2: ThePriorityBoost . . . . . . . . . . . . . . 85 
8.4 Attempt#3: BetterAccounting . . . . . . . . . . . . . . 86 
8.5 TuningMLFQandOtherIssues . . . . . . . . . . . . . 87 
8.6 MLFQ:Summary ..................... 89 
References ............................ 91 
Homework ............................ 93 


9Scheduling: Proportional Share 99 
9.1 Basic Concept: Tickets Represent Your Share . . . . . . 99 
9.2 TicketMechanisms .................... 100 

9.3 Implementation ...................... 102 

9.4 HowToAssignTickets?.................. 103 

9.5 WhyNotDeterministic? ................. 103 

9.6 Summary .......................... 105 
References ............................ 106 
Homework ............................ 107 

10 Multiprocessor Scheduling 111 
11 Summary Dialogue on CPU Virtualization 113 
12 The Abstraction: Address Spaces 117 
12.1 EarlySystems ....................... 117 

12.2 Multiprogramming and Time Sharing . . . . . . . . . . 117 
12.3 TheAddressSpace .................... 119 

12.4 Goals ............................ 123 

12.5 Summary .......................... 124 
References ............................ 125 


13 Interlude: Memory API 127 
13.1 TypesofMemory ..................... 127 

13.2 The malloc() Call .................... 128 

13.3 The free() Call ..................... 130 

13.4 CommonErrors ...................... 131 

13.5 UnderlyingOSSupport.................. 134 

13.6 OtherCalls ......................... 135 

13.7 Summary .......................... 135 
References ............................ 136 

14 Mechanism: Address Translation 137 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 

14.1 Assumptions ........................ 138 

14.2 AnExample ........................ 139 

14.3 Dynamic (Hardware-based) Relocation . . . . . . . . . 141 
14.4 OSIssues .......................... 145 

14.5 Summary .......................... 147 
References ............................ 149 
Homework ............................ 150 

15 Segmentation 155 
15.1 Segmentation: Generalized Base/Bounds . . . . . . . 156 
15.2 Which Segment Are We Referring To? . . . . . . . . . . 159 
15.3 WhatAboutTheStack? .................. 160 

15.4 SupportforSharing .................... 161 

15.5 Fine-grained vs. Coarse-grained Segmentation . . . . 162 
15.6 OSSupport......................... 163 

15.7 Summary .......................... 165 
References ............................ 167 
Homework ............................ 169 


16 Free-Space Management (INCOMPLETE) 175 
16.1 Assumptions ........................ 176 

16.2 Low-levelMechanisms .................. 177 

16.3 BasicStrategies ...................... 186 

16.4 OtherApproaches ..................... 187 

16.5 Summary .......................... 189 

16.6 References ......................... 190 


17 Paging: Introduction 191 
17.1 WhereArePageTablesStored? . . . . . . . . . . . . . 195 
17.2 What’s Actually In The Page Table? . . . . . . . . . . . 196 
17.3 Paging:AlsoTooSlow .................. 197 
References ............................ 200 
Homework ............................ 201 


18 Paging: Faster Translations (TLBs) 207 
18.1 WhoHandlestheMiss?.................. 208 

18.2 TLBContents:What’sInThere? . . . . . . . . . . . . . 211 
18.3 TLBIssue:ContextSwitches . . . . . . . . . . . . . . . 212 
18.4 Issue:ReplacementPolicy ................ 214 

18.5 RealCode ......................... 215 

OPERATING SYSTEMS ARPACI-DUSSEAU 
18.6 AnExample ........................ 216 

18.7 Summary .......................... 217 
References ............................ 219 
Homework ............................ 221 


19 Paging: Smaller Tables 225 
19.1 SimpleSolution:BiggerPages . . . . . . . . . . . . . . 225 
19.2 Hybrid Approach: Paging and Segments . . . . . . . . 226 
19.3 Multi-levelPageTables .................. 230 

19.4 InvertedPageTables ................... 239 

19.5 SwappingthePageTablestoDisk . . . . . . . . . . . . 240 
19.6 Paging:Summary ..................... 240 
References ............................ 241 
Homework ............................ 242 


20 Beyond Physical Memory: Mechanisms 245 
20.1 SwapSpace......................... 246 

20.2 ThePresentBit....................... 247 

20.3 ThePageFault ....................... 248 

20.4 WhatIfMemoryIsFull? ................. 249 

20.5 PageFaultControlFlow ................. 250 

20.6 When Replacements Really Occur . . . . . . . . . . . . 251 
20.7 Summary .......................... 252 
References ............................ 254 


21 Beyond Physical Memory: Policies 255 
21.1 CacheManagement .................... 256 

21.2 The Optimal Replacement Policy . . . . . . . . . . . . 257 
21.3 ASimplePolicy:FIFO .................. 259 

21.4 AnotherSimplePolicy:Random. . . . . . . . . . . . . 260 
21.5 UsingHistory:LRU .................... 262 

21.6 WorkloadExamples .................... 264 

21.7 Implementing Historical Algorithms . . . . . . . . . . 266 
21.8 ApproximatingLRU ................... 267 

21.9 ConsideringDirtyPages ................. 269 

21.10OtherVMPolicies ..................... 269 

21.11Thrashing ......................... 270 

21.12Summary .......................... 271 
References ............................ 272 
Homework ............................ 274 

ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 


22 Case Study: The VAX/VMS Virtual Memory System 279 
22.1 Background ........................ 279 

22.2 Memory Management Hardware . . . . . . . . . . . . 280 
22.3 ARealAddressSpace ................... 283 

22.4 PageReplacement ..................... 284 

22.5 OtherNeatVMTricks................... 286 

22.6 Summary .......................... 287 
References ............................ 288 


23 Summary Dialogue on Memory Virtualization 289 



II Concurrency 293 
24 A Dialogue on Concurrency 295 
25 Concurrency: An Introduction 297 
25.1 AnExample:ThreadCreation . . . . . . . . . . . . . . 299 
25.2 WhyItGetsWorse:SharedData . . . . . . . . . . . . . 301 
25.3 The Heart of the Problem: Uncontrolled Scheduling . 304 
25.4 TheWishForAtomicity.................. 307 

25.5 One More Problem: Waiting For Another . . . . . . . . 309 
25.6 Summary:WhyinOSClass?. . . . . . . . . . . . . . . 309 References ............................ 311 

26 Interlude: Thread API 313 
26.1 ThreadCreation ...................... 313 

26.2 ThreadCompletion .................... 315 

26.3 Locks ............................ 318 

26.4 ConditionVariables .................... 320 

26.5 CompilingandRunning ................. 322 

26.6 Summary .......................... 323 
References ............................ 325 


27 Locks 327 
27.1 Locks:TheBasicIdea ................... 327 

27.2 PthreadLocks ....................... 328 

27.3 BuildingALock ...................... 329 

27.4 EvaluatingLocks ..................... 330 

27.5 ControllingInterrupts................... 330 

OPERATING SYSTEMS ARPACI-DUSSEAU 
27.6 TestAndSet(AtomicExchange) . . . . . . . . . . . . . 333 
27.7 BuildingAWorkingSpinLock. . . . . . . . . . . . . . 335 
27.8 EvaluatingSpinLocks .................. 336 

27.9 Compare-And-Swap ................... 337 

27.10 Load-Linked and Store-Conditional . . . . . . . . . . . 339 
27.11Fetch-And-Add ...................... 340 

27.12Summary:SoMuchSpinning . . . . . . . . . . . . . . 342 
27.13 A Simple Approach: Just Yield, Baby . . . . . . . . . . 343 
27.14 Using Queues: Sleeping Instead Of Spinning . . . . . . 344 
27.15 Different OS, Different Support . . . . . . . . . . . . . 347 
27.16Two-PhaseLocks ..................... 348 

27.17Summary .......................... 349 
References ............................ 350 


28 Using Locks (INCOMPLETE) 353 
28.1 ConcurrentCounter .................... 353 

28.2 ConcurrentLinkedList .................. 354 

28.3 ConcurrentHashTable .................. 355 


29 Condition Variables 357 
29.1 De.nitionandRoutines.................. 359 

29.2 The Producer/Consumer (Bound Buffer) Problem . . . 362 
29.3 CoveringConditions ................... 371 

29.4 Summary .......................... 372 
References ............................ 373 


30 Semaphores 375 
30.1 Semaphores:ADe.nition ................ 375 

30.2 BinarySemaphores(Locks). . . . . . . . . . . . . . . . 377 
30.3 Semaphores As Condition Variables . . . . . . . . . . . 378 
30.4 The Producer/Consumer (Bounded-Buffer) Problem . 380 
30.5 Reader-WriterLocks ................... 384 

30.6 TheDiningPhilosophers ................. 386 

30.7 HowToImplementSemaphores . . . . . . . . . . . . . 389 
30.8 Summary .......................... 390 
References ............................ 392 


31 Deadlock 395 
31.1 WhyDoDeadlocksOccur? ................ 396 

31.2 ConditionsforDeadlock ................. 397 

ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 

31.3 Prevention ......................... 398 

31.4 AvoidanceviaScheduling ................ 402 

31.5 DetectandRecover .................... 403 

31.6 Summary .......................... 404 
References ............................ 405 


32 Real-World Concurrency Bugs (INCOMPLETE) 407 
References ............................ 409 


33 Summary Dialogue on Concurrency 411 


III Persistence 413 
34 A Dialogue on Persistence 415 
35 I/O Devices 417 
35.1 SystemArchitecture .................... 417 

35.2 ACanonicalDevice .................... 418 

35.3 TheCanonicalProtocol .................. 419 

35.4 Lowering CPU Overhead with Interrupts . . . . . . . 420 
35.5 More Ef.cient Data Movement with DMA . . . . . . . 422 
35.6 MethodsofDeviceInteraction . . . . . . . . . . . . . . 423 
35.7 Fitting into the OS: The Device Driver . . . . . . . . . 424 
35.8 Case Study: A Simple IDE Disk Driver . . . . . . . . . 426 
35.9 HistoricalNotes ...................... 429 

35.10Summary .......................... 430 
References ............................ 431 


36 Hard Disk Drives 433 
36.1 TheInterface ........................ 433 

36.2 BasicGeometry ...................... 434 

36.3 ASimpleDiskDrive ................... 435 

36.4 I/OTime:DoingTheMath................ 439 

36.5 DiskScheduling ...................... 443 

36.6 Summary .......................... 447 
References ............................ 448 
Homework ............................ 449 


37 Redundant Arrays of Inexpensive Disks (RAIDs) 455 
OPERATING SYSTEMS ARPACI-DUSSEAU 
37.1 InterfaceandRAIDInternals. . . . . . . . . . . . . . . 456 
37.2 FaultModel ........................ 457 

37.3 HowtoEvaluateaRAID ................. 458 

37.4 RAIDLevel0:Striping .................. 458 

37.5 RAIDLevel1:Mirroring ................. 463 

37.6 RAID Level 4: Saving Space with Parity . . . . . . . . 466 
37.7 RAIDLevel5:RotatingParity . . . . . . . . . . . . . . 471 
37.8 RAIDComparison:ASummary . . . . . . . . . . . . . 472 
37.9 OtherInterestingRAIDIssues . . . . . . . . . . . . . . 473 
37.10Summary .......................... 474 
References ............................ 475 
Homework ............................ 477 


38 The Abstraction: Files and Directories 483 
38.1 FilesandDirectories.................... 484 

38.2 TheFileSystemInterface ................. 486 

38.3 CreatingFiles ....................... 486 

38.4 ReadingandWritingFiles ................ 487 

38.5 Reading And Writing, But Not Sequentially . . . . . . 490 
38.6 Getting Information About Files . . . . . . . . . . . . . 490 
38.7 RemovingFiles ...................... 492 

38.8 MakingDirectories .................... 493 

38.9 ReadingDirectories .................... 494 

38.10DeletingDirectories .................... 495 

38.11HardLinks ......................... 495 

38.12SymbolicLinks ...................... 497 

38.13 Making and Mounting a File System . . . . . . . . . . 499 
38.14Summary .......................... 501 
References ............................ 502 
Homework ............................ 503 


39 File System Implementation 505 
39.1 TheWayToThink ..................... 506 

39.2 OverallOrganization ................... 506 

39.3 FileOrganization:TheInode . . . . . . . . . . . . . . . 509 
39.4 DirectoryOrganization .................. 514 

39.5 FreeSpaceManagement ................. 515 

39.6 Access Paths: Reading and Writing . . . . . . . . . . . 515 
39.7 CachingandBuffering .................. 520 

39.8 Summary .......................... 521 

ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 

References ............................ 522 
Homework ............................ 524 


40 Locality and The Fast File System 531 
40.1 TheProblem:PoorPerformance . . . . . . . . . . . . . 531 
40.2 FFS: Disk Awareness Is The Solution . . . . . . . . . . 533 
40.3 Organizing Structure: The Cylinder Group . . . . . . . 533 
40.4 Policies: How To Allocate Files and Directories . . . . 535 
40.5 TheLarge-FileException ................. 535 

40.6 AFewOtherThingsAboutFFS . . . . . . . . . . . . . 537 
40.7 Summary .......................... 540 
References ............................ 541 


41 Crash Consistency: FSCK and Journaling 543 
41.1 ADetailedExample .................... 544 

41.2 Solution #1: The File System Checker . . . . . . . . . . 548 
41.3 Solution #2: Journaling (or Write-Ahead Logging) . . . 550 
41.4 Solution#3:OtherApproaches . . . . . . . . . . . . . 560 
41.5 Summary .......................... 561 
References ............................ 562 


42 Log-structured File Systems 565 
42.1 WritingToTheLog:SomeDetails . . . . . . . . . . . . 566 
42.2 How Can We Find Those Pesky Inodes? . . . . . . . . 567 
42.3 Solution Through Indirection: The Inode Map . . . . . 568 
42.4 Even More Problems: Where To Put The Inode Map? . 569 
42.5 ReadingAFileFromDisk:ARecap . . . . . . . . . . . 570 
42.6 A New Problem: Garbage Collection . . . . . . . . . . 571 
42.7 How Can We Determine Which Blocks Are Live? . . . 571 
42.8 A Policy Question: Which Blocks To Clean, And When? 572 
42.9 CrashRecovery ...................... 573 

42.10Summary .......................... 573 
References ............................ 575 


43 Modern File Systems (INCOMPLETE) 577 
43.1 WAFLandChecksums .................. 577 

43.2 XFSandB-Trees ...................... 577 

43.3 ZFSandRAID ....................... 577 
References ............................ 578 

OPERATING SYSTEMS ARPACI-DUSSEAU 

44 Summary Dialogue on Persistence 579 
45 A Dialogue on Distribution 581 
46 Distributed Systems 583 
46.1 CommunicationBasics .................. 584 

46.2 Unreliable Communication Layers . . . . . . . . . . . 585 
46.3 Reliable Communication Layers . . . . . . . . . . . . . 588 
46.4 Communication Abstractions . . . . . . . . . . . . . . 593 
46.5 RemoteProcedureCall(RPC) . . . . . . . . . . . . . . 594 
46.6 Summary .......................... 600 
References ............................ 601 


47 Sun’s Network File System (NFS) 603 
47.1 ABasicDistributedFileSystem . . . . . . . . . . . . . 603 
47.2 OnToNFS ......................... 605 

47.3 Focus: Simple and Fast Server Crash Recovery . . . . . 605 
47.4 Key To Fast Crash Recovery: Statelessness . . . . . . . 606 
47.5 TheNFSv2Protocol .................... 608 

47.6 From Protocol to Distributed File System . . . . . . . . 610 
47.7 Handling Server Failure with Idempotent Operations . 612 
47.8 Improving Performance: Client-side Caching . . . . . 614 
47.9 TheCacheConsistencyProblem . . . . . . . . . . . . . 615 
47.10 Assessing NFS Cache Consistency . . . . . . . . . . . . 617 
47.11 Implications on Server-Side Write Buffering . . . . . . 617 
47.12Summary .......................... 619 
References ............................ 621 


48 The Andrew File System (AFS) 623 
48.1 AFSVersion1 ....................... 623 

48.2 ProblemswithVersion1 ................. 625 

48.3 ImprovingtheProtocol .................. 626 

48.4 AFSVersion2 ....................... 626 

48.5 CacheConsistency .................... 627 

48.6 CrashRecovery ...................... 628 

48.7 ScaleofAFSv2 ....................... 629 

48.8 Other Improvements: Namespaces, Security, Etc. . . . 629 
48.9 Summary .......................... 629 
References ............................ 630 

ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 



VAppendices 631 
49 A Dialogue on Virtual Machine Monitors 633 
50 Virtual Machine Monitors 635 
50.1 Introduction ........................ 635 

50.2 Motivation:WhyVMMs?................. 636 

50.3 VirtualizingtheCPU ................... 637 

50.4 VirtualizingMemory ................... 641 

50.5 TheInformationGap ................... 646 

50.6 Summary .......................... 647 
References ............................ 649 


51 VMM Memory Management (INCOMPLETE) 651 
51.1 Introduction ........................ 651 
References ............................ 651 



General Index 653 
Advice, Asides, Design Tips, Data Structures, and Hardware Support 665 
Cruces 669 
OPERATING SYSTEMS ARPACI-DUSSEAU 


List of Figures 
2.1 Simple Example: Code That Loops and Prints . . . . . . . 8 
2.2 AProgramthatAccessesMemory . . . . . . . . . . . . . . 10 
2.3 AMultithreadedprogram .................. 12 

2.4 AProgramThatDoesI/O .................. 15 

4.1 Process:StateTransitions ................... 33 

4.2 Thexv6ProcStructure .................... 35 

5.1 The code for p1.c ....................... 38 

5.2 The code for p2.c ....................... 39 

5.3 The code for p3.c ....................... 41 

6.1 Thexv6ContextSwitchCode ................ 55 

7.1 FIFOSimpleExample ..................... 63 

7.2 WhyFIFOIsNotThatGreat ................. 64 

7.3 SJFSimpleExample ...................... 65 

7.4 SJFWithLateArrivalsFromBandC . . . . . . . . . . . . 66 
7.5 STCFSimpleExample..................... 67 

7.6 SJFAgain(BadforResponseTime) . . . . . . . . . . . . . 68 
7.7 RoundRobin .......................... 68 

7.8 PoorUseofResources..................... 71 

7.9 Overlap Allows Better Use of Resources . . . . . . . . . . 71 
7.10 HomeworkOutput ...................... 75 

7.11 Generating Homework Solutions . . . . . . . . . . . . . . 75 
xxiii 

8.1 MLFQExample ........................ 81 

8.2 Long-runningJobOverTime................. 82 

8.3 AlongCameAnInteractiveJob . . . . . . . . . . . . . . . 83 
8.4 A Mixed I/O-intensive and CPU-intensive Workload . . . 84 
8.5 Without (Left) and With (Right) Priority Boost . . . . . . . 85 
8.6 Without (Left) and With (Right) Gaming Tolerance . . . . 87 
8.7 LowerPriority,LongerQuanta. . . . . . . . . . . . . . . . 88 
12.1 OperatingSystems:TheEarlyDays . . . . . . . . . . . . . 118 
12.2 ThreeProcesses:SharingMemory . . . . . . . . . . . . . . 119 
12.3 AnExampleAddressSpace ................. 120 

14.1 AProcessAndItsAddressSpace . . . . . . . . . . . . . . 141 
14.2 Physical Memory with a Single Relocated Process . . . . . 142 
14.3 AFixed-SizedStackAddressSpace . . . . . . . . . . . . . 150 
15.1 AnAddressSpace(Again) .................. 156 

15.2 Placing Segments In Physical Memory . . . . . . . . . . . 157 
15.3 Non-compacted and Compacted Memory. . . . . . . . . . 164 
16.1 AnAllocatedRegionPlusHeader . . . . . . . . . . . . . . 180 
16.2 Speci.cContentsOfTheHeader . . . . . . . . . . . . . . 180 
16.3 Free Space With Three Chunks Allocated . . . . . . . . . . 183 
16.4 Free Space With Two Chunks Allocated . . . . . . . . . . . 184 
16.5 Free Space With Two Chunks Allocated . . . . . . . . . . . 185 
17.1 ASimple64-byteAddressSpace. . . . . . . . . . . . . . . 191 
17.2 64-Byte Address Space Placed In Physical Memory . . . . 192 
17.3 TheAddressTranslationProcess. . . . . . . . . . . . . . . 194 
17.4 Example: Page Table in Kernel Physical Memory . . . . . 196 
17.5 Anx86PageTableEntry(PTE). . . . . . . . . . . . . . . . 197 
17.6 AccessingMemoryWithPaging . . . . . . . . . . . . . . . 199 
18.1 TLBControlFlowAlgorithm. . . . . . . . . . . . . . . . . 209 
18.2 TLB Control Flow Algorithm (OS Handled) . . . . . . . . 210 
18.3 TLB Hits/Misses During Array Access. . . . . . . . . . . . 216 
18.4 AMIPSTLBEntry. ...................... 216 

18.5 Discovering TLB Sizes and Miss Costs. . . . . . . . . . . . 222 
19.1 A 16-KB address space with 1-KB pages . . . . . . . . . . 227 
OPERATING SYSTEMS ARPACI-DUSSEAU 

19.2 Linear (Left) and Multi-Level (Right) Page Tables . . . . .230 
19.3 A 16-KB address space with 64-byte pages . . . . . . . . . 233 
19.4 Multi-level Page Table Control Flow . . . . . . . . . . . . 239 
20.1 PhysicalMemoryandSwapSpace. . . . . . . . . . . . . . 247 
20.2 Page-Fault Control Flow Algorithm (Hardware) . . . . . . 251 
20.3 Page-Fault Control Flow Algorithm (Software) . . . . . . 252 
21.1 Random Performance over 10,000 Trials . . . . . . . . . . 261 
21.2 The No-Locality, 80-20, and Looping Workloads . . . . . . 265 
21.3 The80-20WorkloadwithClock . . . . . . . . . . . . . . . 268 
22.1 TheVAX/VMSAddressSpace ................ 281 

25.1 ASingle-ThreadedAddressSpace. . . . . . . . . . . . . . 298 
25.2 AMulti-ThreadedAddressSpace . . . . . . . . . . . . . . 298 
25.3 SimpleThreadCreationCode . . . . . . . . . . . . . . . . 299 
25.4 SharingData:OhOh ..................... 302 

25.5 TheProblem:UpCloseandPersonal . . . . . . . . . . . . 306 
26.1 CreatingaThread ....................... 315 

26.2 WaitingforThreadCompletion . . . . . . . . . . . . . . . 317 
26.3 Simpler Argument Passing to a Thread . . . . . . . . . . . 318 
26.4 AnExampleWrapper ..................... 320 

27.1 FirstAttempt:ASimpleFlag ................. 333 

27.2 A Simple Spin Lock using Test-and-set . . . . . . . . . . . 336 
27.3 Compare-and-swap ...................... 338 

27.4 Load-linked and Store-conditional . . . . . . . . . . . . . . 339 
27.5 UsingLL/SCToBuildALock ................ 340 

27.6 TicketLocks .......................... 342 

27.7 Lockwithtest-and-setandyield . . . . . . . . . . . . . . . 343 
27.8 Lock with Queues, test-and-set, yield, and wakeup . . . . 345 
27.9 Linux-basedFutexLocks ................... 347 

29.1 AParentWaitingForItsChild . . . . . . . . . . . . . . . . 357 
29.2 Parent Waiting For Child: Spin-based Approach . . . . . . 358 
29.3 Parent Waiting For Child: Use A Condition Variable . . . 360 
29.4 ThePutandGetRoutines(Version1) . . . . . . . . . . . . 363 
29.5 Producer/Consumer Threads (Version 1) . . . . . . . . . . 364 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

29.6 Producer/Consumer: Single CV and If Statement . . . . . 365 
29.7 Producer/Consumer: Single CV and While . . . . . . . . 367 
29.8 Producer/Consumer: Two CVs and While . . . . . . . . . 368 
29.9 TheFinalPutandGetRoutines . . . . . . . . . . . . . . . 369 
29.10TheFinalWorkingSolution . . . . . . . . . . . . . . . . . 370 
29.11Covering Conditions: An Example . . . . . . . . . . . . . 371 
30.1 InitializingASemaphore ................... 376 

30.2 Semaphore: De.nitions of Wait and Post . . . . . . . . . . 376 
30.3 ABinarySemaphore,a.k.a.aLock . . . . . . . . . . . . . 377 
30.4 AParentWaitingforitsChild . . . . . . . . . . . . . . . . 378 
30.5 Parent Waiting for its Child with Semaphores . . . . . . . 379 
30.6 ThePutandGetRoutines .................. 380 

30.7 Adding the Full and Empty Conditions . . . . . . . . . . . 381 
30.8 Adding Mutual Exclusion (Incorrectly) . . . . . . . . . . . 383 
30.9 Adding Mutual Exclusion (Correctly) . . . . . . . . . . . . 384 
30.10ASimpleReader-WriterLock . . . . . . . . . . . . . . . . 386 
30.11TheDiningPhilosophers ................... 387 

30.12Implementing Semaphores with Locks and CVs . . . . . . 390 
31.1 TheDeadlockDependencyGraph . . . . . . . . . . . . . . 396 
35.1 Prototypical System Architecture. . . . . . . . . . . . . . . 418 
35.2 ACanonicalDevice....................... 419 

35.3 TheFileSystemStack ..................... 426 

35.4 The xv6 IDE Disk Driver (Simpli.ed) . . . . . . . . . . . . 428 
36.1 Adiskwithjustasingletrack . . . . . . . . . . . . . . . . 435 
36.2 Asingletrackplusahead .................. 435 

36.3 Threetracksplusahead ................... 436 

36.4 Three tracks plus a head: After seeking . . . . . . . . . . . 437 
36.5 Threetracks:TrackSkewof2 . . . . . . . . . . . . . . . . 438 
36.6 SSTF:SchedulingRequests21and26 . . . . . . . . . . . . 444 
36.7 SSTF:Sometimesnotgoodenough . . . . . . . . . . . . . 446 
38.1 AnExampleDirectoryTree.................. 485 

39.1 FileReadTimeline....................... 517 

39.2 FileCreationTimeline..................... 519 

OPERATING SYSTEMS ARPACI-DUSSEAU 

40.1 FFS: Standard vs. Parameterized Placement . . . . . . . . 538 
46.1 Example UDP/IP Client/Server Code . . . . . . . . . . . 586 
46.2 ASimpleUDPLibrary .................... 587 

46.3 MessagePlusAcknowledgment . . . . . . . . . . . . . . . 589 
46.4 MessagePlusAcknowledgment . . . . . . . . . . . . . . . 590 
46.5 MessagePlusAcknowledgment . . . . . . . . . . . . . . . 591 
47.1 AGenericClient/ServerSystem . . . . . . . . . . . . . . . 604 
47.2 Distributed File System Architecture . . . . . . . . . . . . 605 
47.3 ClientCode:ReadingfromaFile . . . . . . . . . . . . . . 606 
47.4 SomeexamplesoftheNFSProtocol . . . . . . . . . . . . . 609 
47.5 Reading A File: Client-side and File Server Actions . . . .611 
47.6 TheThreeTypesofLoss ................... 613 

47.7 TheCacheConsistencyProblem . . . . . . . . . . . . . . . 615 
48.1 AFSv1ProtocolHighlights .................. 624 

50.1 VMMMemoryVirtualization ................ 641 

THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 


ADialogueontheBook 
Professor: Welcome to this book! It’s called Operating Systems in Three Easy Pieces,and I am here to teach you the things you need to know about operating systems. I am called “Professor”; who are you? 
Student: Hi Professor! I am called “Student”, as you might have guessed. And I am here and ready to learn! 
Professor: Sounds good. Any questions? 
Student: Sure! Why is it called “Three Easy Pieces”? 
Professor: That’s an easy one. Well, you see, there are these great lectures on Physics by Richard Feynman... 
Student: Oh! The guy who wrote “Surely You’re Joking, Mr. Feynman”, right? Great book! Is this going to be hilarious like that bookwas? 
Professor: Um... well, no. That book was great, and I’m glad you’ve read it. Hopefully this book is more like his notes on Physics. Someof the basics were summed up in a book called “Six Easy Pieces”. He was talking about Physics; we’re going to do Three Easy Pieces on the .ne topic ofOperating Systems. This is appropriate, as Operating Systems are abouthalf as hard as Physics. 
Student: Well, I liked physics, so that is probably good. What are those pieces? 
Professor: They are the three key ideas we’re going to learn about: virtu­
1 
alization, concurrency,and persistence.In learning about these ideas, we’ll learn all about how an operating system works, including how it de­cides what program to run next on a CPU, how it handles memory overload in a virtual memory system, how virtual machine monitors work, how to manage information on disks, and even a little about how to build a dis­tributed system that works when parts of them have failed. That sort of stuff. 
Student: Ihave no idea what you’re talking about, really. 
Professor: Good! That means you are in the right class. 
Student: Ihave another question: what’s the bestway to learn this stuff? 
Professor: Excellent query! Well, each person needs to .gure this out on their own, of course, but here is what I would do: go to class,to hear the professor introduce the material. Then, say at the end of every week, read these notes, to help the ideas sink into your head a bit better. Of course, some time later (hint: before the exam!), read the notes againto .rm up your knowledge. Of course, your professor will no doubt assign some homeworks and projects, so you should do those; in particular, doing projects where you write real code to solve real problems is the best way to put theideaswithin these notes into action. As Confucius said... 
Student: Oh, I know! ’I hear and I forget. I see and I remember. I do and I understand.’ Or something like that. 
Professor: (surprised) How did you know what I was going to say?! 
Student: It seemed to follow. Also, I am a big fan of Confucius. 
Professor: Well, I think we are going to get along just .ne! Just .ne indeed. 
Student: Professor – just one more question, if I may. What are these dialogues for? I mean, isn’t this just supposed to be a book? Why not present the material directly? 
Professor: Ah, good question, good question! Well, I think it is sometimes useful to pull yourself outside of a narrative and think a bit;these dialogues are those times. So you and I are going to work together to make sense of all of these pretty complex ideas. Are you up for it? 
Student: So we have to think? Well, I’m up for that. I mean, what else do 
OPERATING SYSTEMS ARPACI-DUSSEAU 
Ihavetodoanyhow?It’snotlikeIhavemuchofalifeoutsideofthisbook. Professor: Meneither,sadly.Solet’sgettowork! 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

Introduction to Operating Systems 
If you are taking an undergraduate operating systems course,you should already have some idea of what a computer program does when it runs. If not, the course is going to be dif.cult – so you should probably drop the course, or run to the nearest bookstore and quickly consume the necessary background material (both Patt/Patel[PP03] and Bryant/O’Halloran [BOH10] are pretty great books). 
So what happens when a program runs? 
Well, a running program does one very simple thing: it executes instructions. Many millions (and these days, even billions)of times every second, the processor fetches an instruction from memory, de­codes it (i.e., .gures out which instruction this is), and executes it (i.e., it does the thing that it is supposed to do, like add two numbers together, access memory, check a condition, and so forth). After it is done with this instruction, the processor moves on to the next in­struction, and so on, and so on, until the program .nally completes1. 
Thus, we have just described the basics of the Von Neumann model of computing2.Sounds simple, right? But in this class, we 
1
Of course, modern processors do many bizarre and frighteningthings underneath the hood to make programs run faster, e.g., executing multiple instructions at once, and even issuing and completing them out of order! But that is not our concern here; we are just concerned with the simple model most programs assume: that instructions seem­ingly execute one at a time. 
2Von Neumann was one of the early pioneers of computing systems. He also did pioneering work on game theory and atomic bombs, and played intheNBA for six years. OK, one of those things isn’t true. 
5 
THE CRUX OF THE PROBLEM: 
HOW TO VIRTUALIZE RESOURCES 
The central question we will answer in these notes is quite simple: 
how does the operating system virtualize resources? This is the crux 
of our problem. Note that why the OS does this is not the main ques­
tion, as the answer should be obvious: it makes the system easier to 
use. Thus, we focus on the how:what mechanisms and policies are 
implemented by the OS to attain virtualization? How does the OS 
do so ef.ciently? What hardware support is needed? 
Note that we will use the “crux of the problem”, in shaded boxes 
such as this one, as a way to call out speci.c problems we are trying 
to solve in building an operating system. Thus, within a note on 
aparticular topic, you may .nd one or more cruces (yes, this is the 
proper plural) which highlight the problem. The details within the 
note, of course, present the solution, or at least the basic parameters 
of a solution. 
will be learning that while a program runs, a lot of other wild things are going on with the primary goal of making the system easy to use. 
There is a body of software, in fact, that is responsible for making it easy to run programs (even allowing you to seemingly run many at the same time), allowing programs to share memory, enabling pro­grams to interact with devices, and other fun stuff like that.That 
body of software is called the operating system (OS)3,asit isin charge of making sure the system operates correctly and ef.ciently in an easy-to-use manner. 
The primary way the OS does this is through a general technique that we call virtualization.That is, the OS takes a physical resource (such as the processor, or memory, or a disk) and transforms itinto a more general, powerful, and easy-to-use virtual form of itself. Thus, we sometimes refer to the operating system as a virtual machine. 
Of course, in order to allow users to tell the OS what to do and 
thus make use of the features of the virtual machine (such as run­
ning a program, or allocating memory, or accessing a .le), theOS 
3Another early name for the OS was the supervisor or even the master control pro­
gram.Apparently, this last name sounded a little overzealous (see the movie Tron for 
details) and thus, thankfully, “operating system” caught oninstead. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
also provides some interfaces (APIs) that you can call. A typical OS, in fact, exports a few hundred system calls that are available to appli­cations. Because the OS provides these calls to run programs,access memory and devices, and other related actions, we also sometimes say that the OS provides a standard library to applications. 
Finally, because virtualization allows many programs to run(thus sharing the CPU), and many programs to concurrently access their own instructions and data (thus sharing memory), and many pro­grams to access devices (thus sharing disks and so forth), theOS is sometimes known as a resource manager.Each of the CPU, memory, and disk is a resource of the system; it is thus the operating system’s role to manage those resources, doing so ef.ciently or fairly or in­deed with many other possible goals in mind. 
To understand the role of the OS a little bit better, let’s takea look at some examples. 
2.1 Virtualizing the CPU 
Figure 2.1 depicts our .rst program. It doesn’t do much. In fact, all it does is call a routine, Spin(),that repeatedly checksthe time and returns once it has run for 1 second. Then, it prints out thestring that the user passed in on the command line, and then it repeatsthat, forever. 
Let’s say we save this .le as cpu.c and decide to compile and run 
it on a system with a single processor (or CPU as we will sometimes 
call it). Here is what we will see: 
prompt> gcc -o cpu cpu.c -Wall
prompt> ./cpu "A"
A 
A 
A 
A 
ˆC 
prompt> 

Not too interesting. The system begins running the program, which 
repeatedly checks the time until a second has elapsed. Once a second 
has passed, the code prints the input string passed in by the user (in 
this example, the letter “A”), and continues. Note the program will 
run forever; only by pressing “Control-c” (which on UNIX-based 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
#include <stdio.h> 
#include <stdlib.h> 
#include <sys/time.h>
#include <assert.h> 
#include "common.h" 

int 
main(int argc, char *argv[])
{ 

if (argc != 2) {fprintf(stderr, "usage: cpu <string>\n");exit(1);
}
char *str = argv[1];
while (1) {

Spin(1);
printf("%s\n", str); 
} 
return 0; 

} 
Figure 2.1: Simple Example: Code That Loops and Prints 
systems will terminate the program running in the foreground) can we halt the program. Now, let’s do the same thing, but this time, let’s run many differ­ent instances of this same program: 
prompt> ./cpu A & ; ./cpuB & ; ./cpu C & ; ./cpu D&
[1] 7353
[2] 7354
[3] 7355
[4] 7356
A 
B 
D 
C 
A 
B 
D 
C 
A 
C 
B 
D 
... 

OPERATING SYSTEMS ARPACI-DUSSEAU 
Well, now things are getting a little more interesting. Even though 
we have only one processor, somehow all four of these programs 
seem to be running at the same time! How does this magic happen?4 
It turns out that the operating system, with some help from the hardware, is in charge of this illusion,i.e., the illusion that the system has a very large number of virtual CPUs. Turning a single CPU (or small set of them) into a seemingly in.nite number of CPUs and thus allowing many programs to seemingly run at once is what we call virtualizing the CPU.It isthe focus of Part I of these notes. 
Of course, to run programs, and stop them, and otherwise tell the OS which programs to run, there need to be some interfaces (APIs) that you can use to communicate your desires to the OS. We’ll talk about these APIs throughout these notes; indeed, they are themajor way in which most users interact with operating systems. 
You might also notice that the ability to run multiple programs at once raises all sorts of new questions. For example, if two programs want to run at a particular time, which should run? This question is answered by a policy of the operating system; policies are used in many different places within an OS to answer these types of ques­tions, and thus we will study them as we learn about the basic mech­anisms that operating systems implement (such as the ability to run multiple programs at once). Hence the role of the OS as a resource manager. 

2.2 Virtualizing Memory 
Now let’s consider memory. The model of physical memory pre­sented by modern machines is very simple. Memory is just an array of bytes; to read memory, one must specify an address to be able to access the data stored there; to write (or update)memory, one must also specify the data to be written to the given address. 
Memory is accessed all the time when a program is running. A program keeps all of its data structures in memory, and accesses them through various instructions, like loads and stores or other ex­plicit memory-accessing operations. And of course, each instruction 
4
Note how we ran four processes at the same time, by using the & symbol. Doing so runs a job in the background, which means that the user is able to immediately issue their next command, which in this case is another program to run. Thesemi-colonbetween commands allows us to specify multiple jobs on the command line. 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
#include <unistd.h> 
#include <stdio.h> 
#include <stdlib.h> 
#include "common.h" 

int 
main(int argc, char *argv[])
{ 

int *p=malloc(sizeof(int)); //a1 
assert(p != NULL);
printf("(%d) address of p: %08x\n",

getpid(), (unsigned) p); // a2 *p=0; //a3 while (1) {
Spin(1); 
*p= *p+ 1;
printf("(%d) p: %d\n", getpid(), *p); // a4 

} 
return 0; 
} 

Figure 2.2: A Program that Accesses Memory 
of the program is in memory, and thus memory is accessed on each instruction fetch too. 
Let’s take a look at a program (in Figure 2.2) that allocates some memory by calling malloc().The output of this program can be found here: 
prompt> ./mem
(2134) memory address of p: 00200000
(2134) p: 1
(2134) p: 2
(2134) p: 3
(2134) p: 4
(2134) p: 5
ˆC 

The program does a couple of things. First, it allocates some mem­ory (line a1). Then, it prints out the address of the memory (a2), and then puts the number zero into the .rst slot of the newly allocated memory (a3). Finally, it loops, delaying for a second and increment­ing the value stored at the address held in p. With every print state­ment, it also prints out what is called the process identi.er (the PID) of the running program. This PID is unique per running process. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
Again, this .rst result is not too interesting. The newly allocated 
memory is at address 00200000.As the program runs, it slowly 
updates the value and prints out the result. 
Now, we again run multiple instances of this same program to see what happens: 
prompt> ./mem &; ./mem &
[1] 24113
[2] 24114
(24113) memory address of p: 00200000
(24114) memory address of p: 00200000
(24113) p: 1
(24114) p: 1
(24114) p: 2
(24113) p: 2
(24113) p: 3
(24114) p: 3
(24113) p: 4
(24114) p: 4 
... 

We see from the example that each running program has allocated memory at the same address (00200000), and yet each seems to be updating the value at 00200000 independently! It is as if each run­ning program has its own private memory, instead of sharing the same physical memory with other running programs. 
Indeed, that is exactly what is happening here as the OS is vir­tualizing memory.Each process accesses its own private address space,which the OS somehow maps onto the physical memory of the machine. A memory reference within one running program does not affect the address space of other processes (or the OS itself); as far as the running program is concerned, it has physical memory all to itself. Exactly how all of this is accomplished is the subject of Part II of these notes. 

2.3 Concurrency 
Another main theme of this book is concurrency.We use this conceptual term to refer to a host of problems that arise, and must be addressed, when working on many things at once (i.e., concurrently) in the same program. The problems of concurrency arose .rst within the operating system itself; as you can see in the examples above on virtualization, the OS is juggling many things at once, .rst running 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
#include <stdio.h> 
#include <stdlib.h> 
#include "common.h" 

volatile int counter = 0; 
int loops; 

void *worker(void *arg) { 
int i; 
for (i = 0;i < loops; i++){ 

counter++; 
} 
return NULL; 

} 
int 
main(int argc, char *argv[])
{ 

if (argc != 2) {fprintf(stderr, "usage: threads <value>\n");exit(1);
}
loops = atoi(argv[1]);
pthread_t p1, p2;
printf("Initial value : %d\n", counter);
Pthread_create(&p1, NULL, worker, NULL);
Pthread_create(&p2, NULL, worker, NULL);
Pthread_join(p1, NULL);
Pthread_join(p2, NULL);
printf("Final value : %d\n", counter); 
return 0; 

} 
Figure 2.3: A Multithreaded program 
one process, then another, and so forth. As it turns out, doingso leads to some deep and interesting problems. 
Unfortunately, the problems of concurrency are no longer limited just to the OS itself. Indeed, modern multithreaded programs ex­hibit the same problems. Let us demonstrate with an example ofa multithreaded program (Figure 2.3). 
Although you might not understand this example fully at the mo­ment (and we’ll learn a lot more about it in later chapters, in the sec­tion of the book on concurrency), the basic idea is simple. Themain program creates two threads;you can think of a thread as a func-
OPERATING SYSTEMS ARPACI-DUSSEAU 
tion running within the same memory space as other functions,with more than one of them active at a time. In this example, each thread starts running in a routine called worker(),in which it simply in­crements a counter in a loop for loops number of times. 
Below is a transcript of what happens when we run this program with the input value for the variable loops set to 1000. The value of loops determines how many times each of the two workers will increment the shared counter in a loop. When the program is run with the value of loops set to 1000, what do you expect the .nal value of counter will be? 
prompt> gcc -o thread thread.c -Wall -lpthreadprompt> ./thread 1000Initial value : 0 Final value : 2000 
As you probably guessed, when the two threads are .nished, the .nal value of the counter is 2000, as each thread incremented the counter 1000 times. Indeed, when the input value of loops is set to N,we would expect the .nal output of the program to be 2N.But life is not so simple, as it turns out. Let’s run the same program, but with higher values for loops,and see what happens: 
prompt> ./thread 100000Initial value : 0 Final value : 143012 // huh?? prompt> ./thread 100000Initial value : 0 Final value : 137298 // what the?? 
In this run, when we gave an input value of 100,000, instead of getting a .nal value of 200,000, we instead .rst get 143,012. Then, when we run the program a second time, we not only again get the wrong value, but also a different value than the last time. In fact, if you run the program over and over with high values of loops,you may .nd that sometimes you even get the right answer! So why is this happening? 
As it turns out, the reason for these odd and unusual outcomes re­late to how instructions are executed, which is one at a time. Unfor­tunately, a key part of the program above, where the shared counter is incremented, takes three instructions: one to load the value of the counter from memory into a register, one to increment it, and one to 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
THE CRUX OF THE PROBLEM: 
HOW TO BUILD CORRECT CONCURRENT PROGRAMS 
When there are many concurrently executing threads within the 
same memory space, how can we build a correctly working pro­
gram? What primitives are needed from the OS? What mechanisms 
should be provided by the hardware? How can we use them to solve 
the problems of concurrency? 
store it back into memory. Because these three instructions do not ex­ecute atomically (all at once), strange things can happen, as we have seen. It is this problem of concurrency that we will address ingreat detail in the second part of this book. 

2.4 Persistence 
The third major theme of the course is persistence.In system memory, data can be easily lost, as devices such as DRAM store values in a volatile manner; when power goes away or the system crashes, any data in memory is lost. Thus, we need hardware and software to be able to store data persistently;such storage is thus critical to any system as users care a great deal about their data. 
The hardware comes in the form of some kind of input/output or I/O device; in modern systems, a hard drive is a common repository for long-lived information, although solid-state drives (SSDs)are making headway in this arena as well. 
The software in the operating system that usually manages the disk is called the .le system;it is thus responsible for storing any .les the user creates in a reliable and ef.cient manner on the disksof the system. 
Unlike the abstractions provided by the OS for the CPU and mem­ory, the OS does not create a private, virtualized disk for each appli­cation. Rather, it is assumed that often times, users will want to share information that is in .les. For example, when writing a C program, 
you might .rst use an editor (e.g., Emacs5)to create and edit the 
5
You should be using Emacs. If you are using vi, there is probably something wrong with you. If you are using something that is not a real code editor, that is even worse. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
#include <stdio.h> #include <unistd.h> #include <assert.h> #include <fcntl.h> #include <sys/types.h> 
int main(int argc, char *argv[]){ 
int fd = open("/tmp/file",O_WRONLY | O_CREAT | O_TRUNC,
S_IRWXU);assert(fd > -1);
int rc = write(fd, "hello world\n", 13);
assert(rc == 13);
close(fd); 
return 0; } 
Figure 2.4: A Program That Does I/O 
C.le (emacs -nw main.c). Once done, you might use the com­piler to turn the source code into an executable (e.g., gcc -o main main.c). When you’re .nished, you might run the new executable (e.g., ./main). Thus, you can see how .les are shared across differ­ent processes. First, Emacs creates a .le that is input to the compiler; the compiler uses that to create a new executable .le; .nally,the new executable is then run. And thus a new program is born! 
To understand this better, let’s once again look at some code.Fig­ure 2.4 presents code to create a .le called /tmp/file that contains the string “hello world”. 
To accomplish this task, the program makes three calls into the operating system. The .rst, a call to open(),opens the .le and cre­ates it; the second, write(),writes some data to the .le; the third, close(),simply closesthe .le thus indicating the program won’t be writing any more data to it. These system calls are routed to the part of the operating system called the .le system,which then handles the requests and returns some kind of error code to the user. 
You might be wondering what the OS does in order to actually write to disk. We would show you but you’d have to promise to close your eyes .rst; it is that unpleasant. As anyone who has writ-
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
THE CRUX OF THE PROBLEM: 
HOW TO STORE DATA PERSISTENTLY 
The .le system is the part of the OS in charge of managing persistent 
data. What techniques are needed to do so correctly? What mech­
anisms and policies are required to do so with high performance? 
How is reliability achieved, in the face of failures in hardware and 
software? 
ten a device driver6 knows, getting a device to do something on your behalf is an intricate and detailed process. It requires a deep knowl­edge of the low-level device interface and its exact semantics. Fortu­nately, the OS provides a standard and simple way to access devices through its system calls. Thus, the OS is sometimes seen as a stan­dard library. 
Of course, there are many more details in how devices are ac­
cessed, and how .le systems manage data persistently atop said de­
vices. For performance reasons, most .le systems .rst delay such 
writes for a while, hoping to batch them into larger groups forper­
formance reasons. To handle the problems of system crashes dur­
ing writes, most .le systems incorporate some kind of intricate write 
protocol, such as journaling or copy-on-write,carefully ordering 
writes to disk to ensure that if a failure occurs during the write se­
quence, the system can recover to reasonable state afterwards. To 
make different common operations ef.cient, .le systems employ many 
different data structures and access methods, from simple lists to 
complex b-trees. If all of this doesn’t make sense yet, good! We’ll 
be talking about all of this quite a bit more in the third part ofthis 
book, where we’ll discuss devices and I/O in general, and thendisks, 
RAIDs, and .le systems in great detail. 

2.5 Design Goals 
So now you have some idea of what an OS actually does: it takes 
physical resources,such as a CPU, memory, or disk, and virtualizes 
them. It handles tough and tricky issues related to concurrency. And 
6
Adevice driver is some code in the operating system that knowshow to deal with aspeci.c device. We will talk more about devices and device drivers later. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
it stores .les persistently,thus making them safe over the long-term. Given that we want to build such a system, we probably want to have some goals in mind to help focus our design and implementation and make trade-offs as necessary; .nding the right set of trade-offs is a key to building any system. 
One of the most basic goals is to build up some abstractions in or­der to make the system convenient and easy to use. Abstractions are fundamental to everything we do in computer science. Abstraction makes it possible to write a large program by dividing it into small and understandable pieces, to write such a program in a high-level 
language like C7 without thinking about assembly, to write code in assembly without thinking about logic gates, and to build a proces­sor out of gates without thinking too much about transistors.Ab­straction is so fundamental that sometimes we forget its importance, but we won’t here; thus, in each section, we’ll discuss some ofthe major abstractions that have developed over time, giving youa way to think about pieces of the OS. 
One goal in designing and implementing an operating system is to provide high performance;another way to say this is our goal is to minimize the overheads of the OS. Virtualization and making the system easy to use are well worth it, but not at any cost; thus, we must strive to provide virtualization and other OS features without excessive overheads. These overheads arise in a number of forms: extra time (more instructions) and extra space (in memory or on disk). We’ll seek solutions that minimize one or the other or both, if possible. 
Another goal will be to provide protection between applications, as well as between the OS and applications. Because we wish to allow many programs to run at the same time, we want to make sure that the malicious or accidental bad behavior of one does not harm others; we certainly don’t want an application to be able to harm the OS itself (as that would affect all programs running on the system). Protection is at the heart of one of the main principles underlying an operating system, which is that of isolation;isolating processes from one another is the key to protection and thus underlies much ofwhat an OS must do. 
The operating system must also run non-stop; when it fails, all 
7
Some of you might object to calling C a high-level language. Remember this is an OS course, though, where we’re happy not to code in assembly all the time! 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
applications running on the system fail as well. Because of this de­pendence, operating systems often strive to provide a high degree of reliability.As operating systems grow evermore complex (some­times containing millions of lines of code), building a reliable oper­ating system is quite a challenge – and indeed, much of the on-going research in the .eld (including some of our own work [BS+09,SS+10]) focuses on this exact problem. 
Other goals make sense: energy-ef.ciency is important in our in­creasingly green world; security (an extension of protection, really) against malicious applications is critical, especially in these highly­networked times; mobility is increasingly important as OSes are run on smaller and smaller devices. Depending in how the system is used, the OS will have different goals and thus likely be implemented in at least slightly different ways. However, as we will see, many of the principles we will present on how to build operating systems are useful in the range of different devices. 

2.6 Some History 
Before closing this introduction, let us present a brief history of how operating systems developed. Like any system built by hu­mans, good ideas accumulated in operating systems over time,as engineers learned what was important in their design. Here, we dis­cuss a few major developments. 
Early Operating Systems: Just Libraries 
In the beginning, the operating system didn’t do too much. Basically, it was just a set of libraries of commonly-used functions; forexample, instead of having each programmer of the system write low-level I/O handling code, the “OS” would provide such APIs, and thus make life easier for the developer. 
Usually, on these old mainframe systems, one program ran at a time, as controlled by a human operator. Much of what you think amodern OS would do (e.g., deciding whatorder to run jobs in) was performed by this operator. If you were a smart developer,you would be nice to this operator, so that they might move your jobto the front of the queue. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

Beyond Libraries: Protection 
In moving beyond being a simple library of commonly-used services, operating systems took on a more central role in managing machines. One important aspect of this was the realization that code runonbe­half of the OS was special; it had control of devices and thus should be treated differently than normal application code. Why is this? Well, imagine if you allowed any application to read from anywhere on the disk; the notion of privacy goes out the window, as any pro­gram could read any .le. Thus, implementing a .le system (to man­age your .les) as a library makes little sense. Instead, something else was needed. 
Thus, the idea of a system call was invented, pioneered by the At­las computing system [K+61,L78]. Instead of providing OS routines as a library (where you just make a procedure call to access them), the idea here was to add a special pair of hardware instructions and hardware state to make the transition into the OS a more formal, con­trolled process. 
HARDWARE SUPPORT:PROTECTED TRANSFER OF CONTROL 
The hardware assists the OS by providing different modes of ex­ecution. In user mode,applications do not have full access to hard­ware resources. In kernel mode,the OS has access to the full re­sources of the machine. Special instructions to trap into the ker­nel and return-from-trap back to user-mode programs are also pro­vided. We will see numerous cases of where a little hardware sup­port goes a long way in building an ef.cient, effective operating sys­tem. 
The key difference between a system call and a procedure call is that a system call transfers control (i.e., jumps) into the OSwhile simultaneously raising the hardware privilege level.User appli­cations run in what is referred to as user mode which means the hardware restricts what applications can do; for example, anappli­cation running in user mode can’t typically initiate an I/O request to the disk, access any physical memory page, or send a packet on the network. When a system call is initiated (usually througha spe­cial hardware instruction called a trap), the hardware transfers con­trol to a pre-speci.ed trap handler (that the OS set up previously) 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
and simultaneously raises the privilege level to kernel mode.In ker­nel mode, the OS has full access to the hardware of the system and thus can do things like initiate an I/O request or make more memory available to a program. When the OS is done servicing the request, it passes control back to the user via a special return-from-trap in­struction, which reverts to user mode while simultaneously passing control back to where the application left off. 

The Era of Multiprogramming 
Where operating systems really took off was in the era of computing beyond the mainframe, that of the minicomputer.Classic machines like the PDP family from Digital Equipment made computers hugely more affordable; thus, instead of having one mainframe per large or­ganization, now a smaller collection of people within an organization could likely have their own computer. Not surprisingly, one of the major impacts of this drop in cost was an increase in developeractiv­ity; more smart people got their hands on computers and thus made computer systems do more interesting and beautiful things. 
In particular, multiprogramming became commonplace as peo­ple wished to make better use of machine resources. Instead ofjust running one job at a time, the OS would load a number of jobs into memory and switch rapidly between them, thus improving CPU uti­lization. This switching was particularly important because I/O de­vices were slow; having a program wait on the CPU while its I/O was being serviced was a waste of CPU time. Instead, why not switch to another job and run it for a while? 
The desire to support multiprogramming and overlap in the pres­ence of I/O and interrupts forced innovation in the conceptual devel­opment of operating systems along a number of directions. Issues such as memory protection became important; we wouldn’t want one program to be able to access the memory of another program. Understanding how to deal with the concurrency issues introduced by multiprogramming was also critical; making sure the OS wasbe­having correctly despite the presence of interrupts is a great chal­lenge. We will study these issues and related topics later in the notes. 
One of the major practical advances of the time was the introduc­tion of the UNIX operating system, primarily thanks to Ken Thomp­son(andDennisRitchie)atBellLabs(yes,thephonecompany). UNIX took many good ideas from different operating systems (particularly 
OPERATING SYSTEMS ARPACI-DUSSEAU 
from Multics [O72]), but made them simpler and easier to use. Soon this team was shipping tapes containing UNIX source code to peo­ple around the world, many of whom then got involved and added to the system themselves. UNIX,quite simply, gave programmers a terri.c playground in which to develop applications and alsoto de­velop operating system ideas, and thus much of what we learn really starts with this one hugely important system. Interestingly, also in­vented by this same team (and including Brian Kernighan) was the Cprogramming language; thus, UNIX became one of the .rst oper­ating systems to be written (mostly) in a high-level language. 

The Modern Era 
Beyond the minicomputer came a new type of machine, cheaper, faster, and for the masses: the personal computer,or PC as we call it today. Led by Apple’s early machines (e.g., the Apple II) and the IBM PC, this new breed of machine would soon become the domi­nant force in computing, as their low-cost enabled one machine per desktop instead of a shared minicomputer per workgroup. 
Unfortunately, for operating systems, the PC at .rst represented a great leap backwards, as early systems forgot (or never knew of) the lessons learned in the era of minicomputers. For example, early op­erating systems such as DOS (the Disk Operating System, from Mi­crosoft) didn’t think memory protection was important; thus, a ma­licious (or poorly-programmed) application could scribbleall over memory. The .rst generations of the Mac OS (v9 and earlier) took a cooperative approach to job scheduling; thus, a thread that acciden­tally got stuck in an in.nite loop could take over the entire system, forcing a reboot. The painful list of OS features missing in this gen­eration of systems is long, too long for a full discussion here. 
Fortunately, after some years of suffering, the old featuresof mini­computer operating systems started to .nd their way onto the desk­top. For example, Mac OS X has UNIX at its core, including all of the features one would expect from such a mature system. Windows has similarly adopted many of the great ideas in computing history, start­ing in particular with Windows NT, a great leap forward in Microsoft OS technology. Even today’s cell phones run operating systems that are much more like what a minicomputer ran in the 1970s than what aPC ran in the 1980s (thank goodness); itis good to see that thegood ideas developed in the heyday of OS development have found their 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
way into the modern world. Even better is that these ideas continue to develop, providing more features and making modern systems even better for applications. 


2.7 Summary 
Thus, we have an introduction to the OS. Today’s operating sys­tems make systems relatively easy to use, and virtually all operating systems you use today have been in.uenced by the developments we will discuss throughout these notes. 
Unfortunately, due to time constraints, there are a number ofparts of the OS we won’t cover in these notes. For example, there is a lot of networking code in the operating system; we leave it to you to take the networking class to learn more about that. Similarly, graphics de­vices are particularly important; take the graphics course to expand your knowledge in that direction. Finally, some operating system books talk a great deal about security;we will do so in the sense that the OS must provide protection between running programs and give users the ability to protect their .les, but we won’t delve into deeper security issues that one might .nd in a security course. 
However, there are many important topics that we will cover, in­cluding the basics of virtualization of the CPU and memory, concur­rency, and persistence via devices and .le systems, learningabout other aspects of systems should be a relatively straightforward exer­cise. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

References 
[BS+09] “Tolerating File-System Mistakes with EnvyFS” 
Lakshmi N. Bairavasundaram, Swaminathan Sundararaman, Andrea C. Arpaci-Dusseau, 
Remzi H. Arpaci-Dusseau 
USENIX ’09, San Diego, CA, June 2009 

Afun paper about using multiple .le systems at once to tolerate a mistake in any one of them. 
[B75] “The Mythical Man-Month” Fred Brooks Addison-Wesley, 1975 
Aclassic text on software engineering; well worth the read. 
[BOH10] “Computer Systems: A Programmer’s Perspective” Randal E. Bryant and David R. O’Hallaron Addison-Wesley, 2010 
Another great intro to how computer systems work. Has a littlebit of overlap with this book –so if you’dlike, you can skipthe last few chapters of that book, or simply read them to get a different perspective on some of the same material. After all, one good way to build up your own knowledge is to hear as many other perspectives as possible, and then develop your own opinion and thoughts on the matter. 
[K+61] “One-Level Storage System” 
T. Kilburn, D.B.G. Edwards, M.J. Lanigan, F.H. Sumner IRE Transactions on Electronic Computers, April 1962 
The Atlas pioneered much of what you see in modern systems. However, this paper is not the best read. If you were to only read one, you might try the historicalperspective below [L78]. 
[L78] “The Manchester Mark I and Atlas: A Historical Perspective” 
S. H. Lavington Communications of the ACM archive Volume 21, Issue 1 (January 1978), pages 4-12 
Anice piece of history on the early development of computer systems and the pioneering efforts of the Atlas. Of course, one could go back and read the Atlas papers themselves, but this paper provides a great overview and adds some historical perspective. 
[O72] “The Multics System: An Examination of its Structure” Elliott Organick, 1972 
Agreat overview of Multics. So many good ideas, and yet it was an over-designed system, shooting for too much, and thus never really worked as expected. A classic example of what Fred Brooks would call the “second-system effect” [B75]. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
[PP03] “Introduction to Computing Systems: 
From Bits and Gates to C and Beyond” 
Yale N. Patt and Sanjay J. Patel 
McGraw-Hill, 2003 

One of our favorite intro to computing systems books. Starts at transistors and gets you all the way upto C. 
[RT74] “The UNIX Time-Sharing System” Dennis M. Ritchie and Ken Thompson CACM, Volume 17, Number 7, July 1974, pages 365-375 Agreat summary of UNIX written as it was taking over the world of computing, by the people who wrote it. 
[SS+10] “Membrane: Operating System Support for Restartable File Systems” Swaminathan Sundararaman, Sriram Subramanian, Abhishek Rajimwale, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau, MichaelM. Swift FAST ’10, San Jose, CA, February 2010 
The great thing about writing your own class notes: you can advertise your own research. But this paper is actually pretty neat – when a .le system hits a bugandcrashes, Membrane auto­magically restarts it, all without applications or the rest of the system being affected. 
OPERATING SYSTEMS ARPACI-DUSSEAU 


Part I 
Virtualization 

25 

ADialogueonVirtualization 
Professor: And thus we reach the .rst of our four pieces on operating 
systems: virtualization. 
Student: But what is virtualization, oh noble professor? 
Professor: Imagine we have a peach. 

Student: Apeach? (incredulous) 
Professor: Yes, a peach. Let us call that the physical peach. But we have 
many eaters who would like to eat this peach. What we would liketo present 
to each eater is their own peach, so that they can be happy. We call the peach 
we give eaters virtual peaches; we somehow create many of these virtual 
peaches out of the one physical peach. And the important thing: in this 
illusion, it looks to each eater like they have a physical peach, but in reality 
they don’t. 

Student: So you are sharing the peach, but you don’t even know it? 
Professor: Right! Exactly. 
Student: But there’s only one peach. 
Professor: Yes. And...? 
Student: Well, if I was sharing a peach with somebody else, I think I would 

notice. 

27 
Professor: Ah yes! Good point. But that is the thing with many eaters; most of the time they are napping or doing something else, and thus, you can snatch that peach away and give it to someone else for a while. And thus we create the illusion of many virtual peaches, one peach for each person! 
Student: Sounds like a bad campaign slogan. You are talking about com­puters, right Professor? 
Professor: Ah, young grasshopper, you wish to have a more concrete ex­ample. Good idea! Let us take the most basic of resources, the CPU. Assume there is one physical CPU in a system (though now there are often two or four or more). What virtualization does is take that single CPU and make it look like many virtual CPUs to the applications running on the system. Thus, while each applications thinks it has its own CPU to use,there is really only one. And thus the OS has created a beautiful illusion: it has virtualized the CPU. 
Student: Wow! That sounds like magic. Tell me more! How does that work? 
Professor: In time, young student, in good time. Sounds like you are ready to begin. 
Student: Iam! Well, sort of. 
Professor: Good! And thus we begin... 
OPERATING SYSTEMS ARPACI-DUSSEAU 

Abstraction: The Process 
In this note, we discuss one of the most fundamental abstractions that the OS provides to users: the process.The de.nition of a pro­cess, informally, is quite simple: it is a running program [V+65,B70]. The program itself is a lifeless thing: it just sits there on the disk, a bunch of instructions (and maybe some static data), waiting to spring into action. It is the operating system that takes these bytesandgets them running, transforming the program into something useful. 
It turns out that one often wants to run more than one program at once; for example, consider your desktop or laptop where you might like to run a web browser, mail program, a game, a music player,and so forth. In fact, a typical system may be seemingly running tens or even hundreds of processes at the same time. Doing so makes the system easy to use, as one never need be concerned with whethera CPU is available; one simply runs programs. Hence our challenge: 
THE CRUX OF THE PROBLEM: HOW TO PROVIDE THE ILLUSION OF MANY CPUS? Although there are only a few physical CPUs available, how can the OS provide the illusion of a nearly-endless supply of saidCPUs? 
The OS creates this illusion by virtualizing the CPU. By running one process, then stopping it and running another, and so forth, the OS can promote the illusion that many virtual CPUs exist when in fact there is only one physical CPU (or a few). This basic technique, known as time sharing of the CPU, allows users to run as many 
29 
TECHNIQUE:TIME SHARING (AND SPACE SHARING) 
Time sharing is one of the most basic techniques used by an OS to 
share a resource. By allowing the resource to be used for a little while 
by one entity, and then a little while by another, and so forth,the 
resource in question (e.g., the CPU, or a network link) can be shared 
by many. The natural counterpart of time sharing is space sharing, 
where a resource is divided (in space) among those who wish to use 
it. For example, disk space is naturally a space-shared resource, as 
once a block is assigned to a .le, it is not likely to be assignedto 
another .le until the user deletes it. 
concurrent processes as they would like; the potential cost is perfor­mance, as each will run more slowly if the CPU(s) must be shared. 
To implement virtualization of the CPU, and to implement it well, the OS will need both some low-level machinery as well as some high-level intelligence. We call the low-level machinery mechanisms; mechanisms are low-level methods or protocols that implement a needed piece of functionality. For example, we’ll learn below how to implement a context switch,which gives the OS the ability to stop running one program and start running another on a given CPU; this time-sharing mechanism is employed by all modern OSes. 
On top of these mechanisms resides some of the intelligence in the OS, in the form of policies.Policies are algorithms for making some kind of decision within the OS. For example, given a number of possible programs to run on a CPU, which program should the OS run? A scheduling policy in the OS will make this decision, likely using historical information (e.g., which program has run more over the last minute?), workload knowledge (e.g., what types of programs are run), and performance metrics (e.g., is the system optimizing for interactive performance, or throughput?) to make its decision. 
4.1 The Abstraction: A Process 
The abstraction provided by the OS of a running program is some­thing we will call a process.As we said above, a process is simply arunning program; atany instant in time, we can summarize apro­cess by taking an inventory of the different pieces of the system it accesses or affects during the course of its execution. 
OPERATING SYSTEMS ARPACI-DUSSEAU DESIGN TIP:SEPARATION OF POLICY/MECHANISM 
In many operating systems, a common design paradigm is to sepa­
rate high-level policies from their low-level mechanisms [L+75]. You 
can think of the mechanism as providing the answer to a how ques­
tion about a system; for example, how does an operating system per­
form a context switch? The policy provides the answer to a which 
question; for example, which process should the operating system 
run right now? Separating the two allows one easily to change poli­
cies without having to rethink the mechanism and is thus a formof 
modularity,a general software design principle. 
To understand what constitutes a process, we thus have to under­stand its machine state:what a program can read or update when it is running. At any given time, what parts of the machine are impor­tant to the execution of this program? 
One obvious component of machine state that comprises a pro­cess is its memory.All instructions lie in memory; the data that the running program reads and updates sits in memory as well. Thusthe memory that the process can address (sometimes called its address space)is part of the process. 
Also part of the process’s machine state are registers;many in­
structions explicitly read or update registers and thus clearly they 
are important to the execution of the process. 
Note that there are some particularly special registers thatform part of this machine state. For example, the program counter (PC) (sometimes called the instruction pointer or IP)tells us which in­struction of the program is currently being executed; similarly a stack pointer and associated frame pointer are used to manage the stack for function parameters, local variables, and return addresses. 
Finally, programs often access persistent storage devices too. Such 
I/O information might include a list of the .les the process currently 
has open. 

4.2 Process API 
Though we defer discussion of a real process API until a subse­
quent chapter, here we .rst give some idea of what must be included 
in any interface of an operating system. 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
• 	
Create: An operating system must include some method to cre­ate new processes. When you type something at a shell, or double-click on an application icon, the OS is invoked to create anew process to run the program you have indicated. 

• 	
Destroy: As there is an interface for process creation, systems also provide an interface to destroy processes forcefully. Of course, many processes will run and just exit by themselves when complete; when they don’t, however, the user may wish to kill them, and thus an interface to halt a runaway process is quite useful. 

• 	
Wait: Sometimes it is useful to wait for a process to stop run­ning; thus some kind of waiting interface is often provided. 

• 	
Miscellaneous Control: Other than killing or waiting for a process, there are sometimes other controls that are possible. For example, most operating systems provide some kind of method to suspend a process (stop it from running for a while) and then resume it (continue it running). 

• 	
Status: There are usually interfaces to get some status informa­tion about a process as well, such as how long it has run for, or what state it is in. 



4.3 Process States 
Now that we have some idea of what a process is (though we will continue to re.ne this notion), let us talk about the different states a process can be in at a given time. The notion that a process can be in one of these states arose in early computer systems [V+65,DV66]. 
In a simpli.ed view, a process can be in one of three states: 
• 	
Running:In the running state, a process is running on a pro­cessor. This means it is executing instructions. 

• 	
Ready:In the ready state, a process is ready to run but for some reason the OS has chosen not to run it at this given moment. 

• 	
Blocked:In the blocked state, a process has performed some kind of operation that makes it not ready to run until some other event takes place. A common example: when a process initiates an I/O request to a disk, it becomes blocked and thus some other process can use the processor. 


OPERATING SYSTEMS ARPACI-DUSSEAU 

Descheduled 
Scheduled 
I/O: initiate I/O: done 
Figure 4.1: Process: State Transitions 
If we were to map these states to a graph, we would arrive at the diagram in Figure 4.1. As you can see in the diagram, a process can be moved between the ready and running states at the discretion of the OS. Being moved from ready to running means the process has been scheduled;being moved from running to ready means the process has been descheduled.Once a process has become blocked (e.g., by initiating an I/O operation), the OS will keep it as such until some event occurs (e.g., I/O completion); at that point, the process moves to the ready state again (and potentially immediately to run­ning again, if the OS so decides). 

4.4 Data Structures 
The OS is a program, and like any program, it has some key data structures that track various relevant pieces of information. To track the state of each process, for example, the OS likely will keepsome kind of process list for all processes that are ready, as well as some additional information to track which process is currently running. The OS must also track, in some way, blocked processes; when an I/O event completes, the OS should make sure to wake the correct process and make it ready to run again. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
DATA STRUCTURE:THE PROCESS LIST 
Operating systems are replete with various important data struc­
tures that we will discuss in these notes. The process list is the .rst 
such structure. It is one of the simpler ones, but certainly any OS 
that has the ability to run multiple programs at once will havesome­
thing akin to this structure in order to keep track of all the running 
programs in the system. Sometimes people refer to the individual 
structure that stores information about a process as a Process Con­
trol Block (PCB). 
Figure 4.2 shows what type of information an OS needs to track about each process in the xv6 kernel [CK+08]. Similar processstruc­tures exist in “real” operating systems such as Linux, Mac OS X, or Windows; look them up and see how much more complex they are. 
From the .gure, you can see a couple of important pieces of in­formation the OS tracks about a process. The register context will hold, for a stopped process, the contents of its register state. When a process is stopped, its register state will be saved to this memory lo­cation; by restoring these registers (i.e., placing their values back into the actual physical registers), the OS can resume running theprocess. We’ll learn more about this technique known as a context switch in future chapters. 
You can also see from the .gure that there are some other statesa process can be in, beyond running, ready, and blocked. Sometimes a system will have an initial state that the process is in when it is being created. Also, a process could be placed in a .nal state where it has exited but has not yet been cleaned up (in UNIX-based systems,this 
is called the zombie state1). This .nal state can be useful as it allows other processes (usually the parent that created the process) to exam­ine the return code of the process and see if it executed successfully. When .nished, the parent will then make one .nal call to indicate to the OS that it can completely forget about the now-extinct process (the UNIX wait() system call does this). 
1
Yes, the zombie state. Just like real zombies, these zombies are relatively easy to kill. However, different techniques are usually recommended. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
// the registers xv6 will save and restore // to stop and subsequently restart a process struct context {int eip;int esp;int ebx; int ecx; int edx; int esi; int edi; int ebp;};  
// the different states a process can be in enum proc_state { UNUSED, EMBRYO, SLEEPING,RUNNABLE, RUNNING, ZOMBIE };  
// the information xv6 tracks about each process// including its register context and state struct proc { char *mem; // Start of process memory uint sz; // Size of process memory char *kstack; // Bottom of kernel stack// for this process enum proc_state state; // Process state int pid; // Process ID struct proc *parent; // Parent process void *chan; // If non-zero, sleeping on chan int killed; // If non-zero, have been killed struct file *ofile[NOFILE]; // Open files struct inode *cwd; // Current directory struct context context; // Switch here to run process struct trapframe *tf; // Trap frame for the// current interrupt};  
Figure 4.2: The xv6 Proc Structure  
4.5  Summary  
We have introduced the most basic abstraction of the OS: the pro­cess. It is quite simply viewed as a running program. With this conceptual view in mind, we will now move on to the nitty-gritty: the low-level mechanisms needed to implement processes, andthe higher-level policies required to schedule them in an intelligent way. By combining mechanisms and policies, we will build up our under­standing of how an operating system virtualizes the CPU.  
ARPACI-DUSSEAU  THREE EASY PIECES (V0.5)  



References 
[CK+08] “The xv6 Operating System” Russ Cox, Frans Kaashoek, Robert Morris, Nickolai Zeldovich From: http://pdos.csail.mit.edu/6.828/2008/index.html 
[DV66] “Programming Semantics for Multiprogrammed Computations” Jack B. Dennis and Earl C. Van Horn Communications of the ACM, Volume 9, Number 3, March 1966 
This paper de.ned many of the early terms and concepts around building multiprogrammed sys­tems. 
[H70] “The Nucleus of a Multiprogramming System” Per Brinch Hansen Communications of the ACM, Volume 13, Number 4, April 1970 
This paper introduces one of the .rst microkernels in operating systems history, called Nucleus. The idea of smaller, more minimal systems is a theme that rearsits head repeatedly in OS history; it all began with Brinch Hansen’s work described herein. 
[L+75] “Policy/mechanism separation in Hydra” 
R. Levin, E. Cohen, W. Corwin, F. Pollack, W. Wulf. SOSP 1975. 
[V+65] “Structure of the Multics Supervisor” 
V.A. Vyssotsky, F. J. Corbato, R. M. Graham 
Fall Joint Computer Conference, 1965 

An early paper on Multics, which described many of the basic ideas and terms that we .nd in modern systems. 
OPERATING SYSTEMS ARPACI-DUSSEAU 


Interlude: Process API 
In this interlude, we discuss process creation in UNIX systems. UNIX presents one of the most intriguing ways to create a new process with a pair of system calls: fork() and exec().A third routine, wait(),can be usedby a process wishing to wait for a process it has created to complete. We now present these interfaces in more detail, with a few simple examples to motivate us. 
ASIDE:INTERLUDES 
Interludes will cover more practical aspects of systems, including aparticular focus on operating system APIs and how to use them. If you don’t like practical things, you could skip these interludes. But you should like practical things, because, well, they are generally useful in real life. 
5.1 The fork() System Call 
The fork() system call is used to create a new process [C63]. 
However, be forewarned: it is certainly the strangest routine you will ever call 1.More speci.cally, you have a running program whose code looks like what you see in Figure 5.1. 
1Well, OK, we admit that we don’t know that for sure; who knows what routines you call when no one is looking? But fork() is pretty odd. 
37 
#include <stdio.h> 
#include <stdlib.h> 
#include <unistd.h> 

int 
main(int argc, char *argv[])
{ 

printf("hello world (pid:%d)\n", (int) getpid());
int rc = fork();
if (rc < 0){

// fork failed; exitfprintf(stderr, "fork failed\n");exit(1);
}else if(rc ==0) {// child (new process)printf("hello, I am child (pid:%d)\n", (int) getpid()); 
}else {// parent goes down this path (original process)printf("hello, I am parent of %d (pid:%d)\n",
rc, (int) getpid()); } return 0; 
} 
Figure 5.1: The code for p1.c 
When you run this program (called p1.c), what you see is the following: 
prompt> ./p1
hello world (pid:29146)
hello, I am parent of 29147 (pid:29146) 
hello, I am child (pid:29147) 
prompt> 

Let us understand what happened in more detail in p1.c.When it .rst started running, the process prints out a hello world message; included in that message is its process identi.er,also known as a PID.TheprocesshasaPIDof29146;in UNIX systems,thePIDisused to name the process if one wants to do something with the process, such as (for example) stop it from running. So far, so good. 
Now the interesting part begins. The process calls the fork() system call, which the OS provides as a way to create a new process. The odd part: the process that is created is an (almost) exact copy of the calling process.That means that to the OS, it now looks like there 
OPERATING SYSTEMS ARPACI-DUSSEAU 
#include <stdio.h> #include <stdlib.h> #include <unistd.h> #include <sys/wait.h>  
int main(int argc, char *argv[]){ printf("hello world (pid:%d)\n", (int) getpid());int rc = fork();if (rc < 0) {// fork failed; exitfprintf(stderr, "fork failed\n");exit(1);}else if(rc ==0){// child (new process)printf("hello, I am child (pid:%d)\n", (int) getpid()); }else {// parent goes down this path (original process)int wc = wait(NULL);printf("hello, I am parent of %d (wc:%d) (pid:%d)\n",rc, wc, (int) getpid()); } return 0; }  
Figure 5.2: The code for p2.c  
are two copies of the program p1 running, and both are about to re­turn from the fork() system call. The newly-created process (called the child,in contrast to the creating parent)doesn’t start running at main(),like you might expect (note, the “hello, world” message only got printed out once); rather, it just comes into life as if it had called fork() itself. You might have noticed: the child isn’t an exact copy. Speci.­cally, although it now has its own copy of the address space (i.e., its own private memory), its own registers, its own PC, and so forth, the value it returns to the caller of fork() is different. Speci.cally, while the parent receives the PID of the newly-created child, the child is simply returned a 0. This differentiation is useful, becauseit is sim­ple then to write the code that handles the two different cases(as above). You might also have noticed: the output is not deterministic. When the child process is created, there are now two active processes in the system that we care about: the parent and the child. Assuming  
THREE  
ARPACI-DUSSEAU  EASY PIECES  
(V0.5)  

we are running on a system with a single CPU (for simplicity), then either the child or the parent might run at that point. In our exam­ple (above), the parent did and thus printed out its message .rst. In other cases (not shown), the opposite might happen. We’ll seea lot more of this type of non-determinism when we study concurrency in the future. 

5.2 Adding wait() System Call 
So far, we haven’t done much: just created a child that prints out amessage and exits. Sometimes, as it turns out, it is quite useful for aparent to waitfor achild process to .nish whatithas been doing. This task is accomplished with the wait() system call (or its more complete sibling waitpid()); see Figure 5.2. 
In this example (p2.c), the parent process calls wait() to delay 
its execution until the child .nishes executing. When the child is 
done, wait() returns to the parent. 
Adding a wait() call to the code above makes the output deter­
ministic. Can you see why? Go ahead, think about it. Now that you have thought a bit, here is the output: 
prompt> ./p2
hello world (pid:29266)
hello, I am child (pid:29267) 
hello, I am parent of 29267 (wc:29267) (pid:29266) 
prompt> 

With this code, we now know that the child will always print .rst. Why? Well, it might simply run .rst, as before, and thus print before the parent. However, if the parent does run .rst, it will immediately call wait();this system call won’t return until the childhas run and 
exited 2.Thus, even when the parent runs .rst, it politely waits for the child to .nish running, then wait() returns, and then the parent prints its message. 
2
There are a few cases where wait() returns before the child exits; read the man page for more details, as always. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
#include <stdio.h> #include <stdlib.h> #include <unistd.h> #include <string.h>#include <sys/wait.h> int main(int argc, char *argv[]){ printf("hello world (pid:%d)\n", (int) getpid());int rc = fork();if (rc < 0) {// fork failed; exitfprintf(stderr, "fork failed\n");exit(1);}else if(rc ==0){// child (new process)printf("hello, I am child (pid:%d)\n", (int) getpid()); char *myargs[3];myargs[0] = strdup("wc"); // program: "wc" (word count) myargs[1] = strdup("p3.c"); // argument: file to count myargs[2] = NULL; // marks end of array execvp(myargs[0], myargs); // runs word count printf("this shouldn’t print out");}else {// parent goes down this path (original process)int wc = wait(NULL);printf("hello, I am parent of %d (wc:%d) (pid:%d)\n",rc, wc, (int) getpid()); } return 0; }  
Figure 5.3: The code for p3.c  
5.3  Finally, the exec() System Call  
A.nal and important piece of the process creation API is the exec() system call3.This system call is useful when you want to run a program that is different from the calling program. For ex­ample, calling fork() in p2.c is only useful if you want to keep  
3Actually, there are six variants of exec(): execl(), execle(), execlp(), execv(),and execvp().Read the man pages to learn more.  
ARPACI-DUSSEAU  THREE EASY PIECES (V0.5)  

running copies of the same program. However, sometimes you want to runa different program; exec() does just that (Figure 5.3). 
In this example, the child process calls execvp() in order to run the program wc,which is the wordcounting program. In fact, it runs wc on the source .le p3.c,thus telling ushow many lines, words, and bytes are found in the .le: 
prompt> ./p3
hello world (pid:29383)
hello, I am child (pid:29384) 

29 107 1030 p3.c 
hello, I am parent of 29384 (wc:29384) (pid:29383) 
prompt> 
If fork() was strange, exec() is not so normal either. What it does: given the name of an executable (e.g., wc), and some arguments (e.g., p3.c), it takes the code from that executable and overwrites its current code segment with it; the heap and stack and other parts of the memory space of the program are reinitialized. Then the OSsim­ply runs that program, passing in any arguments as the argv of that process. Thus, it does not create a new process; rather, it transforms the currently running program (formerly p3)into a different running program (wc). After the exec() in the child, it is almost as if p3.c never ran; a successful call to exec() never returns. 

5.4 Why? Motivating the API 
Of course, one big question you might have: why would we build such an odd interface to what should be the simple act of creating a new process? Well, as it turns out, the separation of fork() and exec() is essential in building a UNIX shell. 
The shell is just a program that is running 4.You type a command (i.e., the name of an executable program, plus any arguments)to it; it .gures out where the executable is, calls fork() to create a new child process to run the command, calls some variant of exec() to run the command, and then waits for the command to complete by calling wait().When the child completes, the shell returns from 
4
And there are lots of shells; tcsh, bash,and zsh to name a few. You should pick one, read its man pages, and learn more about it; all UNIX experts do. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
wait() and prints out a prompt again, ready for your next com­
mand. The separation of fork() and exec() allows the shell to do a 
whole bunch of really cool things rather easily. For example: 
prompt> wc p3.c > newfile.txt 
In the example above, the output of the program wc is redirected into the output .le newfile.txt.The way the shell accomplishes this is quite simple: when the child is created, before calling exec(), the shell closes standard output and opens the .le newfile.txt. By doing so, any print outs from the soon to be running program wc are redirected to the .le instead of the screen. UNIX pipes are imple­mented in a similar way but with the pipe() system call. There is a lot more detail there to be learned and understood; for now, suf.ce it to say that the fork()/exec() combination is a very powerful way to create and manipulate processes. 

5.5 Other Parts of the API 
Beyond fork(), exec(),and wait(),there are a lot of other interfaces for interacting with processes in UNIX systems. For ex­ample, the kill() system call is used to send signals to a process, including directives to go to sleep, die, and other useful things you might want to do. In fact, the entire signals subsystem provides quite arich way to deliver external events to processes, includingways for processes to receive and process those signals. 
There are many command-line tools that are useful as well. For example, using the ps command allows you to see which processes are running; read the man pages for some useful .ags to pass to ps. The tool top is also quite helpful, as it displays the processes of the system and how much CPU and other resources they are eating up. Humorously, many times when you run it, top claims it is the top resource hog; perhaps it is a bit of an egomaniac. Finally, there are many different kinds of CPU meters you can use to get a quick glance understanding of the load on your system; for example, we always keep MenuMeters (from Raging Menace software) running on our Macintosh toolbars, so we can see how much CPU is being utilized at any moment in time. In general, the more information about what is going on, the better. 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 

5.6 Summary 
We have introduced some of the APIs dealing with UNIX process creation: fork(), exec(),and wait().However, we have just skimmed the surface. For more detail, read Stevens [S92], of course, particularly chapters 8, 9, and 10 on Process Control, Process Rela­tionships, and Signals. There is much to extract from the wisdom therein. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

References 
[C63] “A Multiprocessor System Design” Melvin E. Conway AFIPS ’63 Fall Joint Computer Conference New York, USA 1963 
An early paper on how to design multiprocessing systems; may be the .rst place the term fork() was used in the discussion of spawning new processes. 
[DV66] “Programming Semantics for Multiprogrammed Computations” Jack B. Dennis and Earl C. Van Horn Communications of the ACM, Volume 9, Number 3, March 1966 
Aclassic paper that outlines the basics of multiprogrammed computer systems. Undoubtedly had great in.uence on Project MAC, Multics, and eventually UNIX. 
[S92] “Advanced Programming in the UNIX Environment” 
W. Richard Stevens and Stephen A. Rago Addison-Wesley, 1992 All nuances and subtleties of using UNIX APIs are found herein. Buy this book! Read it! And most importantly, live it. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 


Mechanism: Limited Direct Execution 
In order to virtualize the CPU, the operating system needs to some­how share the physical CPU among many jobs running seemingly at the same time. The basic idea is simple: run one process for a little while, then run another one, and so forth. By time sharing the CPU in this manner, virtualization is achieved. 
There are a few challenges, however, in building such virtualiza­tion machinery. The .rst is performance:how can we implement vir­tualization without adding excessive overhead to the system? The second is control:how can we run processes ef.ciently while retain­ing control over the CPU? Control is particularly important to the OS, as it is in charge of resources; without it, a process couldsimply run forever and take over the machine, or access information that it shouldn’t be allowed to access. Attaining performance whilemain­taining control is thus one of the central challenges in building an operating system. 
THE CRUX: 
HOW TO EFFICIENTLY VIRTUALIZE THE CPU WITH CONTROL 
The OS must virtualize the CPU in an ef.cient manner, but while retaining control over the system. To do so, both hardware andop­erating systems support will be required. The OS will often use a judicious bit of hardware support in order to accomplish its work effectively. 
47 
6.1 Basic Technique: Limited Direct Execution 
To make a program run as fast as one might expect, not surpris­ingly OS developers came up with a simple technique, which we call limited direct execution.The “direct execution” part of the idea is simple: just run the program directly on the CPU. Thus, when the OS wishes to start a program running, it just locates its entrypoint (i.e., the main() routine or something similar), jumps to it, and starts running the user’s code. 
Sounds simple, no? But it does give rise to a few problems in our quest to virtualize the CPU. The .rst is simple: if we just run apro­gram, how can the OS make sure the program doesn’t do anything that we don’t want it to do, while still running it ef.ciently?The sec­ond: when we are running a process, how does the operating system stop it from running and switch to another process, thus implement­ing the time sharing we require to virtualize the CPU? In answering these questions below, you should get a much better sense of what is needed to virtualize the CPU. In developing these techniques, you’ll also see where the “limited” part of the name arises from. 

6.2 Problem #1: Restricted Operations 
Direct execution has the obvious advantage of being fast; thepro­
gram runs natively on the hardware CPU and thus executes as quickly 
as one would expect. But running on the CPU introduces a problem: 
what if the process wishes to perform some kind of restricted opera­
tion, such as issuing an I/O request to a disk? 
THE CRUX:HOW TO PERFORM RESTRICTED OPERATIONS 
Aprocess must be able to perform I/O and some other restricted 
operations, but without giving the process complete controlover the 
system. How can the OS and hardware work together to do so? 
One approach would simply be to let any process do whatever it wants in terms of I/O and other related operations. However, doing so would prevent the construction of many kinds of systems that are desirable. For example, if we wish to build a .le system that checks permissions before granting access to a .le, we can’t simply let any 
OPERATING SYSTEMS ARPACI-DUSSEAU 
user process issue I/Os to the disk; if we did, a process could simply read the entire disk and thus all protections would be lost. 
Thus, the approach we take is to introduce a new processor mode, known as user mode;any code that runsin user mode is restricted in what it can do. For example, when running in user mode, a process can’t issue any I/O requests; doing so would result in the processor raising an exception; the OS would then likely kill the process. 
In contrast to user mode is kernel mode,which the operating sys­tem (or kernel) runs in. In this mode, code that runs can do whatit likes, including privileged operations such as issuing I/O requests and executing all types of restricted instructions. 
We are still left with a challenge, however: what should a user process do when it wishes to perform some kind of privileged oper­ation, such as reading from disk? To enable this, virtually all modern hardware provides the ability for user programs to perform a system call.Pioneered on ancient machines such as the Atlas [K+61,L78], system calls allow the kernel to carefully expose certain keypieces of functionality to user programs, such as accessing the .le system, creating and destroying processes, communicating with other pro­cesses, and allocating more memory. Most operating systems expose afew hundred such operations (see the POSIX standard for details on what modern Unix systems expose [P10]); early Unix systemsex­posed a much more concise subset of around twenty calls). 
To execute a system call, a program must execute a special trap instruction. This instruction simultaneously jumps into the kernel and raises the privilege level to kernel mode; once in the kernel, the system can now perform whatever privileged operations are needed (if allowed), and thus do the required work for the calling process. When .nished, the OS calls a special return-from-trap instruction, which, as you might expect, returns into the calling user program while simultaneously reducing the privilege level back to user mode. 
The hardware needs to be a bit careful when executing a trap, in that it must make sure to save enough of the caller’s register state in order to be able to return correctly when the OS issues the return-from-trap instruction. On x86, for example, the processor will push the program counter, .ags, and a few other registers onto a stack; the return-from-trap will pop these values off the stack and resume execution of the user-mode program (see [I11] for details). Other hardware systems use different conventions, but the basic concepts are similar across platforms. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
HARDWARE SUPPORT:PROTECTED TRANSFER OF CONTROL 
The hardware assists the OS by providing different modes of execu­
tion. In user mode,applications do not have full accessto hardware 
resources. In kernel mode,the OS hasaccessto the full resources of 
the machine. Special instructions to trap into the kernel and return-
from-trap back to user-mode programs are also provided, as well 
instructions that allow the OS to tell the hardware where the trap 
table resides in memory. 
There is one important detail left out of this discussion: howdoes the trap know which code to run inside the OS? Clearly, the call­ing process can’t specify an address to jump to (as you would when making a procedure call); this would allow programs to jump any­where into the kernel which clearly is a bad idea (imagine jumping into code to access a .le, but just after a permission check). Thus the kernel must carefully control what code executes upon a trap. 
The kernel does so by setting up a trap table at boot time. When the machine boots up, it does so in privileged (kernel) mode, and thus is free to con.gure machine hardware as need be. One of the .rst things the OS thus does is to tell the hardware what code to run when certain exceptional events occur. For example, whatcode should run when a hard-disk interrupt takes place, when a keyboard interrupt occurs, or when program makes a system call? The OS informs the hardware of the locations of these trap handlers,usu­ally with some kind of special instruction. Once the hardwareis in­formed, it remembers the location of these handlers until thema­chine is next rebooted, and thus the hardware knows what to do (i.e., what code to jump to) when system calls and other exceptional events take place. 
One last aside: being able to execute the instruction to tell the hardware where the trap tables are is a very powerful capability. Thus, as you might have guessed, it is also a privileged operation. If you try to execute this instruction in user mode, the kernelwon’t let you, and you can probably guess what will happen (hint: adios, offending program). What you might think about: what types ofhor­rible things could you do to a system if you could install your own trap table? 
OPERATING SYSTEMS ARPACI-DUSSEAU ASIDE:WHY SYSTEM CALLS LOOK LIKE PROCEDURE CALLS 
You may wonder why a call to a system call, such as open() or read(),looks exactly like a typical procedure call in C; that is, if it looks just like a procedure call, how does the system know it’s a system call, and do all the right stuff? The simple reason: it is a procedure call, but hidden inside that procedure call is the famous trap instruction. More speci.cally, when you call open() (for exam­ple), you are executing a procedure call into the C library. Therein, whether for open() or any of the other system calls provided, the li­brary uses an agreed-upon calling convention with the kernelto put the arguments to open in well-known locations (e.g., on the stack, or in speci.c registers), puts the system-call number into a well-known location as well (again, onto the stack or a register), and then exe­cutes the aforementioned trap instruction. The code in the library after the trap unpacks return values and returns control to the pro­gram that issued the system call. Thus, the parts of the C library that make system calls are hand-coded in assembly, as they need to carefully follow convention in order to process arguments and re­turn values correctly, as well as execute the hardware-speci.c trap instruction. And now you know why you personally don’t have to write assembly code to trap into an OS; somebody has already writ­ten that assembly for you. 

6.3 Problem #2: Switching Between Processes 
The next problem with direct execution is achieving a switch be­tween processes. Switching between processes should be simple, right? The OS should just decide to stop one process and start an­other. What’s the big deal? But it actually is a little bit tricky: speci.­cally, if a process is running on the CPU, this by de.nition means the OS is not running. If the OS is not running, how can it do anything at all? (hint: it can’t) While this sounds almost philosophical, it is a real problem: there is clearly no way for the OS to take an action if it is not running on the CPU. Thus we arrive at the crux of the problem. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
THE CRUX:HOW TO REGAIN CONTROL OF THE CPU How can the operating system regain control of the CPU so that it can switch between processes? 
ACooperative Approach: Wait For System Calls 
One approach that some systems have taken in the past (for exam­ple, early versions of the Macintosh operating system [M11],or the old Xerox Alto system [A79]) is known as the cooperative approach. In this style, the OS trusts the processes of the system to behave rea­sonably. Processes that run for too long are assumed to periodically give up the CPU so that the OS can decide to run some other task. 
TECHNIQUE:DEALING WITH MISBEHAVIOR 
Operating systems often have to deal with misbehaving processes, 
those that either through design (maliciousness) or accident (bugs) 
attempt to do something that they shouldn’t. In modern systems, the 
way the OS tries to handle such malfeasance is to simply terminate 
the offender. One strike and you’re out! Perhaps a little brutal, but 
what else should the OS do when you try to access memory illegally 
or execute an illegal instruction? 
Thus, you might ask, how does a friendly process give up the CPU in this utopian world? Most processes, as it turns out, transfer control of the CPU to the OS quite frequently by making system calls, for example, to open a .le and subsequently read it, or to send a message to another machine, or to create a new process. Systems like this often include an explicit yield system call, which does nothing except to transfer control to the OS so it can run other processes. 
Applications also transfer control to the OS when they do some­thing illegal. For example, if an application divides by zero, or tries to access memory that it shouldn’t be able to access, it will generate a trap to the OS. The OS will then have control of the CPU again (and likely terminate the offending process). 
Thus, in a cooperative scheduling system, the OS regains control 
of the CPU by waiting for a system call or an illegal operation of some 
kind to take place. You might also be thinking: isn’t this passive 
OPERATING SYSTEMS ARPACI-DUSSEAU 
approach less than ideal? What happens, for example, if a process (whether malicious, or just full of bugs) ends up in an in.niteloop, and never makes a system call? What can the OS do then? 

ANon-Cooperative Approach: The OS Takes Control 
Without some additional help from the hardware, it turns out the OS can’t do much at all when a process refuses to make system calls(or mistakes) and thus return control to the OS. In fact, in the cooperative approach, your only recourse when a process gets stuck in an in.nite loop is to resort to the age-old solution to all problems in computer systems: reboot the machine.Thus, we again arrive at a subproblem of our general quest to gain control of the CPU. 
THE CRUX:HOW TO GAIN CONTROL WITHOUT COOPERATION 
How can the OS gain control of the CPU even if processes are not being cooperative? 
The answer turns out to be simple and was discovered by a num­ber of people building computer systems many years ago: a timer interrupt [M+63]. A timer device can be programmed to raise an in­terrupt every so many milliseconds; when the interrupt is raised, the currently running process is halted, and a precon.gured interrupt handler in the OS runs. At this point, the OS has regained control of the CPU, and thus can do what it pleases: stop the current process from running, and start a new one running. 
As we discussed before with system calls, the OS must inform the hardware of which code to run when the timer interrupt occurs; thus, at boot time, the OS does exactly that. Second, also during the boot sequence, the OS must start the timer, which is of course aprivileged operation. Once the timer has begun, the OS can thus feel safe in that control will eventually be returned to it, and thus the OS is free to run user programs. The timer can also be turned off (also a privileged operation), something we will discuss later when we understand concurrency in more detail. 
Note that the hardware has some responsibility when an inter­rupt occurs, in particular to save enough of the state of the program that was running when the interrupt occurred such that a subsequent 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
HARDWARE SUPPORT:THE TIMER INTERRUPT 
The addition of a timer interrupt gives the OS the ability to regain 
control of the CPU even if processes act in a non-cooperative fashion. 
Thus, this hardware is key in helping the OS maintain control of the 
system. 
return-from-trap instruction will be able to resume the running pro­gram correctly. This set of actions is quite similar to the behavior of the hardware during an explicit system-call trap into the kernel, with various registers thus getting saved (e.g., onto a kernel stack) and thus easily restored by the return-from-trap instruction. 

Saving and Restoring Context 
Now that the OS has regained control, whether cooperatively via a system call, or more forcefully via a timer interrupt, a decision has to be made: whether to continue running the currently-running pro­cess, or switch to a different one. This decision is made by a part of the operating system known as the scheduler,and we will discuss scheduling policies in great detail in the next few chapters. 
If the decision is made to switch, the OS then executes a low-level piece of code which we refer to as a context switch.A context switch is conceptually simple: all the OS has to do is save a few register values for the currently-executing process and restore a fewfor the soon-to-be-executing process. By doing so, the OS thus ensures that when the return-from-trap instruction is .nally executed, instead of returning to the process that was running, the system resumesexe­cution of another process. 
To save the context of the currently-running process, the OS will execute some low-level assembly code to save the general purpose registers, PC, as well as the kernel stack pointer of the currently-running process, and then restore said registers, PC, and switch to the kernel stack for the soon-to-be-executing process. By switch­ing stacks, the kernel enters the call to the switch code in thecon­text of one process (the one that was interrupted) and returnsin the context of another (the soon-to-be-executing one). Whenthe OS then .nally executes a return-from-trap instruction, the soon-to-be-
OPERATING SYSTEMS ARPACI-DUSSEAU 
#void swtch(struct context *old, struct context *new);# #Save current register context in old #and then loadregister context fromnew. .globl swtchswtch: #Save oldregistersmovl 4(%esp), %eax # put old ptr into eax popl 0(%eax) # save the old IP movl %esp, 4(%eax) # and stack movl %ebx, 8(%eax) # and other registers movl %ecx, 12(%eax)movl %edx, 16(%eax)movl %esi, 20(%eax)movl %edi, 24(%eax)movl %ebp, 28(%eax) #Load newregistersmovl 4(%esp), %eax # put new ptr into eax movl 28(%eax), %ebp # restore other registers movl 24(%eax), %edimovl 20(%eax), %esimovl 16(%eax), %edxmovl 12(%eax), %ecxmovl 8(%eax), %ebxmovl 4(%eax), %esp # stack is switched here pushl 0(%eax) # return addr put in place ret # finally return into new ctxt  
Figure 6.1: The xv6 Context Switch Code  
executing process becomes the currently-running process. And thus the context switch is complete. Figure 6.1 shows the context switch code for xv6; see if you can .nd something similar within Linux(but be wary; it is notably more complicated).  
6.4  Summary  
We have described some key low-level mechanisms to implement CPU virtualization, a set of techniques which we collectively refer to as limited direct execution.The basic idea is straightforward: just run the program you want to run on the CPU, but .rst make sure to set up the hardware so as to limit what the process can do without OS assistance.  
ARPACI-DUSSEAU  THREE EASY PIECES (V0.5)  

ASIDE:HOW LONG CONTEXT SWITCHES TAKE 
Anatural question you might have is: how long does something like 
acontext switch take? Or even asystem call? For those of you that 
are curious, there is a tool called lmbench [MS96] that measures ex­
actly those things, as well as a few other performance measures that 
might be relevant. 
Results have improved quite a bit over time, roughly trackingpro­cessor performance. For example, in 1996 running Linux 1.3.37 on a200-MHz P6CPU, system calls took roughly 4microseconds, and acontextswitch roughly 6microseconds [MS96]. Modern systems perform almost an order of magnitude better, with sub-microsecond results on systems with 2-or 3-GHz processors. 
It should be noted that not all operating-system actions track CPU performance. As Ousterhout observed, many OS operations are memory intensive, and memory bandwidth has not improved as dramatically as processor speed over time [O90]. Thus, depending on your workload, buying the latest and greatest processor may not speed up your OS as much as you might hope. 
This general approach is taken in real life as well. For example, those of you who have children, or, at least, have heard of children, may be familiar with the concept of baby proo.ng aroom, i.e., lock­ing all cabinets with dangerous stuff in them and plugging allthe electrical sockets. When the room is thus readied, you can letyour baby roam freely, relaxed in the knowledge that the most dangerous aspects of the room have been restricted. 
In an analogous manner, the OS “baby proofs” the CPU, by .rst (during boot time) setting up the trap handlers and starting an inter­rupt timer, and then by only running processes in a restrictedmode. By doing so, the OS can feel quite assured that processes can run ef.ciently, only requiring OS intervention to perform privileged op­erations or when they have monopolized the CPU for too long and thus need to be switched out. 
We thus have the basic mechanisms for virtualizing the CPU in place. But a major question is left unanswered: which processshould we run at a given time? It is this question that the scheduler must answer, and thus the next topic of our study. 
OPERATING SYSTEMS ARPACI-DUSSEAU 


References 
[A79] “Alto User’s Handbook” Xerox Palo Alto Research Center September 1979 Available: http://history-computer.com/Library/AltoUsersHandbook.pdf 
An amazing system for its time, way ahead of most in almost all respects. Became famous because of Steve Jobs visiting, taking some notes, and going off to Apple to build Lisa and eventually the .rst Mac. 
[I11] “Intel 64 and IA-32 Architectures Software Developer’s Manual” Volume 3A: System Programming Guide, Part 1 Volume 3B: System Programming Guide, Part 2 Intel Corporation, January 2011 
[K+61] “One-Level Storage System” 
T. Kilburn, D.B.G. Edwards, M.J. Lanigan, F.H. Sumner IRE Transactions on Electronic Computers, April 1962 
The Atlas pioneered much of what you see in modern systems. However, this paper is not the best one to read. If you were to only read one, you might try the historical perspective below [L78]. 
[L78] “The Manchester Mark I and Atlas: A Historical Perspective” 
S. H. Lavington Communications of the ACM archive Volume 21, Issue 1 (January 1978), pages 4-12 
Anice piece of history on the early development of computer systems and the pioneering efforts of the Atlas. 
[M+63] “A Time-Sharing Debugging System for a Small Computer” 
J. McCarthy, S. Boilen, E. Fredkin, J. C. R. Licklider AFIPS ’63 (Spring), May, 1963, New York, USA 
An early paper about time-sharing that refers to using a timerinterrupt; it may not be the.rst work that does so though. The quote that discusses the purposeof such hardware: “Thebasic task of the channel 17 clock routine is to decide whether to remove the current user from core and if so to decide which user program to swap in as he goes out.” 
[MS96] “lmbench: Portable tools for performance analysis” Larry McVoy and Carl Staelin USENIX Annual Technical Conference, January 1996 
Afun paper about howto measure a number of different things about your OS and its perfor­mance. Download lmbench and give it a try; the home page has thesource code as well as some old results: http://www.bitmover.com/lmbench 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
[M11] “Mac OS 9” January 2011 Available: http://en.wikipedia.org/wiki/Mac OS 9 
Not the best reference but probably a reasonable source of information on this classic Mac operat­ing system. 
[O90] “Why Aren’t Operating Systems Getting Faster as Fast asHardware?” 
J. Ousterhout 
USENIX Summer Conference, June 1990 

Aclassic paper on the nature of operating system performance. Some of the measurements therein helped shape later benchmarks such as lmbench. 
[P10] “The Single UNIX Speci.cation, Version 3” 
The Open Group, May 2010 
Available: http://www.unix.org/version3/ 

This is hard and painful to read, so probably avoid it if you can. 
OPERATING SYSTEMS ARPACI-DUSSEAU 



Homework 
ASIDE:MEASUREMENT HOMEWORKS 
Measurement homeworks are small exercises where you write code to run on a real machine, in order to measure some aspect of OS or hardware performance. The idea behind such homeworks is to give you a little bit of hands-on experience with a real operating system. 
In this homework, you are to measure the costs of a system call and context switch. Measuring the cost of a system call is relatively easy. For example, you could repeatedly call a really simple system call (e.g., performing a 0-byte read), and time how long it takes; di­viding the time by the number of iterations gives you a rough esti­mate of the cost of a system call. 
One thing you’ll have to take into account is the precision andac­curacy of your timer. A typical timer that you can use is gettimeofday(); read the man page for details. What you’ll see there is that gettimeofday() returns the time in microseconds since 1970; however, this does not mean that the timer is precise to the microsecond. Measure back-to­back calls to gettimeofday() to learn something about how pre­cise the timer really is; this will tell you how many iterations of your null system-call test you’ll have to run in order to get a good mea­surement result. 
If gettimeofday() is not precise enough for you, you might look into using the rdtsc instruction available on x86 machines. This instruction reads the current value of a cycle timer; you’ll have to convert the results to seconds yourself of course. 
Measuring the cost of a context switch is a little trickier. The lm­bench benchmark does so by running two processes on a single CPU, and setting up two UNIX pipes between them; a pipe is just one of many ways processes in a UNIX system can communicate with one another. The .rst process then issues a write to the .rst pipe,and waits for a read on the second; upon seeing the .rst process wait­ing for something to read from the second pipe, the OS puts the .rst process in the blocked state, and switches to the other process, which reads from the .rst pipe and then writes to the second. When thesec­ond process tries to read from the .rst pipe again, it blocks, and thus 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

the back-and-forth cycle of communication continues. By measuring the cost of communicating like this repeatedly, lmbench can make a good estimate of the cost of a context switch. You can try to re-create something similar here, using pipes, or perhaps some other commu­nication mechanism such as UNIX sockets. 
One dif.culty in measuring context-switch cost arises in systems with more than one CPU; what you need to do on such a system is ensure that your context-switching processes are locatedon the same processor. Fortunately, most operating systems have calls to bind a process to a particular processor; on Linux, for example, thesched setaffinity() call is what you’re looking for. By ensur­ing both processes are on the same processor, you are making sure to measure the cost of the OS stopping one process and restoring an­other on the same CPU. 
OPERATING SYSTEMS ARPACI-DUSSEAU 


Scheduling: Introduction 
By now you should understand the basic machinery of running pro­cesses, including how to context-switch between processes and the details therein. Thus, the low-level mechanisms should be clear. 
However, we have yet to understand the high-level policies that the OS scheduler employs. In this note, we will do just that, present­ing a series of scheduling policies (sometimes called disciplines) that people have developed over the years. 
We will now develop some scheduling policies that have been put forth through the years. The origins of scheduling, in fact, predate computer systems, as early approaches were taken from the .eld of operations management and applied to computer systems. This should be no surprise: assembly lines and many other human con­structions also require scheduling. 
7.1 Workload Assumptions 
Before getting into the range of possible policies, let us .rst make anumber of simplifying assumptions about the processes running in the system, sometimes collectively called the workload.These assumptions are clearly unrealistic, but that is alright (for now), be­cause we will relax them as we go and eventually develop what we will refer to as ... (dramatic pause) ... 
61 
a fully-operational scheduling discipline1. We will make the following assumptions about the processes, some­
times called jobs,that are running in the system: 
1. 
Each job runs for the same amount of time. 

2. 
All jobs arrive at the same time. 

3. 
All jobs only use the CPU (i.e., they perform no I/O) 

4. 
The run-time of each job is known. 


We said all of these assumptions were unrealistic, but just assome animals are more equal than others in Orwell’s Animal Farm [O45], some assumptions are more unrealistic than others in this chapter. In particular, it might bother you that the run-time of each job is known: this would make the scheduler omniscient, which, although itwould be great (probably), is not likely to happen anytime soon. 

7.2 Scheduling Metrics 
Beyond making workload assumptions, we also need one more 
thing to enable us to compare different scheduling policies:a schedul­
ing metric.A metric is just something that we use to measure some­
thing, and of course there are a number of different metrics that make 
sense in scheduling. 
For now, however, let us also simplify our life by simply having a single metric: turnaround time.The turnaround time of a job, is de­.ned as the time at which the job .nally completes minus the time at which the job arrived in the system. More formally, the turnaround time Tturnaround is: 
Tturnaround =Tcompletion - Tarrival (7.1) 
Because we have assumed that all jobs arrive at the same time, for 
now Tarrival =0and hence Tturnaround =Tcompletion.This fact will 
change as we relax the aforementioned assumptions. 
You should note that turnaround time is a performance metric, which will be our primary focus this chapter. Another metric of in­terest is fairness,as measured (for example) by Jain’s Fairness Index [J91]. Performance and fairness are often at odds in scheduling; a 
1
Said in the same way you would say “A fully-operational Death Star.” 
OPERATING SYSTEMS ARPACI-DUSSEAU 
scheduler, for example, may optimize performance but at the cost of preventing a few jobs from running, thus decreasing fairness. This conundrum shows us that life isn’t always perfect. 

7.3 First In, First Out (FIFO) 
The most basic algorithm a scheduler can implement is known as First In, First Out (FIFO)scheduling or sometimes First Come, First Served (FCFS). FIFO has a number of positive properties: it is clearly very simple and thus easy to implement. And given our assumptions, it works pretty well. 
Let’s do a quick example together. Imagine three jobs arrive in the system, A, B, and C, at roughly the same time (Tarrival =0). Because FIFO has to put some job .rst, let’s assume that while they all arrived simultaneously, A arrived just a hair before B which arrived just a hair before C. Assume also that each job runs for 10 seconds. What will the average turnaround time be for these jobs? 
ABC 

Figure 7.1: FIFO Simple Example 
From Figure 7.1, you can see that A .nished at 10, B at 20, and C at 30. Thus, the average turnaround time for the three jobs is simply 
10+20+30 
3 =20.Computing turnaround time is as easy as that. 
But now let’s relax one of our assumptions. In particular, let’s relax assumption 1, and thus no longer assume that each job runs for the same amount of time. How does FIFO perform now? What kind of workload could you construct to make FIFO perform poorly? (think about this before reading on) 
Presumably you’ve .gured this out by now, but just in case, let’s do an example to show how jobs of different lengths can lead to trou­ble for FIFO scheduling. In particular, let’s again assume three jobs (A, B, and C), but this time A runs for 100 seconds while B and C run for 10 each. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

Figure 7.2: Why FIFO Is Not That Great 
As you can see from Figure 7.2, Job A runs .rst and takes up the 
full 100 seconds before B or C even get a chance to run. Thus, theav­
erage turnaround time for the system is high: a painful 110 seconds 
( 100+110+120 
3 =110). 
This problem is generally referred to as the convoy effect [B+79], where a number of relatively-short potential consumers of a resource get queued behind a heavyweight resource consumer. This might re­mind you of a single line at a grocery store and what you feel like when you see the person in front of you with three carts full of pro­visions and their checkbook out; it’s going to be a while. 
So what should we do? How can we develop a better algorithm 
to deal with our new reality of jobs that run for different amounts of 
time? Think about it .rst; then read on. 

7.4 Shortest Job First (SJF) 
It turns out that a very simple approach solves this problem; in fact it is an idea stolen from operations research [C54,PV56]and ap­plied to scheduling of jobs in computer systems. This new schedul­ing discipline is known as Shortest Job First (SJF),and the name should be easy to remember because it describes the policy quite completely: it runs the shortest job .rst, then the next shortest, and so on. 
Let’s take our example above but with SJF as our scheduling pol­icy. Figure 7.3 shows the results of running A, B, and C. Hope­fully the diagram makes it clear why SJF performs much better with regards to average turnaround time. Simply by running B and C before, A, SJF reduces average turnaround from 110 seconds to50 
( 10+20+120 
3 =50), more than a factor of two improvement. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

Figure 7.3: SJF Simple Example 
In fact, given our assumptions about jobs all arriving at the same time, we could prove that SJF is indeed an optimal scheduling algo­rithm. However, you are in a systems class, not theory or operations research; no proofs are allowed. 
Thus we arrive upon a good approach to scheduling with SJF, but our assumptions are still fairly unrealistic. Let’s relax another. In particular, we can target assumption 2, and now assume that jobs can arrive at any time instead of all at once. What problems does this lead to? (think about it again) 
Here we can illustrate the problem again with an example. This time, assume A arrives at t =0and needs to run for 100 seconds, whereas B and C arrive at t =10and each need to run for 10 seconds. With pure SJF, we’d get the schedule seen in Figure 7.4. 
As you can see from the .gure, even though B and C arrived shortly after A, they still are forced to wait until A has completed, and thus suffer the same convoy problem. Average turnaround time 
for these three jobs is 103.33 seconds ( 100+(110-10)+(120-10) ). What 
3 
can a scheduler do? 
DESIGN TIP:PRINCIPLE OF SJF Shortest Job First represents a general scheduling principle that can be applied to any system where the perceived turnaround time per customer (or, in our case, a job) matters. Think of any line youhave waited in: if the establishment in question cares about customer sat­isfaction, it is likely they have taken SJF into account. For example, grocery stores commonly have a “ten-items-or-less” line to ensure that shoppers with only a few things to purchase don’t get stuck be­hind the family preparing for some upcoming nuclear winter. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

Time 
Figure 7.4: SJF With Late Arrivals From B and C 

7.5 Shortest Time-to-Completion First (STCF) 
As you might have guessed, given our previous discussion about 
mechanisms such as timer interrupts and context switching, the sched­
uler can certainly do something else when B and C arrive: it can 
preempt job A and decide to run another job, perhaps continuing A 
later. SJF by our de.nition is a non-preemptive scheduler, and thus 
suffers from the problems described above. 
ASIDE:PREEMPTIVE SCHEDULERS 
In the old days of batch computing, a number of non-preemptive 
schedulers were developed; such systems would run each job to 
completion before considering whether to run a new job. Virtually 
all modern schedulers are preemptive,andquite willing to stop one 
process from running in order to run another. This implies that the 
scheduler employs the mechanisms we learned about previously; in 
particular, the scheduler can perform a context switch,stopping one 
running process temporarily and resuming (or starting) another. 
Fortunately, there is a scheduler which does exactly that: add pre­emption to SJF, known as the Shortest Time-to-Completion First (STCF)or Preemptive Shortest Job First PSJF scheduler [CK68]. Any time a new job enters the system, it determines of the remaining jobs and new job, which has the least time left, and then schedules that one. Thus, in our example, STCF would preempt A and run B and C to completion; only when they are .nished would A’s remain­ing time be scheduled. Figure 7.5 shows an example. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

Time  
Figure 7.5: STCF Simple Example  
The result is a much-improved average turnaround time: 50 sec­onds ( (120-0)+(20-10)+(30-10) 3 ). And as before, given our new as­sumptions, STCF is provably optimal; given that SJF is optimal if all jobs arrive at the same time, you should probably be able tosee the intuition behind the optimality of STCF. Thus, if we knew that job lengths, and jobs only used the CPU, and our only metric was turnaround time, STCF would be a great policy. In fact, for a number of early batch computing systems, these types of scheduling algorithms made some sense. However, thein­troduction of time-shared machines changed all that. Now users would sit at a terminal and demand interactive performance from the system as well. And thus, a new metric was born: response time. Response time is de.ned as the time from when the job arrives in asystem to the .rst time it is scheduled. More formally:  
Tresponse = Tfirstrun - Tarrival  (7.2)  
For example, if we had the schedule above (with A arriving at time 0, and B and C at time 10), the response time of each job is as follows: 0 for job A, 0 for B, and 10 for C (average: 3.33). As you might be thinking, STCF and related disciplines are not particularly good for response time. If three jobs arrive at the same time, for example, the third job has to wait for the previous two jobs to run in their entirety before being scheduled just once. While great for turnaround time, this approach is quite bad for response time and interactivity. Indeed, imagine sitting at a terminal, typing, and hav­ing to wait 10 seconds to see a response from the system just because some other job got scheduled in front of yours: not too pleasant. Thus, we are left with another problem: how can we build a sched­uler that is sensitive to response time?  
THREE  
ARPACI-DUSSEAU  EASY PIECES  
(V0.5)  


0 5 1015202530 Time 
Figure 7.6: SJF Again (Bad for Response Time) 
ABCABCABCABCABC 
0 5 1015202530 Time 
Figure 7.7: Round Robin 

7.6 Round Robin 
To solve this problem, we will introduce a new scheduling al­gorithm. This approach is classically known as Round-Robin (RR) scheduling [K64]. The basic idea is simple: instead of running jobs to completion, RR runs a job for a time slice (sometimes called a scheduling quantum)and then switches to the next job in the run queue. It repeatedly does so until the jobs are .nished. For this rea­son, RR is sometimes called time-slicing.Note that the length of a time slice must be a multiple of the timer-interrupt period; thus if the timer interrupts every 10 milliseconds, the time slice couldbe 10, 20, or any other multiple of 10 ms. 
To understand RR in more detail, let’s look at an example. As­sume three jobs A, B, and C arrive at the same time in the system, and that they each wish to run for 5 seconds. An SJF scheduler runs each job to completion before running another (Figure 7.6). In con­trast, RR with a time-slice of 1 second would cycle through thejobs quite quickly (Figure 7.7). 
0+1+2 
The average response time of RR is: 3 =1;for SJF, average response time is: 0+5+10 =5.
3 
OPERATING SYSTEMS ARPACI-DUSSEAU 
DESIGN TIP:AMORTIZATION The general technique of amortization is commonly used in systems when there is a .xed cost to some operation. By incurring that cost less often (i.e., by performing the operation fewer times), the total cost to the system is reduced. For example, if the time slice isset to 10 ms, and the context-switch cost is 1 ms, roughly 10% of time is spent context switching and is thus wasted. If we want to amortize this cost, we can increase the time slice, e.g., to 100 ms. In this case, less than 1% of time is spent context switching, and thus the cost of time-slicing has been amortized. 
As you can see, the length of the time slice is critical for RR. The shorter it is, the better the performance of RR under the response-time metric. However, making the time slice too short is problem­atic: suddenly the cost of context switching will dominate overall performance. Thus, deciding on the length of the time slice presents atrade-off to asystem designer, making itlong enough to amortize the cost of switching without making it so long that the systemis no longer responsive. 
Note that the cost of context switching does not arise solely from the OS actions of saving and restoring a few registers. When pro­grams run, they build up a great deal of state in CPU caches, TLBs, branch predictors, and other on-chip hardware. Switching toan­other job causes this state to be .ushed and new state relevantto the currently-running job to be brought in, which may exact a noticeable performance cost [MB91]. 
RR, with a reasonable time slice, is thus an excellent scheduler if response time is our only metric. But what about our old friend turnaround time? Let’s look at our example above again. A, B, and C, each with running times of 5 seconds, arrive at the same time, and RR is the scheduler with a (long) 1-second time slice. We can see from the picture above that A .nishes at 13, B at 14, and C at 15, for an average of 14. Pretty awful! 
It is not surprising, then, that RR is indeed one of the worst poli­cies if turnaround time is our metric. Intuitively, this should make sense: what RR is doing is stretching out each job as long as it can, by only running each job for a short bit before moving to the next. Because turnaround time only cares about when jobs .nish, RR is nearly pessimal, even worse than simple FIFO in many cases. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
More generally, any policy (such as RR) that is fair,i.e., that evenly divides the CPU among active processes on a small time scale, will perform poorly on metrics such as turnaround time. Indeed, this is an inherent trade-off: if you are willing to be unfair, you canrun shorter jobs to completion, but at the cost of response time; if you instead value fairness, response time is lowered, but at the cost of turnaround time. This type of trade-off is common in systems; you can’t have your cake and eat it too. 
We have developed two types of schedulers. The .rst type (SJF, STCF) optimizes turnaround time, but is bad for response time. The second type (RR) optimizes response time but is bad for turnaround. And we still have two assumptions which need to be relaxed: as­sumption 3 (that jobs do no I/O), and assumption 4 (that the run­time of each job is known). Let’s tackle those assumptions next. 

7.7 Incorporating I/O 
First we will relax assumption 3. Of course all programs perform I/O. Imagine a program that didn’t take any input: it would produce the same output each time. Imagine one without output: it is the tree falling in the forest, with no one to see it; it doesn’t matter that it ran. 
Ascheduler clearly has a decision to make when a job initiatesan I/O request, because the currently-running job won’t be using the CPU during the I/O; it is blocked waiting for I/O completion. If the I/O is sent to a hard disk drive, the process might be blocked for a few ms or longer, depending on the current I/O load of the drive. Thus, the scheduler should probably schedule anotherjob on the CPU at that time. 
The scheduler also has to make a decision when the I/O com­pletes. When that occurs, an interrupt is raised, and the OS runs and moves the process that issued the I/O from blocked back to the ready state. Of course, it could even decide to run the job at that point. How should the OS treat each job? 
To understand this issue better, let us assume we have two jobs, Aand B, which each need 50 ms of CPU time. However, there is one obvious difference: A runs for 10 ms and then issues an I/O request (assume here that I/Os each take 10 ms), whereas B simply uses the CPU for 50 ms and performs no I/O. Imagine the scheduler decides to run A .rst, then B after (Figure 7.8). 
OPERATING SYSTEMS ARPACI-DUSSEAU 

Time 
Figure 7.8: Poor Use of Resources 
Assume we are trying to build a STCF scheduler. How should such a scheduler account for the fact that A is broken up into 5 10-ms sub-jobs, whereas B is just a single 50-ms CPU demand? Clearly, just running one job and then the other without considering how to take I/O into account makes little sense. 
Acommon approach is to treat each 10-ms sub-job of Aas an in­dependent job. Thus, when the system starts, its choice is whether to schedule a 10-ms A or a 50-ms B. With STCF, the choice is clear: choose the shorter one, in this case A. Then, when the .rst sub-job of Ahas completed, only B is left, and it begins running. Then a new sub-job of A is submitted, and it preempts B and runs for 10 ms. Do­ing so allows for overlap to occur, with the CPU being used by one process while waiting for the I/O of another process to complete; the system is thus better utilized (see Figure 7.9). 
And thus we see how a scheduler might incorporate I/O. By treat­ing each CPU burst as a job, the scheduler makes sure processesthat are “interactive” get run frequently. While those interactive jobs are performing I/O, other CPU-intensive jobs run, thus better utilizing the processor. 
ABABABABAB 

Time 
Figure 7.9: Overlap Allows Better Use of Resources 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
7.8  No More Oracle  
With a basic approach to I/O in place, we come to our .nal as­sumption: that the scheduler knows the length of each job. As we said before, this is likely the worst assumption we could make. In fact, in a general-purpose OS (like the ones we care about), the OS usually knows very little about the length of each job. Thus, how can we build an approach that behaves like SJF/STCF without such aprioriknowledge? Further, how can we incorporate some of the ideas we have seen with the RR scheduler so that response time is also quite good?  
DESIGN TIP:OVERLAP When possible, overlap operations to maximize the utilization of systems. Overlap is useful in many different domains, includ­ing when performing disk I/O or sending messages to remote ma­chines; in either case, starting the operation and then switching to other work is a good idea, and improved the overall utilization and ef.ciency of the system.  
7.9  Summary  
We have introduced the basic ideas behind scheduling and devel­oped two families of approaches. The .rst runs the shortest job re­maining and thus optimizes turnaround time; the second alternates between all jobs and thus optimizes response time. Both are bad where the other is good, alas, an inherent trade-off common insys­tems. We have also seen how we might incorporate I/O into the picture, but have still not solved the problem of the fundamental in­ability of the OS to see into the future. Shortly, we will see how to overcome this problem, by building a scheduler that uses the recent past to predict the future. This scheduler is known as the multi-level feedback queue,and it isthe topic of the next chapter.  
OPERATING SYSTEMS  ARPACI-DUSSEAU  


References 
[B+79] “The Convoy Phenomenon” 
M. Blasgen, J. Gray, M. Mitoma, T. Price ACM Operating Systems Review, 13:2, April 1979 
Perhaps the .rst reference to convoys, which occurs in databases as well as the OS. 
[C54] “Priority Assignment in Waiting Line Problems” 
A. Cobham Journal of Operations Research, 2:70, pages 70–76, 1954 
The pioneering paper on using an SJF approach in scheduling the repair of machines. 
[K64] “Analysis of a Time-Shared Processor” Leonard Kleinrock Naval Research Logistics Quarterly, 11:1, pages 59–73, March 1964 
May be the .rst reference to the round-robin scheduling algorithm; certainly one of the .rst analyses of said approach to scheduling a time-shared system. 
[CK68] “Computer Scheduling Methods and their Countermeasures” Edward G. Coffman and Leonard Kleinrock AFIPS ’68 (Spring), April 1968 
An excellent early introduction to and analysis of a number ofbasic scheduling disciplines. 
[J91] “The Art of Computer Systems Performance Analysis: 
Techniques for Experimental Design, Measurement, Simulation, and Modeling” 

R. Jain Interscience, New York, April 1991 
The standard text on computer systems measurement. A great reference for your library, for sure. 
[O45] “Animal Farm” George Orwell Secker and Warburg (London), 1945 
Agreat but depressing allegorical book about power and its corruptions. Some say it is a critique of Stalin and the pre-WWII Stalin era in the U.S.S.R; we say it’s a critique of pigs. 
[PV56] “Machine Repair as a Priority Waiting-Line Problem” Thomas E. Phipps Jr. and W. R. Van Voorhis Operations Research, 4:1, pages 76–86, February 1956 
Follow-on work that generalizes the SJF approach to machine repair from Cobham’s original work; also postulates the utility of an STCF approach in such an environment. Speci.cally, “There are certain types of repair work, ... involving much dismantlingandcovering the .oor withnuts and bolts, which certainly should not be interrupted once undertaken; in other cases it would be inadvisable to continue work on a long job if one or more short ones became available (p.81).” 
[MB91] “The effect of context switches on cache performance” Jeffrey C. Mogul and Anita Borg ASPLOS, 1991 
Anice study on howcache performance can be affected by context switching; less of an issue in today’s systems where processors issue billions of instructions per second but context-switches still happen in the millisecond time range. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

Homework 
ASIDE:SIMULATION HOMEWORKS 
Simulation homeworks come in the form of simulators you run to make sure you understand some piece of the material. The simula­tors are generally python programs that enable you both to generate different problems (using different random seeds) as well asto have the program solve the problem for you (with the -c .ag) so that you can check your answers. Running any simulator with a -h or --help .ag will provide with more information as to all the options the simulator gives you. 
This program, scheduler.py,allows you to see how different schedulers perform under scheduling metrics such as response time, turnaround time, and total wait time. Three schedulers are “imple­mented”: FIFO, SJF, and RR. 
There are two steps to running the program. 
First, run without the -c .ag: this shows you what problem to solve without revealing the answers. For example, if you wantto compute response, turnaround, and wait for three jobs using the FIFO policy, run this: 
./scheduler.py -p FIFO -j 3 -s 100 
If that doesn’t work, try this: 
python ./scheduler.py -p FIFO -j 3 -s 100 
This speci.es the FIFO policy with three jobs, and, importantly, aspeci.c random seed of 100. If you want to see the solution for this exact problem, you have to specify this exact same randomseed again. Let’s run it and see what happens. Figure 7.10 shows the output you should see. 
As you can see from this example, three jobs are generated: job0 of length 1, job 1 of length 4, and job 2 of length 7. As the program states, you can now use this to compute some statistics and seeif you have a grip on the basic concepts. 
Once you are done, you can use the same program to solve the 
problem and see if you did your work correctly. To do so, use the -c 
.ag. Figure 7.11 shows the output. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
prompt> ./scheduler.py -p FIFO -j 3 -s 100 ARG policy FIFOARG jobs 3ARG maxlen 10 ARG seed 100 
Here is the job list, with the run time of each job: Job 0 (length = 1)Job 1 (length = 4)Job 2 (length = 7) 
Compute the turnaround time, response time, and wait time foreach job.When you are done, run this program again, with the same arguments, but with -c, which will thus provide you with the answers. You can use to use -s <somenumber> or your own job list (-l 10,15,20 for example) to generate different problems for yourself. 
Figure 7.10: Homework Output 
prompt> ./scheduler.py -p FIFO -j 3 -s 100 -c ARG policy FIFOARG jobs 3ARG maxlen 10 ARG seed 100 
Here is the job list, with the run time of each job: Job 0 (length = 1)Job 1 (length = 4)Job 2 (length = 7) 
** Solutions ** 
Execution trace: [time 0] Run job 0 for 1.00 secs (DONE)[time 1] Run job 1 for 4.00 secs (DONE)[time 5] Run job 2 for 7.00 secs (DONE) 
Final statistics: Job 0 --Response: 0.00 Turnaround 1.00 Wait 0.00 Job 1 --Response: 1.00 Turnaround 5.00 Wait 1.00 Job 2 --Response: 5.00 Turnaround 12.00 Wait 5.00 
Average --Response: 2.00 Turnaround 6.00 Wait 2.00 
Figure 7.11: Generating Homework Solutions 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
As you can see from the .gure, the -c .ag shows you what hap­pened. Job 0 ran .rst for 1 second, Job 1 ran second for 4, and then Job 2 ran for 7 seconds. Not too hard; it is FIFO, after all! The execu­tion trace shows these results. 
The .nal statistics are useful too: they compute the response time (the time a job spends waiting after arrival before .rst running), the turnaround time (the time it took to complete the job since .rst ar­rival), and the total wait time (any time spent ready but not running). The stats are shown per job and then as an average across all jobs. Of course, you should have computed these things all before running with the -c .ag! 
If you want to try the same type of problem but with different in­puts, try changing the number of jobs or the random seed or both. Different random seeds basically give you a way to generate anin­.nite number of different problems for yourself, and the -c .ag lets you check your own work. Keep doing this until you feel like you really understand the concepts. 
One other useful .ag is -l (that’s a lower-case L), which lets you specify the exact jobs you wish to see scheduled. For example,if you want to .nd out how SJF would perform with three jobs of lengths 5, 10, and 15, you can run:
prompt> ./scheduler.py -p SJF -l 5,10,15
ARG policy SJF
ARG jlist 5,10,15 

Here is the job list, with the run time of each job: Job 0 (length = 5.0)Job 1 (length = 10.0)Job 2 (length = 15.0) 
... 
And then you can use -c to solve it again. Note that when you specify the exact jobs, there is no need to specify a random seed or the number of jobs: the jobs lengths are taken from your comma-separated list. 
Of course, more interesting things happen when you use SJF (shortest­
job .rst) or even RR (round robin) schedulers. Try them and see! And you can always run 
./scheduler.py -h 
to get a complete list of .ags and options (including options such as setting the time quantum for the RR scheduler). 
OPERATING SYSTEMS ARPACI-DUSSEAU 
Questions 
1. 
Compute the response time and turnaround time when run­ning three jobs of length 200 with the SJF and FIFO schedulers. 

2. 
Now do the same but with jobs of different lengths: 100, 200, and 300. 

3. 
Now do the same, but also with the RR scheduler and a time-slice of 1. 

4. 
For what types of workloads does SJF deliver the same turnaround times as FIFO? 

5. 
For what types of workloads and quantum lengths does SJF deliver the same response times as RR? 

6. 
What happens to response time with SJF as job lengths increase? Can you use the simulator to demonstrate the trend? 

7. 
What happens to response time with RR as quantum lengths increase? Can you write an equation that gives the worst-case response time, given N jobs? 


THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 



Scheduling:The Multi-Level Feedback Queue 
In this note, we’ll tackle the problem of developing one of themost well-known approaches to scheduling, known as the Multi-level Feed­back Queue (MLFQ).The Multi-level Feedback Queue (MLFQ) sched­uler was .rst described by Corbato et al. in 1962 [C+62] in a sys­tem known as the Compatible Time-Sharing System (CTSS), and this work, along with later work on Multics, led the ACM to award Cor­bato its highest honor, the Turing Award. It has subsequentlybeen re.ned throughout the years to the implementations you will en­counter in modern systems. 
The fundamental problem MLFQ tries to address is two-fold. First, it would like to optimize turnaround time,which, as we saw in the previous note, is done by running shorter jobs .rst; unfortunately, the OS doesn’t generally know how long a job will run for, exactly the knowledge that algorithms like SJF (or STCF) require. Second, MLFQ would like to make a system feel responsive to interactive users (i.e., users sitting and staring at the screen, waitingfor a pro­cess to .nish), and thus minimize response time;unfortunately,algo­rithms like Round Robin reduce response time but are terriblefor turnaround time. Thus, our problem: given that we in general do not know anything about a process, how can we build a schedulerto achieve these goals? 
79 
THE CRUX: HOW TO SCHEDULE WITHOUT PERFECT KNOWLEDGE? 
How can we design a scheduler that both minimizes response 
time for interactive jobs while also minimizing turnaround time 
without apriori knowledge of job length? 
8.1 MLFQ: Basic Rules 
To build such a scheduler, in this chapter we will describe the basic algorithms behind a multi-level feedback queue; although the speci.cs of many implemented MLFQs differ [E95], the basic ap­proaches are all similar. 
In our treatment, the MLFQ has a number of distinct queues,each assigned a different priority level.At any given time, a job that is ready to run is on a single queue. MLFQ uses priorities to decide which job should run at a given time: a job with higher priority(i.e., ajob on ahigher queue) is chosen to run. 
Of course, more than one job may be on a given queue, and thus 
have the same priority. In this case, we will just use round-robin 
scheduling among those jobs. 
Thus, the key to MLFQ scheduling lies in how the scheduler sets priorities. Rather than giving a .xed priority to each job, MLFQ varies the priority of a job based on its observed behavior.If, for exam­ple, a job repeatedly relinquishes the CPU while waiting for input from the keyboard, MLFQ will keep its priority high, as this ishow an interactive process might behave. If, instead, a job uses the CPU intensively for long periods of time, MLFQ will reduce its priority. In this way, MLFQ will try to learn about processes as they run, and thus use the history of the job to predict its future behavior. 
Thus, we arrive at the .rst two basic rules for MLFQ: 
• 
Rule 1: If Priority(A) > Priority(B), A runs (B doesn’t). 

• 
Rule 2: If Priority(A) = Priority(B), A & B run in RR. 


If we were to put forth a picture of what the queues might look 
like at a given instant, we might see something like what you can see 
in Figure 8.1. 
In the .gure, two jobs (A and B) are at the highest priority level, 
OPERATING SYSTEMS ARPACI-DUSSEAU 
[High Priority] Q8 

Q7 
Q6 
Q5 
Q4 
Q3 
Q2 
[Low Priority] Q1 

Figure 8.1: MLFQ Example 
while job C is in the middle and Job D is at the lowest priority. Given our current knowledge of how MLFQ works, the scheduler would just alternate time slices between A and B because they are thehigh­est priority jobs in the system. 

8.2 Attempt #1: How to Change Priority 
We now must decide how MLFQ is going to change the priority level of a job (and thus which queue it is on) over the lifetime of a job. To do this, we must keep in mind our workload: a mix of inter­active jobs that are short-running (and may frequently relinquish the CPU), and some longer-running “CPU-bound” jobs that need a lot of CPU time but where response time isn’t important. Here is our .rst attempt at a priority-adjustment algorithm: 
• 	
Rule 3: When a job enters the system, it is placed at the highest priority (the topmost queue). 

• 	
Rule 4a: If a job uses up an entire time slice while running, its priority is reduced (i.e., it moves down one queue). 

• 	
Rule 4b: If a job gives up the CPU before the time slice is up, it stays at the same priority level. 


ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 

Q2 
Q1 
Q0 
Figure 8.2: Long-running Job Over Time 
Example 1: A Single Long-Running Job 
To understand this better, let’s look at some examples. First, we’ll just look at what happens when there has been a long running job in the system for a while. Figure 8.2 shows what happens to thisjob over time, in a system with three queues. 
As you can see in the example, the job enters at the highest prior­ity (Q2). After a single time-slice of 10 ms, the scheduler reduces the job’s priority by one, and thus the job is on Q1. After running at Q1 for a time slice, the job is .nally lowered to the lowest priority in the system (Q0), where it remains. Pretty simple, no? 
Example 2: Along Came A Short Job 
Now let’s look at a more complicated example, and hopefully see how MLFQ tries to approximate SJF. In this example, there are two jobs: A, which is a long-running CPU-intensive job, and B, which is a short-running interactive job. Assume A has been runningfor some time, and then B arrives. What do you think will happen? Will MLFQ approximate shortest-job .rst for B? 
Figure 8.3 plots the results of this scenario. A (shown in black) is running along in the lowest-priority queue (as would any long-running CPU-intensive jobs); B (shown in gray) arrives at time T = 100,and thus is inserted into the highest queue;as its run-time is 
OPERATING SYSTEMS ARPACI-DUSSEAU 
Q2 
Q1 Q2 
Q0  
50  100  Time  150  200  
Figure 8.3: Along Came An Interactive Job  
short (only 20 ms), B completes before reaching the bottom queue, in two time slices; then A resumes running (at low priority). From this example, you can hopefully understand one of the ma­jor goals of the algorithm: because it doesn’t know whether a job will be a short job or a long-running job, it .rst assumes it might be a short job, thus giving the job high priority. If it actually is a short job, it will run quickly and complete; if it is not a short job, it will slowly move down the queues, and thus soon prove itself to be a long-running more batch-like process. In this manner, MLFQ approximates SJF.  
Example 3: What About I/O?  
Let’s now look at an example with some I/O. As Rule 4b states above, if a process gives up the processor before using up its time slice, we keep it at the same priority level. The intent of this rule is simple: if an interactive job, for example, is doing a lot of I/O (say bywait­ing for user input from the keyboard or mouse), it will relinquish the CPU before its time slice is complete; in such case, we don’t wish to penalize the job and thus simply keep it at the same level. Figure 8.4 shows an example of how this works, with an interac­tive job B (shown in gray) that needs the CPU only for 1 ms before performing an I/O competing for the CPU with a long-running batch job A (shown in black). The MLFQ approach keeps B at the highest priority because B keeps releasing the CPU; if B is an interactive job, MLFQ further achieves its goal of running interactive jobs quickly.  
ARPACI-DUSSEAU  THREE EASY PIECES (V0.5)  


Q1 

Time 
Figure 8.4: A Mixed I/O-intensive and CPU-intensive Workload 
Problems With Our Current MLFQ 
We thus have a basic MLFQ algorithm. It seems to do a fairly good job, sharing the CPU fairly between long-running jobs, and letting short or I/O-intensive interactive jobs run quickly. Unfortunately, the approach we have developed thus far contains a few serious problems. Can you think of any? (pause and think on it a minute) 
First, there is the problem of starvation:if there are “too many” interactive jobs in the system, they will combine to consume all CPU time, and thus long-running jobs will never receive any CPU time (hence the name, starvation). Clearly, we’d like to make someprogress on these jobs even in this scenario. 
Second, a smart user could rewrite their program to game the scheduler.Gaming the scheduler generally refers to the idea of do­ing something sneaky to trick the scheduler into giving you more than your fair share of the resource. The algorithm we have de­scribed is susceptible to the following attack: before the time slice is over, issue an I/O operation (to some .le you don’t care about) and thus relinquish the CPU; doing so allows you to remain in the same queue, and thus gain a higher percentage of the CPU. In fact, if done just right (e.g., by running for 99% of the time slice before relinquishing the CPU), a job could get most available CPU time. 
Finally, a program may change its behavior over time; what was CPU-bound may transition to a phase of interactivity. With our cur­rent approach, such a job would be out of luck and not be treatedlike the other interactive jobs in the system. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
Q2 
Q2 
Q1 	Q1 

Q0 	Q0 
0 50 100 150 200 250 0 50 100 150 200 250 Time Time 
Figure 8.5: Without (Left) and With (Right) Priority Boost 

8.3 Attempt #2: The Priority Boost 
Let’s try to change the rules and see if we can avoid the problemof 
starvation. What could we do in order to guarantee that CPU-bound 
jobs will make some progress (even if it is not much?). 
The simple idea here is to periodically boost the priority of all the jobs in system. There are many ways to achieve this, but let’s just do something simple: throw them all in the topmost queue. Thus, we add a new rule: 
• 	Rule 5: After some time period S,move all the jobs in the sys­tem to the topmost queue. 
Our new rule solves two problems at once. First, processes are guaranteed not to starve: by sitting in the top queue, a job will share the CPU with other high-priority jobs in a round-robin fashion, and thus eventually receive service. Second, if a CPU-bound job has be­come interactive, the scheduler treats it properly once it has received the priority boost. 
Let’s see an example. In this scenario, we just show the behavior of a long-running job when competing for the CPU with two short-running interactive jobs. Two graphs are shown in Figure 8.5.On the left, there is no priority boost, and thus the long-running job gets starved once the two short jobs arrive; on the right, there is apriority boost every 50 ms (which is likely too small of a value, but used here for the example), and thus we at least guarantee that the long-running job will make some progress, getting boosted to the highest priority every 50 ms and thus getting to run periodically. 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
DESIGN TIP:AVOID VOO-DOO CONSTANTS 
It is pretty clear that avoiding voo-doo constants is a good idea 
whenever possible. Unfortunately, as in the example above, it is of­
ten dif.cult. One could try to make the system learn a good value, 
but that too is not straightforward. The frequent result: a con.gura­
tion .le .lled with default parameter values that a seasoned admin­
istrator can tweak when something isn’t quite working correctly. As 
you can imagine, these are often left unmodi.ed, and thus we are left 
to hope that the default values shipped with the system work well 
in the .eld. This tip brought to you by our old OS professor, John 
Ousterhout, and hence the other name for it: Ousterhout’s Law. 
Of course, the addition of the time period S leads to the obvious question: what should S be set to? John Ousterhout, a well-regarded systems researcher [O11], used to call such values in systems voo­doo constants,because they seemed to require some form of black magic to set them correctly. Unfortunately, S has that .avor. If it is set too high, long-running jobs could starve; too low, and interactive jobs may not get a proper share of the CPU. 

8.4 Attempt #3: Better Accounting 
We now have one more problem to solve: how to prevent gaming of our scheduler? The real culprit here, as you might have guessed, are Rules 4a and 4b, which let a job retain its priority level simply by relinquishing the CPU before the time slice expires. So what should we do instead? 
The solution here is to perform better accounting of CPU time at each level of the MLFQ. Instead of forgetting how much of a time slice a process used at a given level, the scheduler should keep track; once a process has used its allotment, it is demoted to the nextprior­ity queue. Whether it uses the time slice in one long burst or many small ones does not matter. We thus rewrite Rules 4a and 4b to the following single rule: 
• 	Rule 4: Once a job uses up its time allotment at a given level (regardless of how many times it has given up the CPU), its 
OPERATING SYSTEMS ARPACI-DUSSEAU 
priority is reduced (i.e., it moves down one queue). 
Q2 
Q2 
Q1 Q1 
Q0 
Q0 

50 100 150 200 50 100 150 200 Time Time 
Figure 8.6: Without (Left) and With (Right) Gaming Tolerance Let’s look once again at an example. Figure 8.6 shows what hap­pens when a workload tries to game the scheduler with the old Rules 4a and 4b (on the left) as well the new anti-gaming Rule 4. Without any protection from gaming, a process can issue an I/O just before atime slice ends and thus dominate CPU time. With such protec­tions in place, regardless of the I/O behavior of the process,it slowly moves down the queues, and thus cannot gain an unfair percentage of CPU time. 

8.5 Tuning MLFQ and Other Issues 
Afew other issues arise with MLFQ scheduling. One big question is how to parameterize such a scheduler. For example, how many queues should there be? How big should the time slice be per queue? How often should priority be boosted in order to avoid starvation and account for changes in behavior? There are no easy answersto these questions, and thus only some experience with workloads and subsequent tuning of the scheduler will lead to a satisfactory balance. 
For example, most MLFQ variants allow for varying time-slice length across the different queues of the system. The high-priority queues are usually given short time slices; they are comprised of in­teractive jobs, after all, and thus quickly alternating between them makes sense (e.g., 10 or 20 milliseconds). The low-priority queues, in contrast, contain long-running jobs that are CPU-bound. Thus, there is no need to switch between them frequently, and longer time slices 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 

Q2 
Q1 
Q0 
Figure 8.7: Lower Priority, Longer Quanta 
make sense (e.g., hundreds of ms). Figure 8.7 shows an examplein which two long-running jobs run for 10 ms at the highest queue,20 ms at the middle queue, and 40 ms at the lowest. 
The Solaris implementation of MLFQ, known as the Time Sharing scheduling class (TS), is particularly easy to con.gure; it provides asetof tables that determine exactly how the priority of aprocess is altered throughout its lifetime, how long each time slice is, and how often to boost the priority of a job [AD00]; an administrator can muck with this table in order to make the scheduler behave in differ­ent ways. Default values for the table are 60 queues, with slowly in­creasing time-slice lengths from 20 milliseconds (highest priority) to afew hundred milliseconds (lowest), and priorities boostedaround every 1 second or so. 
Other MLFQ schedulers don’t use a table or the exact rules de­scribed in this chapter; rather they adjust priorities usingmathemat­ical formulae. For example, the FreeBSD scheduler (version 4.3) uses aformula to calculate the current priority level of ajob, basing it on how much CPU the process has used [LM+89]; in addition, usage is decayed over time, providing the desired priority boost in a different manner than described herein. See [E95] for an excellent overview of such decay-usage algorithms and their properties. 
Finally, many schedulers have a few other features that you might 
encounter. For example, some schedulers reserve the highestprior­
ity levels for operating system work; thus typical user jobs can never 
OPERATING SYSTEMS ARPACI-DUSSEAU DESIGN TIP:USE ADVICE WHERE POSSIBLE 
As the operating system rarely knows what is best for each and ev­
ery process of the system, it is often useful to provide interfaces to 
allow users or administrators to provide some hints to the OS. We 
often call such hints advice,as the OS need not necessarily pay at­
tention to it, but rather might take the advice into account inorder 
to make a better decision. Such hints are useful in many parts of 
the OS, including the scheduler (e.g., with nice), memory manager 
(e.g., madvise), and .le system (e.g., TIP [P+95]). 
obtain the highest levels of priority in the system. Some systems also allow some user advice to help set priorities; for example, by using the command-line utility nice you can increase or decrease the pri­ority of a job (at least, somewhat), and thus increase or decrease its chances of running at any given time. See the nice man page for details. 

8.6 MLFQ: Summary 
We have described a scheduling approach known as the Multi-Level Feedback Queue (MLFQ). Hopefully you can now see why it is called that: it has multiple levels of queues, and uses feedback to de­termine the priority of a given job. The re.ned set of rules, spread throughout the chapter, are reproduced here for your viewingplea­sure: 
• 
Rule 1: If Priority(A) > Priority(B), A runs (B doesn’t). 

• 
Rule 2: If Priority(A) = Priority(B), A & B run in RR. 

• 	
Rule 3: When a job enters the system, it is placed at the highest priority (the topmost queue). 

• 	
Rule 4: Once a job uses up its time allotment at a given level (regardless of how many times it has given up the CPU), its priority is reduced (i.e., it moves down one queue). 

• 	
Rule 5: After some time period S,move all the jobs in the sys­tem to the topmost queue. 


MLFQ is interesting because instead of demanding apriori knowl­edge of the nature of a job, it instead observes the execution of a 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
job and prioritizes it accordingly. In this way, it manages toachieve the best of both worlds: it can deliver excellent overall performance (similar to SJF/STCF) for short-running interactive jobs, and is fair and makes progress for long-running CPU-intensive workloads. For thisreason,manysystems,includingBSD UNIX derivatives[LM+89,B86], Solaris [M06], and Windows NT and subsequent Windows operating systems [CS97] use a form of MLFQ as their base scheduler. 
DESIGN TIP:LEARNING FROM HISTORY 
The multi-level feedback queue is an excellent example of a system 
that learns from the past to predict the future. Such approaches 
are common in operating systems (and many other places in Com­
puter Science, including hardware branch predictors and caching al­
gorithms). Such approaches work when jobs have phases of behav­
ior and are thus predictable; of course, one must be careful with such 
techniques, as they can easily be wrong and drive a system to make 
worse decisions than they would have with no knowledge at all. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

References 
[AD00] “Multilevel Feedback Queue Scheduling in Solaris” Andrea Arpaci-Dusseau Available: http://www.cs.wisc.edu/˜remzi/solaris-notes.pdf 
Agreat short set of notes by one of the authors on the details ofthe Solaris scheduler. OK, we are probably biased in this description, but the notes are prettydarn good. 
[B86] “The Design of the UNIX Operating System” 
M.J. Bach Prentice-Hall, 1986 One of the classic old books on how a real UNIX operating system is built; a de.nite must-read for kernel hackers. 
[C+62] “An Experimental Time-Sharing System” 
F. J. Corbato, M. M. Daggett, R. C. Daley IFIPS 1962 
Abit hard to read, but the source of many of the .rst ideas in multi-level feedback scheduling. 
Much of this later went into Multics, which one could argue wasthe most in.uential operating 
system of all time. 

[CS97] “Inside Windows NT” 
Helen Custer and David A. Solomon 
Microsoft Press, 1997 
The NT book, if you want to learn about something other than UNIX.Of course, why would you? 
OK, we’re kidding; you might actually work for Microsoft someday you know. 

[E95] “An Analysis of Decay-Usage Scheduling in Multiprocessors” 
D.H.J. Epema SIGMETRICS ’95 
Anice paper on the state of the art of scheduling back in the mid1990s, includinga good overview of the basic approach behind decay-usage schedulers. 
[LM+89] “The Design and Implementation of the 4.3BSD UNIX Operating System” 
S.J. Lef.er, M.K. McKusick, M.J. Karels, J.S. Quarterman Addison-Wesley, 1989 
Another OS classic, written by four of the main people behind BSD. The later versions of this book, while more up to date, don’t quite match the beauty of this one. 
[M06] “Solaris Internals: Solaris 10 and OpenSolaris KernelArchitecture” Richard McDougall Prentice-Hall, 2006 
Agood book about Solaris and howit works. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
[O11] “John Ousterhout’s Home Page” 
John Ousterhout 
Available: http://www.stanford.edu/˜ouster/ 

The home page of the famous Professor Ousterhout. The two co-authors of this book had the pleasure of taking graduate operating systems from Ousterhout while in graduate school; indeed, this is where the two co-authors got to know each other, eventually leading to marriage, kids, and even this book. Thus, you really can blame Ousterhout for thisentire mess you’re in. 
[P+95] “Informed Prefetching and Caching” 
R.H. Patterson, G.A. Gibson, E. Ginting, D. Stodolsky, J. Zelenka SOSP ’95 
Afun paper about some very cool ideas in .le systems, including how applications can give the OS advice about what .les it is accessing and how it plans to access them. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

Homework 
This program, scheduler-mlfq.py,allows you to see how the MLFQ scheduler presented in this chapter behaves. As before,you can use this to generate problems for yourself using random seeds, or use it to construct a carefully-designed experiment to see how MLFQ works under different circumstances. To run the program, type: 
prompt> ./scheduler-mlfq.py 
Use the help .ag (-h)to see the options: 
Usage: scheduler-mlfq.py [options]
Options:-h, --help show this help message and exit -s SEED, --seed=SEED the random seed -n NUMQUEUES, --numQueues=NUMQUEUES 
number of queues in MLFQ (if not using -Q) 
-q QUANTUM, --quantum=QUANTUM
length of time slice (if not using -Q) 

-Q QUANTUMLIST, --quantumList=QUANTUMLIST
length of time slice per queue level,
specified as x,y,z,... where x is the 
quantum length for the highest-priority
queue, y the next highest, and so forth 

-j NUMJOBS, --numJobs=NUMJOBS
number of jobs in the system 
-m MAXLEN, --maxlen=MAXLEN 
max run-time of a job (if random) 
-M MAXIO, --maxio=MAXIO 
max I/O frequency of a job (if random) 

-B BOOST, --boost=BOOST 
how often to boost the priority of all 
jobs back to high priority (0 means never)

-i IOTIME, --iotime=IOTIME 
how long an I/O should last (fixed constant) 
-S, --stay reset and stay at same priority level 
when issuing I/O

-l JLIST, --jlist=JLIST
acomma-separated list ofjobs torun, 
in the form x1,y1,z1:x2,y2,z2:... where
xis starttime,y is runtime,and z 
is how often the job issues an I/O request 

-c compute answers for me 
There are a few different ways to use the simulator. One way is to generate some random jobs and see if you can .gure out how they will behave given the MLFQ scheduler. For example, if you wanted 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
to create a randomly-generated three-job workload, you would sim­ply type: 
prompt> ./scheduler-mlfq.py -j 3 
What you would then see is the speci.c problem de.nition: 
Here is the list of inputs:OPTIONS jobs 3OPTIONS queues 3OPTIONS quantum length for queue 2 is 10 OPTIONS quantum length for queue 1 is 10 OPTIONS quantum length for queue 0 is 10 OPTIONS boost 0 OPTIONS ioTime 0 OPTIONS stayAfterIO False 
For each job, three defining characteristics are given:startTime : at what time does the job enter the system runTime : the total CPU time needed by the job to finish ioFreq : every ioFreq time units, the job issues an I/O
(the I/O takes ioTime units to complete) 
Job List: Job 0: startTime 0 -runTime 84 -ioFreq 7 Job 1: startTime 0 -runTime 42 -ioFreq 2 Job 2: startTime 0 -runTime 51 -ioFreq 4 
Compute the execution trace for the given workloads.
If you would like, also compute the response and turnaround
times for each of the jobs. 

Use the -c flag to get the exact results when you are finished. 

This generates a random workload of three jobs (as speci.ed),on the default number of queues with a number of default settings. If you run again with the solve .ag on (-c), you’ll see the same print out as above, plus the following: 
OPERATING SYSTEMS ARPACI-DUSSEAU 
Execution Trace:  
[time 0] JOB BEGINS by JOB 0[time 0] JOB BEGINS by JOB 1[time 0] JOB BEGINS by JOB 2[time 0] Run JOB 0 at PRI 2 [TICKSLEFT 9 RUNTIME 84 TIMELEFT 83] [time 1] Run JOB 0 at PRI 2 [TICKSLEFT 8 RUNTIME 84 TIMELEFT 82] [time 2] Run JOB 0 at PRI 2 [TICKSLEFT 7 RUNTIME 84 TIMELEFT 81] [time 3] Run JOB 0 at PRI 2 [TICKSLEFT 6 RUNTIME 84 TIMELEFT 80] [time 4] Run JOB 0 at PRI 2 [TICKSLEFT 5 RUNTIME 84 TIMELEFT 79] [time 5] Run JOB 0 at PRI 2 [TICKSLEFT 4 RUNTIME 84 TIMELEFT 78] [time 6] Run JOB 0 at PRI 2 [TICKSLEFT 3 RUNTIME 84 TIMELEFT 77][time 7] IO_START by JOB 0[time 7] Run JOB 1 at PRI 2 [TICKSLEFT 9 RUNTIME 42 TIMELEFT 41] [time 8] Run JOB 1 at PRI 2 [TICKSLEFT 8 RUNTIME 42 TIMELEFT 40][time 9] IO_START by JOB 1  
...  
Final statistics: Job 0: startTime Job 1: startTime Job 2: startTime  0 -response0 -response0 -response  0 -turnaround 175 7 -turnaround 191 9 -turnaround 168  
Avg  2: startTime n/a -response 5.33 -turnaround 178.00  
The trace shows exactly, on a millisecond-by-millisecond time scale, what the scheduler decided to do. In this example, it begins byrun­ning Job 0 for 7 ms until Job 0 issues an I/O; this is entirely pre­dictable, as Job 0’s I/O frequency is set to 7 ms, meaning that every 7ms itruns, itwill issue an I/O and waitfor itto complete before continuing. At that point, the scheduler switches to Job 1, which only runs 2 ms before issuing an I/O. The scheduler prints the entire ex­ecution trace in this manner, and .nally also computes the response and turnaround times for each job as well as an average. You can also control various other aspects of the simulation.For example, you can specify how many queues you’d like to have in the system (-n)and what the quantum length should be for all of those queues (-q); if you want even more control and varied quanta length per queue, you can instead specify the length of the quantum for each queue with -Q,e.g., -Q 10,20,30 simulates a scheduler with three queues, with the highest-priority queue having a 10-ms time slice, the next-highest a 20-ms time-slice, and the low-priority queue a 30-ms time slice.  
ARPACI-DUSSEAU  THREE EASY PIECES (V0.5)  

If you are randomly generating jobs, you can also control how long they might run for (-m), or how often they generate I/O (-M). If you, however, want more control over the exact characteristics of the jobs running in the system, you can use -l (lower-case “L”) or --jlist,which allows you to specify the exact set of jobs you wish to simulate. The list is of the form: x1,y1,z1:x2,y2,z2:... where x is the start time of the job, y is the run time (i.e., how much CPU time it needs), and z the I/O frequency (i.e., after running z ms, the job issues an I/O; if z is 0, no I/Os are issued). 
For example, if you wanted to recreate the example in Figure 8.4, you would specify a job list as follows: 
prompt> ./scheduler-mlfq.py --jlist 0,180,0:100,20,0 -Q 10,10,10 
Running the simulator in this way creates a three-level MLFQ, with each level having a 10-ms time slice. Two jobs are created: Job 0 which starts at time 0, runs for 180 ms total, and never issues an I/O; Job 1 starts at 100 ms, needs only 20 ms of CPU time to complete, and also never issues I/Os.
Finally, there are three more parameters of interest. The -B .ag, if 
set to a non-zero value, boosts all jobs to the highest-priority queue 
every N milliseconds, when invoked as such: 
prompt> ./scheduler-mlfq.py -B N 
The scheduler uses this feature to avoid starvation as discussed in the chapter. However, it is off by default. 
The -S .ag invokes older Rules 4a and 4b, which means that if ajob issues an I/O before completing its time slice, itwill return to that same priority queue when it resumes execution, with its full time-slice intact. This enables gaming of the scheduler. 
Finally, you can easily change how long an I/O lasts by using the -i .ag. By default in this simplistic model, each I/O takes a .xed amount of time of 5 milliseconds or whatever you set it to with this .ag. 
Questions 
1. Run a few randomly-generated problems with just two jobs and two queues; compute the MLFQ execution trace for each. Make your life easier by limiting the length of each job and turning off I/Os. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
2. 
How would you run the scheduler to reproduce each of the examples in the chapter? 

3. 
How would you con.gure the scheduler parameters to behave just like a round-robin scheduler? 

4. 
Craft a workload with two jobs and scheduler parameters so that one job takes advantage of the older Rules 4a and 4b (turned on with the -S .ag) to game the scheduler and obtain 99% of the CPU over a particular time interval. 

5. 
Given a system with a quantum length of 10 ms in its highest queue, how often would you have to boost jobs back to the highest priority level (with the -B .ag) in order to guarantee that a single long-running (and potentially-starving) job gets at least 5% of the CPU? 


THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 



Scheduling: Proportional Share 
In this chapter, we’ll examine a different type of scheduler known as a proportional-share scheduler, also sometimes referred to as a fair-share scheduler. Proportional-share is based around a simple concept: instead of optimizing for turnaround or response time, a scheduler might instead try to simply guarantee that each jobobtain acertain percentage of CPU time. 
An excellent modern example of proportional-share scheduling is 
found in research by Waldspurger and Weihl [WW94], and is known 
as lottery scheduling;however, the idea iscertainly much older [KL88]. 
The basic idea is quite simple: every so often, hold a lottery to deter­
mine which process should get to run next; processes that should run 
more often should be given more chances to win the lottery. Easy, no? 
Now, onto the details! 
9.1 Basic Concept: Tickets Represent Your Share 
Underlying lottery scheduling is one very basic concept: tickets, which are used to represent the share of a resource that a process (or user or whatever) should receive. The percent of tickets that a process has represents its share of the system resource in question. 
Let’s look at an example. Imagine two processes, A and B, and further that A has 75 tickets while B has only 25. Thus, what we would like is for A to receive 75% of the CPU and B the remaining 25%. 
Lottery scheduling achieves this probabilistically (but not deter­ministically) by holding a lottery every so often (say, everytime slice). 
99 
Holding a lottery is straightforward: the scheduler must know how many total tickets there are (in our example, there are 100). The scheduler then picks a winning ticket, which is a number from 0to 
99 1 Assuming A holds tickets 0 through 74 and B 75 through 99, the winning ticket simply determines whether A or B runs. The sched­uler then loads the state of that winning process and runs it. 
Here is an example output of a lottery scheduler’s winning tick­ets: 
6385703976172941363910996883636243 04949 
Here is the resulting schedule: 
ABAABAAAAAABABAAAAAA 
As you can see from the example, the use of randomness in lottery scheduling leads to a probabilistic correctness in meeting the desired proportion, but no guarantee. In our example above, B only gets to run 4 out of 20 time slices (20%), instead of the desired 25% alloca­tion. However, the longer these two jobs compete, the more likely they are to achieve the desired percentages. 

9.2 Ticket Mechanisms 
Lottery scheduling also provides a number of mechanisms to ma­nipulate tickets in different and sometimes useful ways. Oneway is with the concept of ticket currency.Currency allows a user with a set of tickets to allocate tickets among their own jobs in whatever cur­rency they would like; the system then automatically converts said currency into the correct global value. 
For example, assume users A and B have each been given 100 tickets. User A is running two jobs, A1 and A2, and gives them each 500 tickets (out of 1000 total) in User A’s own currency. User Bis running only 1 job and gives it 10 tickets (out of 10 total). Thesystem will convert A1’s and A2’s allocation from 500 each in A’s currency to 50 each in the global currency; similarly, B1’s 10 tickets will be converted to 100 tickets. The lottery will then be held over the global ticket currency (200 total) to determine which job runs. 
1
Computer Scientists always start counting at 0. It is so odd tonon-computer-types that famous people have felt obliged to write about why we do itthis way [D82]. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
DESIGN TIP:RANDOMNESS One of the most beautiful aspects of lottery scheduling is itsuse of randomness.When you have to make a decision, using such a ran­domized approach is often a robust and simple way of doing so.  
Random approaches has at least three advantages over more tradi­tional decisions. First, random often avoids strange corner-case be­haviors that a more traditional algorithm may have trouble handling. For example, consider LRU page replacement (studied in more detail in a future chapter on virtual memory); while often a good replace­ment algorithm, LRU performs pessimally for some cyclic-sequential workloads. Random, on the other hand, has no such worst case.  
Second, random also is lightweight, requiring little state to track al­ternatives. In a traditional fair-share scheduling algorithm, tracking how much CPU each process has received requires per-process ac­counting, which must be updated after running each process. Doing so randomly necessitates only the most minimal of per-process state (e.g., the number of tickets each has).  
Finally, random can be quite fast. As long as generating a random number is quick, making the decision is also, and thus random can be used in a number of places where speed is required. Of course, the faster the need, the more random tends towards pseudo-random.  
User A -> 500 (A’s currency) to A1 -> 50 (global currency) -> 500 (A’s currency) to A2 -> 50 (global currency) User B -> 10 (B’s currency) to B1 -> 100 (global currency)  
Another useful mechanism is ticket transfer.With transfers, a process can temporarily hand off its tickets to another process. This ability is especially useful in a client/server setting, where a client process sends a message to a server asking it to do some work on the client’s behalf. To speed up the work, the client can pass the tickets to the server and thus try to maximize the performance of the server while the server is handling the client’s request. When .nished, the server then transfers the tickets back to the client and all isasbefore. Finally, ticket in.ation can sometimes be a useful technique. With in.ation, a process can temporarily raise or lower the numberof tick- 
ARPACI-DUSSEAU  THREE EASY PIECES (V0.5)  

ets it owns. Of course, in a competitive scenario with processes that do not trust one another, this makes little sense; one greedy process could give itself a vast number of tickets and take over the machine. Rather, in.ation can be applied in an environment where a group of processes trust one another; in such a case, if any one processknows it needs more CPU time, it can boost its ticket value as a way to re­.ect that need to the system, all without communicating with any other processes. 

9.3 Implementation 
Probably the most amazing thing about lottery scheduling is the simplicity of its implementation. Basically, all you need isa good random number generator to pick the winning ticket, a simple data structure to track the processes of the system (e.g., a list),and the total number of tickets. 
Let’s assume we keep the processes in a list. Here is an exam­
ple list comprised of three processes, A, B, and C, each with some 
number of tickets. 
head -> ( A | 100 ) -> ( B| 50 ) ->( C |250 ) -> null 
To make a scheduling decision, we .rst have to pick a random number (the winner) from the total number of tickets (400). Let’s say we pick the number 300. Then, we simply traverse the list, witha simple counter used to help us .nd the winner. Here is some exam­ple code that will do just that: 
counter = 0; 
winner = random(totaltickets); // get winner 
list_t *current = head; 

// loop until the sum of ticket values is > the winner 

while (current) { counter = counter + current->tickets; if (counter > winner)
break; // found the winner 
current = current->next; }// current is the winner: schedule it... 
All the code does is walk the list of processes, adding their ticket value to counter until the value exceeds winner.Once that is the 
OPERATING SYSTEMS ARPACI-DUSSEAU 
case, the current list element is the winning process. With our exam­ple of the winning ticket being 300, the following would take place. First, counter would be incremented to 100 to account for A’s tickets; because 100 is less than 300, the loop would continue. Then, counter would be updated to 150 (B’s tickets), still less than 300 and thus again we continue. Finally, the counter is updated to 400, clearly greater than 300, and thus we would break out of the loop with current pointing at process C as the winner. 
To make this process most ef.cient, it might generally be bestto organize the list in sorted order, from the highest number of tickets to the lowest. The ordering does not effect the correctness ofthe algorithm; however, it does ensure in general that the fewestnumber of list iterations are taken. 

9.4 How To Assign Tickets? 
One problem we have not addressed with lottery scheduling is: how to assign tickets to jobs? This problem is a tough one, because of course how the system behaves is strongly dependent on how tickets are allocated. One approach is to assume that the users know best; in such a case, each user is handed some number of tickets, and a user can allocate tickets to any jobs they run as desired. However,this solution is a non-solution: it really doesn’t tell you what todo. Thus, given a set of jobs, the “ticket-assignment problem” remainsopen. 

9.5 Why Not Deterministic? 
You might also be wondering: why use randomness at all? As we saw above, while randomness gets us a simple (and approxi­mately correct) scheduler, it occasionally will not deliverthe exact right proportions, especially over short time scales. For this reason, Waldspurger invented stride scheduling,a deterministic fair-share scheduler [W95]. 
Stride scheduling is also straightforward. Each job in the system has a stride, which is inverse in proportion to the number of tickets it has. In our example above, with jobs A, B, and C, with 100, 50, and 250 tickets, respectively, we can compute the stride of each by dividing some large number by the number of tickets each process has been assigned. For example, if we divide 10,000 by each of those 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
Pass(A) Pass(B) Pass(C) Who Runs? (stride=100) (stride=200) (stride=40) 
0 00 1000 0 100 200 0 100 200 40 100 200 80 100 200 120 200 200 120 200 200 160 200 200 200 A B C C C A C C ... 
Table 9.1: Stride Scheduling: A Trace 
ticket values, we obtain the following stride values for A, B,andC: 100, 200, and 40. We call this value the stride of each process; every time a process runs, we will increment a counter for it (calledits pass value) by its stride to track its global progress. 
The scheduler then uses the stride and pass to determine which process should run next. The basic idea is simple: at any giventime, pick the process to run that has the lowest pass value so far; when you run a process, increment its pass counter by its stride. A pseu­docode implementation [W95]: 
// select client with minimum pass value 
current = queue_remove_min(queue);
// use resource for quantum
schedule(current);
// compute next pass using stride 
current->pass += current->stride; 
// put back into the queue
queue_insert(queue, current); 

In our example, we start with three processes (A, B, and C), with stride values of 100, 200, and 40, and all with pass values initially at 
0. Thus, at .rst, any of the processes might run, as their pass values are equally low. Assume we pick A. A runs, and when .nished with the time slice, we update its pass value to 100. Then we run B, whose pass value is then set to 200. Finally, we run C, whose pass value is incremented to 40. At this point, the algorithm will pick the low­est pass value, which is C’s, and run it, updating its pass to 80(C’s stride is 40, as you recall). Then C will run again (still the lowest pass value), raising its pass to 120. A will run now, updating its pass to 200 (now equal to B’s). Then C will run twice more, updating itspass to 160 then 200. At this point, all pass values are equal again,andthe 
OPERATING SYSTEMS ARPACI-DUSSEAU 
process will repeat, ad in.nitum. Table 9.1 traces the behavior of the scheduler over time. 
As we can see from the table, C ran .ve times, A twice, and B just once, exactly in proportion to their ticket values of 250, 100, and 50. Lottery scheduling achieves the proportions probabilistically over time; stride scheduling gets them exactly right. 
So why use lottery at all? Well, lottery scheduling has one nice property that stride scheduling does not: no global state. Imagine a new job enters in the middle of our stride scheduling example above; what should its pass value be? Should it be set to 0? If so, it will monopolize the CPU. With lottery scheduling, there is no global state per process; we simply add a new process with whatever ticketsit has, update the single global variable to track how many totaltickets we have, and go from there. In this way, lottery makes it much easier to incorporate new processes in a sensible manner. 

9.6 Summary 
We have introduced the concept of proportional-share schedul­ing and brie.y discussed two implementations: lottery and stride scheduling. Lottery uses randomness in a clever way to achieve pro­portional share; stride does so deterministically. Although both are conceptually interesting, they have not achieved wide-spread adop­tion for a variety of reasons. One is that such approaches do not par­ticularly mesh well with I/O [AC97]; another is that they leave open the hard problem of ticket assignment. General-purpose schedulers such as the MLFQ we discussed previously do so more gracefully and thus are used in many systems. These schedulers thus likely would be useful in domains where some of these problems (such as assignment of shares) are relatively easy to solve, e.g., in avirtual machine environment where you decide to give one-quarter of your CPU cycles to the Windows VM and the rest to your base Linux in­stallation. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

References 
[AC97] “Extending Proportional-Share Scheduling to a Network of Workstations” Andrea C. Arpaci-Dusseau and David E. Culler PDPTA’97, June 1997 
Apaper by one of the authors on howto extend proportional-share scheduling to work better in a clustered environment. 
[D82] “Why Numbering Should Start At Zero” Edsger Dijkstra, August 1982 http://www.cs.utexas.edu/users/EWD/ewd08xx/EWD831.PDF 
Ashort note from E. Dijkstra, one of the pioneers of computer science. We’ll be hearing much more on this guy in the section on Concurrency. In the meanwhile, enjoy this note, which includes this motivating quote: “One of my colleagues – not a computingscientist –accused a number of younger computing scientists of ’pedantry’ because they started numbering at zero.” The note explains why doing so is logical. 
[KL88] “A Fair Share Scheduler” 
J. Kay and P. Lauder 
CACM, Volume 31 Issue 1, January 1988 

An early reference to a fair-share scheduler. 
[WW94] “Lottery Scheduling: Flexible Proportional-Share Resource Management” Carl A. Waldspurger and William E. Weihl OSDI ’94, November 1994 
The landmark paper on lottery scheduling that got the systemscommunity re-energized about scheduling, fair sharing, and the power of simple randomizedalgorithms. 
[W95] “Lottery and Stride Scheduling: Flexible 
Proportional-Share Resource Management” 
Carl A. Waldspurger 
Ph.D. Thesis, MIT, 1995 

The award-winning thesis of Waldspurger’s that outlines lottery and stride scheduling. If you’re thinking of writing a Ph.D. dissertation at some point, you should always have a good example around, to give you something to strive for: this is such a goodone. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
Homework  
This program, lottery.py,allows you to see how a lottery sched­uler works. As always, there are two steps to running the program. First, run without the -c .ag: this shows you what problem to solve without revealing the answers.  
prompt> ./lottery.py -j 3 -s 0 ... Here is the job list, with the run time of each job: Job 0 ( length = 8, tickets = 75 ) Job 1 ( length = 4, tickets = 25 )  
Here is the set of random numbers you will need (at most):Random 0.511274721369 Random 0.40493413745 Random 0.783798589035 Random 0.303312726079 Random 0.476596954152 Random 0.583382039455 Random 0.908112885195 Random 0.504686855817 Random 0.2818378444 Random 0.755804204157 Random 0.618368996675 Random 0.250506341362  
When you run the simulator in this manner, it .rst assigns you some random jobs (here of lengths 8, and 4), each with some number of tickets (here 75 and 25, respectively). The simulator alsogives you alist of random numbers, which you will need to determine what the lottery scheduler will do. Running with -c shows exactly what you are supposed to calcu­late:  
prompt> ./lottery.py -j 2 -s 0 -c ... ** Solutions **  
Random 0.511274721369 -> Winning ticket 51 (of 100) -> Run 0 Jobs: (* job:0 run:8 tix:75 ) ( job:1 run:4 tix:25 ) Random 0.40493413745 -> Winning ticket 40 (of 100) -> Run 0 Jobs: (* job:0 run:7 tix:75 ) ( job:1 run:4 tix:25 ) Random 0.783798589035 -> Winning ticket 78 (of 100) -> Run 1 Jobs: ( job:0 run:6 tix:75 ) (* job:1 run:4 tix:25 ) Random 0.303312726079 -> Winning ticket 30 (of 100) -> Run 0 Jobs: (* job:0 run:6 tix:75 ) ( job:1 run:3 tix:25 )  
THREE  
ARPACI-DUSSEAU  EASY PIECES  
(V0.5)  

Random 0.476596954152 -> Winning ticket 47 (of 100) -> Run 0 
Jobs: (* job:0 run:5 tix:75 ) ( job:1 run:3 tix:25 ) 
Random 0.583382039455 -> Winning ticket 58 (of 100) -> Run 0 
Jobs: (* job:0 run:4 tix:75 ) ( job:1 run:3 tix:25 ) 
Random 0.908112885195 -> Winning ticket 90 (of 100) -> Run 1 
Jobs: ( job:0 run:3 tix:75 ) (* job:1 run:3 tix:25 ) 
Random 0.504686855817 -> Winning ticket 50 (of 100) -> Run 0 
Jobs: (* job:0 run:3 tix:75 ) ( job:1 run:2 tix:25 ) 

Random 0.2818378444 -> Winning ticket 28 (of 100) -> Run 0 
Jobs: (* job:0 run:2 tix:75 ) ( job:1 run:2 tix:25 ) 
Random 0.755804204157 -> Winning ticket 75 (of 100) -> Run 1 
Jobs: ( job:0 run:1 tix:75 ) (* job:1 run:2 tix:25 ) 
Random 0.618368996675 -> Winning ticket 61 (of 100) -> Run 0 
Jobs: (* job:0 run:1 tix:75 ) ( job:1 run:1 tix:25 ) 

Random 0.250506341362 -> Winning ticket 6 (of 25) -> Run 1 Jobs: ( job:0 run:0 tix:---) (* job:1 run:1 tix:25 ) 
As you can see from this trace, what you are supposed to do is use the random number to .gure out which ticket is the winner. Then, given the winning ticket, .gure out which job should run. Repeat this until all of the jobs are .nished running. It’s as simple as that – you are just emulating what the lottery scheduler does, but byhand! 
Just to make this absolutely clear, let’s look at the .rst decision made in the example above. At this point, we have two jobs (job 0which has a runtime of 8 and 75tickets, and job 1which has a runtime of 4 and 25 tickets). The .rst random number we are given is 0.511274721369. From this, we can compute the winning ticket simply by multiplying by the total number of tickets, and taking the integer result: 0.511274721369 * 100 = 51.1274721369 = 51. 
If ticket 51 is the winner, we simply search through the job list until we .nd it. The .rst entry, for job 0, has 75 tickets (0 through 74), and thus we have found our winner, and we run job 0 for the quantum length (1 in this example). All of this is shown in the print out as follows: 
Random 0.511274721369 -> Winning ticket 51 (of 100) -> Run 0 Jobs: (* job:0 run:8 tix:75 ) ( job:1 run:4 tix:25 ) 
As you can see, the .rst line summarizes what happens, and the 
second simply shows the entire job queue, with an * denoting which 
job was chosen. 
The simulator has a few other options, most of which should be self-explanatory. Most notably, the -l/--jlist .ag can be used to 
OPERATING SYSTEMS ARPACI-DUSSEAU 
specify an exact set of jobs and their ticket values, instead of always using randomly-generated job lists. 
prompt> ./lottery.py -hUsage: lottery.py [options] 
Options:
-h, --help
show this help message and exit 
-s SEED, --seed=SEED 
the random seed 
-j JOBS, --jobs=JOBS
number of jobs in the system
-l JLIST, --jlist=JLIST
instead of random jobs, provide a comma-separated list 
of run times and ticket values (e.g., 10:100,20:100
would have two jobs with run-times of 10 and 20, each
with 100 tickets) 
-m MAXLEN, --maxlen=MAXLEN 
max length of job
-T MAXTICKET, --maxtick=MAXTICKET 
maximum ticket value, if randomly assigned
-q QUANTUM, --quantum=QUANTUM
length of time slice 
-c, --compute 
compute answers for me 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
Questions 
1. 
Compute the solutions for simulations with 3 jobs and random seeds of 1, 2, and 3. 

2. 
Now run with two speci.c jobs: each of length 10, but one (job 0) with just 1 ticket and the other (job 1) with 100 (e.g., -l 10:1,10:100). What happens when the number of tickets is so imbalanced? Will job 0 ever run before job 1 completes? How often? In general, what does such a ticket imbalance do to the behavior of lottery scheduling? 

3. 
When running with two jobs of length 100 and equal ticket al­locations of 100 (-l 100:100,100:100), how unfair is the scheduler? Run with some different random seeds to deter­mine the (probabilistic) answer; let unfairness be determined by how much earlier one job .nishes than the other. 

4. 
How does your answer to the previous question change as the quantum size (-q)gets larger? 


OPERATING SYSTEMS ARPACI-DUSSEAU 
10 




Multiprocessor Scheduling 
111 
11 


Summary Dialogue on CPU Virtualization 
Professor: So, Student, did you learn anything? 
Student: Well, Professor, that seems like a loaded question. I think you only want me to say “yes.” 
Professor: That’s true. But it’s also still an honest question. Come on, give aprofessor abreak, will you? 
Student: OK, OK. I think I did learn a few things. First, I learned a little about how the OS virtualizes the CPU. There are a bunch of important mechanisms that I had to understand to make sense of this: traps and trap handlers, timer interrupts, and how the OS and the hardware have to carefully save and restore state when switching between processes. 
Professor: Good, good! 
Student: All those interactions do seem a little complicated though; how can I learn more? 
Professor: Well, that’s a good question. I think there is no substitute for doing; just reading about these things doesn’t quite give youthe proper sense. Do the class projects and I bet by the end it will all kindof make sense. 
Student: Sounds good. What else can I tell you? 
Professor: Well, did you get some sense of the philosophy of the OS in your quest to understand its basic machinery? 
113 
Student: Hmm... I think so. It seems like the OS is fairly paranoid. It wants to make sure it stays in charge of the machine. While it wants a program to run as ef.ciently as possible (and hence the whole reasoning behind limited direct execution), the OS also wants to be able to say “Ah! Not so fast my friend” in case of an errant or malicious process. Paranoia rules the day, and certainly keeps the OS in charge of the machine. Perhaps that is why we think of the OS as a resource manager. 
Professor: Yes indeed – sounds like you are starting to put it together! Nice. 
Student: Thanks. 
Professor: And what about the policies on top of those mechanisms – any interesting lessons there? 
Student: Some lessons to be learned there for sure. Perhaps a little obvious, but obvious can be good. Like the notion of bumping short jobs to the front of the queue – I knew that was a good idea ever since the one time Iwas buying some gum at the store, and the guy in front of me had a credit card that wouldn’t work. He was no short job, let me tell you. 
Professor: That sounds oddly rude to that poor fellow. What else? 
Student: Well, that you can build a smart scheduler that tries to be like SJF and RR all at once – that MLFQ was pretty neat. Building up a real scheduler seems dif.cult. 
Professor: Indeed it is. That’s why there is still controversy to this day over which scheduler to use; see the Linux battles between CFS, BFS, and the O(1) scheduler, for example. And no, I will not spell out the full name of BFS. 
Student: And I won’t ask you to! These policy battles seem like they could rage forever; is there really a right answer? 
Professor: Probably not. After all, even our own metrics are at odds: if your scheduler is good at turnaround time, it’s bad at response time, and vice versa. As Lampson said, perhaps the goal isn’t to .nd the best solution, but rather to avoid disaster. 
Student: That’s a little depressing. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
Professor: Good engineering can be that way. And it can also be uplifting! It’s just your perspective on it, really. I personally think being pragmatic is agood thing, and pragmatists realize that not all problems have clean and easy solutions. Anything else that caught your fancy? 
Student: Ireally liked the notion of gaming the scheduler; it seems like that might be something to look into when I’m next running a job on Amazon’s EC2 service. Maybe I can steal some cycles from some other unsuspecting (and more importantly, OS-ignorant) customer! 
Professor: It looks like I might have created a monster! Professor Franken­stein is not what I’d like to be called, you know. 
Student: But isn’t that the idea? To get us excited about something, so much so that we look into it on our own? Lighting .res and all that? 
Professor: Iguess so. But Ididn’tthink it would work! 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
12 



The Abstraction: Address Spaces 
In the early days, building computer systems was easy. Why, you ask? Because users didn’t expect too much. It is those darned users with their expectations of “ease of use”, “high performance”, “relia­bility”, and so forth that really have led to all these headaches. Next time you meet one of those computer users, thank them for all the problems they have caused. 
12.1 Early Systems 
From the perspective of memory, early machines didn’t provide 
much of an abstraction to users. Basically, the physical memory of 
the machine looked something like what you see in Figure 12.1. 
The OS was a set of routines (a library, really) that sat in memory (starting at physical address 0 in this example), and there would be one running program (a process) that currently sat in physical mem­ory (starting at physical address 64k in this example) and used the rest of memory. There were few illusions here, and the user didn’t expect much from the OS. Life was sure easy for OS developers in those days, wasn’t it? 

12.2 Multiprogramming and Time Sharing 
After a time, because machines were expensive, people began to 
share machines more effectively. Thus the era of multiprogramming 
was born [DV66], in which multiple processes were ready to runat 
117 Figure 12.1: Operating Systems: The Early Days 

agiven time, and the OS would switch between them, for example when one decided to perform an I/O. Doing so increased the effec­tive utilization of the CPU. Such increases in ef.ciency were partic­ularly important in those days where each machine cost hundreds of thousands or even millions of dollars. 
Soon enough, however, people began demanding more of ma­chines, and the era of time sharing was born [S59, L60, M62, M83]. Speci.cally, many realized the limitations of batch computing, par­ticularly on programmers themselves [CV65], who were tired of long (and hence ineffective) program-debug cycles. The notion of interac­tivity thus became important, as many users might be concurrently using a machine, each waiting for (or hoping for) a timely response from their currently-executing tasks. 
One way to implement time sharing would be to run one process for a short while, giving it full access to all memory (like thepicture above), then stop it, save all of its state to some kind of disk (includ­ing all of physical memory), load some other process’s state,run it for a while, and thus implement some kind of crude sharing of the machine [M+63]. 
Unfortunately, this approach has a big problem: it is way too slow, particularly as memory grew. While saving and restoring register-level state (e.g., the PC, general-purpose registers, etc.)is relatively fast, saving the entire contents of memory to disk is brutallynon-
OPERATING SYSTEMS ARPACI-DUSSEAU 
0KB 64KB 128KB 192KB 256KB 320KB 384KB 448KB 512KB 

Figure 12.2: Three Processes: Sharing Memory 
performant. Thus, what we’d rather do is leave processes in memory while switching between them, allowing the OS to implement time sharing ef.ciently (Figure 12.2). 
In the diagram, there are three processes (A, B, and C) and each of them have a small part of the 512-KB physical memory carved out for them. Assuming a single CPU, the OS chooses to run one of the processes (say A), while the others (B and C) sit in the ready queue waiting to be run. 
As time sharing became more popular, you can probably guess that new demands were placed on the operating system. In partic­ular, allowing multiple programs to reside concurrently in memory makes protection an important issue; you don’t want a process to be able to read, or worse, write some other processes’s memory. 

12.3 The Address Space 
However, we have to keep those pesky users in mind, and doing so requires the OS to create an easy to use abstraction of physical memory. We call this abstraction the address space,andit is the run­ning program’s view of memory in the system. Understanding this fundamental OS abstraction of memory is key to your understanding of how memory is virtualized. 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 

the code segment: 
where instructions live 

the heap segment: 
contains malloc’d data 
dynamic data structures 
(it grows downward) 

(it grows upward) 
the stack segment: 
contains local variables 
arguments to routines, 
return values, etc. 

Figure 12.3: An Example Address Space 
The address space of a process contains all of the memory state of the running program. For example, the code of the program (the instructions) have to live in memory somewhere, and thus theyare in the address space. The program, while it is running, uses a stack to keep track of where it is in the function call chain as well asto allocate local variables and pass parameters and return values to and from routines. Finally, the heap is used for dynamically-allocated, user-managed memory, such as that you might receive from a call tomalloc() in C or new in an object-oriented language such as C++ or Java. Of course, there are other things in there too (like statically-initialized variables, and a few other details), but for now let us just assume those three components: code, stack, and heap. 
In the example in Figure 12.3, we have a tiny address space (only 16 KB)1.The program code livesat the top of the address space (start­ing at 0 in this example, and is packed into the .rst 1K of the address space). Code is static (and thus easy to deal with), so we can place it at the top of the address space and know that it won’t need any more space as the program runs. Next, we have the two regions of the address space that may grow 
1
We will often use small examples like this because it is a pain to represent a 32-bit address space and the numbers start to become hard to deal with. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
(and shrink) while the program runs. Those are the heap (at thetop) and the stack (at the bottom). We place them like this because each wishes to be able to grow, and by putting them at opposite ends of the address space, we can allow such growth: they just have to grow in opposite directions. The heap thus starts just after the code (at 1KB) and grows downward (say when a user requests more mem­ory via malloc()); the stack starts at 16KB and grows upward (say when a user makes a procedure call). However, this placement of stack and heap is just a convention; you could arrange the address space in a different way if you’d like (as we’ll see later, whenmul­tiple threads co-exist in an address space, no nice way to divide the address space like this works anymore, alas). 
Of course, when we describe the address space, what we are de­scribing is the abstraction that the OS is providing to the running program. The program really isn’t in memory at physical addresses 0through 16KB; rather it is loaded atsome arbitrary physicalad­dress(es). Examine processes A, B, and C in Figure 12.2; thereyou can see how each process is loaded into memory at a different ad­dress. And now, hopefully you can see the problem: 
THE CRUX:HOW TO VIRTUALIZE MEMORY 
How can the OS build this abstraction of a private, potentially large address space for multiple running processes (all sharing mem­ory) on top of a single, physical memory? 
When the OS does this, we say the OS is virtualizing memory, because the running program thinks it is loaded into memory ata particular address (say 0) and has a potentially very large address space (say 32-bits or 64-bits); the reality is quite different. 
When, for example, process A in Figure 12.2 tries to perform a load at address 0 (which we will call a virtual address), somehow the OS, in tandem with some hardware support, will have to make sure the load doesn’t actually go to physical address 0 but rather to physical address 64KB (where A is loaded into memory). Thisis the key to virtualization of memory, which underlies every modern computer system in the world. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
MANTRA:EVERY ADDRESS YOU SEE IS VIRTUAL 
Ever write a C program that prints out a pointer? The value you see (some large number, often printed in hexadecimal), is a virtual address.Ever wonder where the code of your program is found? You can print that out too, and yes, if you can print it, it also is a virtual address. In fact, any address you can see as a programmer of auser-level program is avirtual address. It’s only the OS, through its tricky techniques of virtualizing memory, that knows where in the physical memory of the machine these instructions and data values lie. So never forget: if you print out an address in a program, it’s a virtual one, an illusion of how things are laid out in memory; only the OS (and the hardware) knows the real truth. 
Here’s a little program that prints out the locations of the main() 
routine (where code lives), the value of a heap-allocated value re­
turned from malloc(),and the location of an integer on the stack: 
#include <stdio.h> 
#include <stdlib.h> 
int main(int argc, char *argv[]) {

printf("location of code : %p\n", (void *)main); 
printf("location of heap : %p\n", (void *)malloc(1)); 
int x = 3; 
printf("location of stack : %p\n", (void *)&x); 
return x; 

} 
When run on a 64-bit Mac OS X machine, we get the following output: 
location of code : 0x1095afe50 
location of heap : 0x1096008c0 
location of stack : 0x7fff691aea64 

From this, you can see that code comes .rst in the address space, then the heap, and the stack is all the way at the other end of this large virtual space. All of these addresses are virtual, and will be translated by the OS and hardware in order to fetch values fromtheir true physical locations. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

12.4 Goals 
Thus we arrive at the job of the OS in this set of notes: to virtualize memory. The OS will not only virtualize memory, though; it will do so with style. To make sure the OS does so, we need some goals to guide us. We have seen these goals before (think of the Introduction), and we’ll see them again, but they are certainly worth repeating. 
One of the major goals of any virtual memory (VM) system is 
transparency.The OS should implement virtual memory in a way 
that is transparent to the running program. Thus, the program shouldn’t 
be aware of the fact that memory is virtualized; rather, the program 
behaves as if it has its own private physical memory. Behind the 
scenes, the OS (and hardware) does all the work to multiplex mem­
ory among many different jobs, and hence implements the illusion. 
Another goal of VM is ef.ciency.The OS should strive to make the virtualization as ef.cient as possible, both in terms of time (i.e., not making programs run much more slowly) and space (i.e., not using too much memory for structures needed to support virtualiza­tion). In implementing time-ef.cient virtualization, the OS will have to rely on hardware support, including hardware features such as TLBs (which we will learn about in due course). 
Finally, a third VM goal is protection.The OS should make sure to protect processes from one another as well as the OS itself from pro­cesses. When one process performs a load, a store, or an instruction fetch, it should not be able to access or affect in any way the memory contents of any other process or the OS itself (that is, anything outside its address space). Protection thus enables us to deliver theproperty of isolation among processes; each process should be running in its own isolated cocoon, safe from the ravages of other faulty or even malicious processes. 
In the next chapters, we’ll focus our exploration on the basicmech­anisms needed to virtualize memory, including hardware and oper­ating systems support. We’ll also investigate some of the more rel­evant policies that you’ll encounter in operating systems, including how to manage free space and which pages to kick out of memory when you run low on space. In doing so, we’ll build up your under­
standing of how a modern VM system really works 2. 
2
Or, we’ll convince you to drop the class. On a more positive note, if you make it through virtual memory, you’re probably going to make it all the way. So hold on! 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
DESIGN TIP:PRINCIPLE OF ISOLATION 
Isolation is a key principle in building reliable systems. Iftwo enti­
ties are properly isolated from one another, this implies that one can 
fail without affecting the other. Operating systems strive to isolate 
processes from each other and in this way prevent one from harm­
ing the other. By using memory isolation, the OS further ensures 
that running programs cannot affect the operation of the underly­
ing OS. Some modern OS’s take isolation even further, by walling 
off pieces of the OS from other pieces of the OS. Such microkernels 
[BH70,R+89,S+03] thus may provide greater reliability thantypical 
monolithic kernel designs. 

12.5 Summary 
We have seen the introduction of a major OS subsystem: virtual memory. The VM system is responsible for providing the illusion of a large, sparse, private address space to programs, which hold all of their instructions and data therein. The OS, with some serious hardware help, will take each of these virtual memory references, and turn them into physical addresses, which can be presentedto the physical memory in order to fetch the desired information. The OS will do this for many processes at once, making sure to protect programs from one another, as well as protect the OS. The entire ap­proach requires a great deal of mechanism (lots of low-level machin­ery) as well as some critical policies to work; we’ll start from the bottom up, describing the critical mechanisms .rst. And thuswe proceed! 
OPERATING SYSTEMS ARPACI-DUSSEAU 

References 
[BH70] “The Nucleus of a Multiprogramming System” Per Brinch-Hansen Communications of the ACM, 13:4, April 1970 
The .rst paper to suggest that the OS, or kernel, should be a minimal and .exible substrate for building customized operating systems; this theme is revisited throughout OS research history. 
[CV65] “Introduction and Overview of the Multics System” 
F. J. Corbato and V. A. Vyssotsky Fall Joint Computer Conference, 1965 
Agreat early Multics paper. Here is the great quote about timesharing: “The impetus for time­sharing .rst arose from professional programmers because oftheir constant frustration in de­bugging programs at batch processing installations. Thus, the original goal was to time-share computers to allow simultaneous access by several persons while giving to each of them the illu­sion of having the whole machine at his disposal.” 
[DV66] “Programming Semantics for Multiprogrammed Computations” Jack B. Dennis and Earl C. Van Horn Communications of the ACM, Volume 9, Number 3, March 1966 
An early paper (but not the .rst) on multiprogramming. 
[L60] “Man-Computer Symbiosis” 
J. C. R. Licklider IRE Transactions on Human Factors in Electronics, HFE-1:1, March 1960 
Afunky paper about howcomputers and people are going to enterinto a symbiotic age; clearly well ahead of its time but a fascinating read nonetheless. 
[M62] “Time-Sharing Computer Systems” 
J. McCarthy Management and the Computer of the Future, MIT Press, Cambridge, Mass, 1962 
Probably McCarthy’s earliest recorded paper on time sharing. However, in another paper [M83], 
he claims to have been thinking of the idea since 1957. McCarthy left the systems area and went 
on to become a giant in Arti.cial Intelligence at Stanford, including the creation of the LISP pro­gramming language. See McCarthy’s home page for more info: http://www-formal.stanford.edu/jmc/ 

[M+63] “A Time-Sharing Debugging System for a Small Computer” 
J. McCarthy, S. Boilen, E. Fredkin, J. C. R. Licklider AFIPS ’63 (Spring), May, 1963, New York, USA 
Agreat early example of a system that swapped program memory to the “drum” when the pro­gram wasn’t running, and then back into “core” memory when it was about to be run. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
[M83] “Reminiscences on the History of Time Sharing” John McCarthy Winter or Spring of 1983 Available: http://www-formal.stanford.edu/jmc/history/timesharing/timesharing.html 
Aterri.c historical note on where the idea of time-sharing might have come from, including some doubts towards those who cite Strachey’s work [S59] as the pioneering work in this area. 
[R+89] “Mach: A System Software kernel” Richard Rashid, Daniel Julin, Douglas Orr, Richard Sanzi, Robert Baron, Alessandro Forin, David Golub, Michael Jones COMPCON 89, February 1989 
Although not the .rst project on microkernels per se, the Machproject at CMU was well-known and in.uential; it still lives today deep in the bowels of Mac OS X. 
[S59] “Time Sharing in Large Fast Computers” 
C. Strachey Proceedings of the International Conference on InformationProcessing, UNESCO, June 1959 
One of the earliest references on time sharing. 
[S+03] “Improving the Reliability of Commodity Operating Systems” Michael M. Swift, Brian N. Bershad, Henry M. Levy SOSP 2003 
The .rst paper to show how microkernel-like thinking can improve operating system reliability. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
13 



Interlude: Memory API 
Inthisinterlude,wediscussthememoryallocationinterfacesin UNIX systems. The interfaces provided are quite simple, and hencethe chapter is short and to the point1. 
13.1 Types of Memory 
In running a C program, there are two types of memory that are allocated. The .rst is called stack memory, and allocations and deal-locations of it are managed implicitly by the compiler for you, the programmer; for this reason it is sometimes called automatic mem­ory. 
Declaring memory on the stack in C is easy. For example, let’s say 
you need some space in a function func() for an integer, called x. 
To declare such a piece of memory, you just do something like this: 
void func() {
int x; // declares an integer on the stack 
... 

} 
The compiler does the rest, making sure to make space on the 
stack when you call into func().When your return from the func­
tion, the compiler deallocates the memory for you; thus, if you want 
1
Indeed, we hope all chapters are short and to the point! But this one is shorter and pointier, we think. 
127 
some information to live beyond the call invocation, you had better not leave that information on the stack. 
It is this need for long-lived memory that gets us to the second type of memory, called heap memory, where all allocations and deal-locations are explicitly handled by you, the programmer. A heavy re­sponsibility, no doubt! And certainly the cause of many bugs.But if you are careful and pay attention, you will use such interfaces cor­rectly and without too much trouble. Here is an example of how one might allocate a pointer to an integer on the heap: 
void func() {int *x=(int *)malloc(sizeof(int)); ... 
} 
Acouple of notes about this small code snippet. First, you might notice that both stack and heap allocation occur on this line:.rst the compiler knows to make room for a pointer to an integer when it sees int *x;subsequently, when the program calls malloc(), it requests space for an integer on the heap; the routine returns the address of such an integer (upon success, or NULL on failure), which is then stored on the stack for use by the program. 
Because of its explicit nature, and because of its more variedus­
age, heap memory presents more challenges to both users and sys­
tems. Thus, it is the focus of the remainder of our discussion. 

13.2 The malloc() Call 
The malloc() call is quite simple: you pass it a size asking for 
some room on the heap, and it either succeeds and gives you backa 
pointer to the newly-allocated space, or fails and returns NULL2. 
The manual page shows what you need to do to use malloc; type man malloc at the command line and you will see: 
#include <stdlib.h> 
... 
void *malloc(size_t size); 
2
Note that NULL in C isn’t really anything special at all; it is just a macro forthe value zero. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
From this information, you can see that all you need to do is in­clude the header .le stdlib.h to use malloc. In fact, you don’t re­ally need to even do this, as the C library, which all C programslink with by default, has the code for malloc() inside of it; adding the header just lets the compiler check whether you are calling malloc() correctly (e.g., passing the right number of arguments to it,of the right type). 
The single parameter malloc() takes is of type size t which simply describes how many bytes you need. However, most pro­grammers do not type in a number here directly (such as 10); indeed, it would be considered poor form to do so. Instead, various routines and macros are utilized. For example, to allocate space for a double-precision .oating point value, you simply do this: 
double *d=(double *)malloc(sizeof(double)); 
This invocation of malloc() uses the sizeof() operator to re­quest the right amount of space; in C, this is generally thought of as a compile-time operator, meaning that the actual size is known at compile time and thus a number (in this case, 8, for a double) is sub­stituted as the argument to malloc().For this reason, sizeof() is correctly thought of as an operator and not a function call (a function call would take place at run time). 
CODING TIP:WHEN IN DOUBT,TRY IT OUT If you aren’t sure how some routine or operator you are using be­haves, there is no substitute for simply trying it out and making sure it behaves as you expect. While reading the manual pages or other documentation is useful, how it works in practice is what matters. Write some code and test it! That is no doubt the best way to make sure your code behaves as you desire. Indeed, that is what we did to double-check the things we were saying about sizeof() were actually true! 
You can also pass in the name of a variable (and not just a type) tosizeof(),but in some cases you may not get the desired results,so be careful. For example, let’s look at the following code snippet: 
int *x= malloc(10 * sizeof(int));
printf("%d\n", sizeof(x)); 

THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

In the .rst line, we’ve declared space for an array of 10 integers, which is .ne and dandy. However, when we use sizeof() in the next line, it returns a small value, such as 4 (on 32-bit machines) or 8(on 64-bitmachines). The reason is thatin this case, sizeof() thinks we are simply asking how big a pointer to an integer is, not how much memory we have dynamically allocated. However, some­times sizeof() does work as you might expect: 
int x[10];
printf("%d\n", sizeof(x)); 

In this case, there is enough static information for the compiler to know that 40 bytes have been allocated. 
Another place to be careful is with strings. When declaring space for a string, use the following idiom: malloc(strlen(s) + 1), which gets the length of the string using the function strlen(), and adds 1 to it in order to make room for the end-of-string character. Using sizeof() may lead to trouble here. 
You might also notice that malloc() returns a pointer to type 
void.Doing so is just the way in C to pass back an address and 
let the programmer decide what to do with it. The programmer fur­
ther helps out by using what is called a cast;in our example above, 
the programmer casts the return type of malloc() to a pointer to 
a double.Casting doesn’t really accomplish anything, other than 
tell the compiler and other programmers who might be reading your 
code: “yeah, I know what I’m doing.” By casting the result of malloc(), 
the programmer is just giving some reassurance; the cast is not needed 
for the correctness of the program. 

13.3 The free() Call 
As it turns out, allocating memory is the easy part of the equation; knowing when, how, and even if to free memory is the hard part. To free heap memory that is no longer in use, programmers simply callfree(),asfollows: 
int *x=malloc(10 * sizeof(int)); 
... 
free(x); 
The routine takes one argument, a pointer that was returned bymalloc().Thus, you might notice, the size of the allocated region 
OPERATING SYSTEMS ARPACI-DUSSEAU 
is not passed in by the user, and must be tracked by the memory-allocation library itself. 

13.4 Common Errors 
There are a number of common errors that arise in the use of malloc() and free().Here are some we’ve seen over and over again in teaching the undergraduate operating systems course. All of these examples compile and run with nary a peep from the com­piler; while compiling a C program is necessary to build a correct C program, it is far from suf.cient, as you will learn. 
Correct memory management has been such a problem, in fact, that many newer languages have support for automatic memory management.In such languages, while you call something akin to malloc() to allocate memory (usually new or something similar to allocate a new object), you never have to call something to free space; rather, a garbage collector runs and .gures out what memory you no longer have references to and frees it for you. 
Forgetting To Allocate Memory 
Many routines expect memory to be allocated before you call them. For example, the routine strcpy(dst, src) copies a string from asource pointer to a destination pointer. However, if you arenot careful, you might do this: 
char *src = "hello"; 
char *dst; // oops! unallocated
strcpy(dst, src); // segfault and die 

When you run this code, it will likely lead to a segmentation fault3,which is a fancy term for YOU DID SOMETHING WRONG WITH MEMORY YOU FOOLISH PROGRAMMER AND I AM ANGRY. 
In this case, the proper code might instead look like this: 
char *src = "hello"; 
char *dst = (char *)malloc(strlen(src) + 1); 
strcpy(dst, src); // work properly 

3
Although it sounds arcane, you will soon learn why such an illegal memory access is called a segmentation fault; if that isn’t incentive to read on, what is? 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
Alternately, you could use strdup() and make your life even easier. Read the strdup man page for more information. 

Not Allocating Enough Memory 
Arelated error is not allocating enough memory, sometimes called a buffer over.ow.In the example above, a common error is to make almost enough room for the destination buffer. 
char *src = "hello"; 
char *dst = (char *)malloc(strlen(src)); //too small! 
strcpy(dst, src); // work properly 

Oddly enough, depending on how malloc is implemented and many other details, this program will often run seemingly correctly. In some cases, when the string copy executes, it writes one byte too far past the end of the allocated space, but in some cases this is harm­less, perhaps overwriting a variable that isn’t used anymore. In some cases, these over.ows can be incredibly harmful, and in fact are the source of many security vulnerabilities in systems [W06]. Inother cases, the malloc library allocated a little extra space anyhow, and thus your program actually doesn’t scribble on some other variable’s value and works quite .ne. In even other cases, the program will in­deed fault and crash. And thus we learn another valuable lesson: even though it ran correctly once, doesn’t mean it’s correct. 
CODING TIP:IT COMPILED OR IT RAN .IT IS CORRECT 
= 
Just because a program compiled(!) or even ran once or many times correctly does not mean the program is correct. Many events may have conspired to get you to a point where you believe it works,but then something changes and it stops. A common student reaction is to say (or yell) “But it worked before!” and then blame the compiler, operating system, hardware, or even (dare we say it) the professor. But the problem is usually right where you think it would be, inyour code. Get to work and debug it before you blame those other com­ponents. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

Forgetting to Initialize Allocated Memory 
With this error, you call malloc() properly, but forget to .ll in some values into your newly-allocated data type. Don’t do this! Ifyou do forget, your program will eventually encounter an uninitialized read,where it reads from the heap some data of unknown value. Who knows what might be in there? If you’re lucky, some value such that the program still works (e.g., zero). If you’re not lucky, something random and harmful. 

Forgetting To Free Memory 
Another common error is known as a memory leak,and it occurs when you forget to free memory. In long-running applicationsor sys­tems (such as the OS itself), this is a huge problem, as slowly leaking memory eventually leads one to run out of memory, at which point arestart is required. Thus, in general, when you are done witha chunk of memory, you should make sure to free it. Note that using a garbage-collected language doesn’t help here: if you still have a ref­erence to some chunk of memory, no garbage collector will everfree it. 
Note that not all memory need be freed, at least, in certain cases. For example, when you write a short-lived program, you might allo­cate some space using malloc().The program runs and is about to complete: is there need to call free() abunch of times justbefore exiting? While it seems wrong not to, it is in this case quite .ne to simply exit. After all, when your program exits, the OS will clean up everything about this process, including any memory it hasal­located. Calling free() abunch of times and then exiting is thus pointless, and, if you do so incorrectly, will cause the program to crash. Just call exit and be happy instead. 

Freeing Memory Before You Are Done With It 
Sometimes a program will free memory before it is .nished using it; such a mistake is called a dangling pointer,and it, as you can guess, is also a bad thing. The subsequent use can crash the program, or overwrite valid memory (e.g., you called free(),but then called malloc() again to allocate something else, which then recycles the errantly-freed memory). 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

Freeing Memory Repeatedly 
Programs also sometimes free memory more than once; this is known as the double free.The result of doing so is unde.ned. As you can imagine, the memory-allocation library might get confused and do all sorts of weird things; crashes are a common outcome. 

Calling free() Incorrectly 
One last problem we discuss is the call of free() incorrectly. After all, free() expects you only to pass to it one of the pointers you re­ceived from malloc() earlier. When you pass in some other value, bad things can (and do) happen. Thus, such invalid frees are dan­gerous and of course should also be avoided. 

Summary 
As you can see, there are lots of ways to abuse memory. Because of frequent errors with memory, a whole ecosphere of tools have devel­oped to help .nd such problems in your code. Check out both purify [HJ92] and valgrind [SN05]; both are excellent at helping you locate the source of your memory-related problems. 


13.5 Underlying OS Support 
You might have noticed that we haven’t been talking about sys­tem calls when discussing malloc() and free().The reason for this is simple: they are not system calls, but rather library calls. Thus the malloc library manages space within your virtual addressspace, but itself is built on top of some system calls which call into the OS to ask for more memory or release some back to the system. 
One such system call is called brk,which isused to change the location of the program’s break:the location of the end of the heap. It takes one argument (the address of the new break), and thus either increases or decreases the size of the heap based on whether the new break is larger or smaller than the current break. An additional call sbrk is passed an increment but otherwise serves a similar purpose. 
Note that you should never directly call either brk or sbrk.They are used by the memory-allocation library; if you try to use them, 
OPERATING SYSTEMS ARPACI-DUSSEAU 
you will likely make something go (horribly) wrong. Stick to malloc() and free() instead. 

13.6 Other Calls 
There are a few other calls that the memory-allocation library sup­ports. For example, calloc() allocates memory and also zeroes it before returning; this prevents some errors where you assumethat memory is zeroed and forget to initialize it yourself (see thepara­graph on “uninitialized reads” above). The routine realloc() can also be useful, when you’ve allocated space for something (say, an array), and then need to add something to it: realloc() logically makes a new larger region of memory, copies the old region intoit, and returns the pointer to the new region. Read the man pages to .nd out more. 

13.7 Summary 
We have introduced some of the APIs dealing with memory al­location. As always, we have just covered the basics; more details are available elsewhere. Read the C book [KR88] and Stevens [S92] (Chapter 7) for more information. For a cool modern paper on how to detect and correct many of these problems automatically, see No­vark et al. [N+07]; this paper also contains a nice summary of com­mon problems and some neat ideas on how to .nd and .x them. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

References 
[HJ92] Purify: Fast Detection of Memory Leaks and Access Errors 
R. Hastings and B. Joyce 
USENIX Winter ’92 

The paper behind the cool Purify tool, now a commercial product. 
[KR88] “The C Programming Language” 
Brian Kernighan and Dennis Ritchie 
Prentice-Hall 1988 

The C book, by the developers of C. Read it once, do some programming, then read it again, and then keep it near your desk or wherever you program. 
[N+07] “Exterminator: Automatically Correcting Memory Errors with High Probability” Gene Novark, Emery D. Berger, and Benjamin G. Zorn PLDI 2007 
Acool paper on .nding and correcting memory errors automatically, and a great overview of many common errors in C and C++ programs. 
[SN05] “Using Valgrind to Detect Unde.ned Value Errors with Bit-precision” 
J. Seward and N. Nethercote 
USENIX ’05 

How to use valgrind to .nd certain types of errors. 
[S92] “Advanced Programming in the UNIX Environment” 
W. Richard Stevens and Stephen A. Rago 
Addison-Wesley, 1992 

We’ve said it before, we’ll say it again: read this book many times and use it as a reference when­ever you are in doubt. The authors are always surprised at how each time they read something in this book, they learn something new, even after many years of Cprogramming. 
[W06] “Survey on Buffer Over.ow Attacks and Countermeasures” Tim Werthman Available: www.nds.rub.de/lehre/seminar/SS06/Werthmann BufferOver.ow.pdf 
Anice survey of buffer over.ows and some of the security problems they cause. Refers to many of the famous exploits. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

14 



Mechanism: Address Translation 
In developing the virtualization of the CPU, we focused on a gen­eral mechanism known as limited direct execution (or LDE). The idea behind LDE is simple: for the most part, let the program run di­rectly on the hardware; however, at certain key points in time(such as when a process issues a system call, or a timer interrupt occurs), arrange so that the OS gets involved and makes sure the “right”thing happens. Thus, the OS, with a little hardware support, tries its best to get out of the way of the running program, to deliver an ef.cient vir­tualization; however, by interposing at those critical points in time, the OS ensures that it maintains control over the hardware. Ef.ciency and control together are two of the main goals of any operatingsys­tem. 
In virtualizing memory, we will pursue a similar strategy, attain­ing both ef.ciency and control while providing the desired virtual­ization. Ef.ciency dictates that we make use of hardware support, which at .rst will be quite rudimentary (e.g., just a few registers) but will grow to be fairly complex (e.g., TLBs, page-table support, and so forth, as you will see). Control implies that the OS ensuresthat no application is allowed to access any memory but its own; thus, to protect applications from one another, and the OS from applications, we will need help from the hardware here too. Finally, we will need alittle more from the VM system, in terms of .exibility;speci.cally, we’d like for programs to be able to use their address spaces inwhat­ever way they would like, thus making the system easier to program. And thus arrive at the re.ned crux: 
137 
THE CRUX: HOW TO EFFICIENTLY AND FLEXIBLY VIRTUALIZE MEMORY 
How can we build an ef.cient virtualization of memory? How do we provide the .exibility needed by applications? How do we main­tain control over which memory locations an application can access, and thus ensure that application memory accesses are properly re­stricted? 
The generic technique we will use, which you can consider an ad­dition to our general approach of limited direct execution, is some­thing that is referred to as hardware-based address translation,or just address translation for short. With address translation, the hard­ware transforms each memory access (e.g., an instruction fetch, load, or store), changing the virtual address provided by the instruction to a physical address where the desired information is actually located. Thus, on each and every memory reference, an address translation is performed by the hardware to redirect application memory refer­ences to their actual locations in memory. 
Of course, the hardware alone cannot virtualize memory, as itjust provides the low-level mechanism for doing so ef.ciently. The OS must get involved at key points to set up the hardware so that the correct translations take place; it must thus manage memory,keep­ing track of which locations are free and which are in use, and judi­ciously intervening to maintain control over how memory is used. 
Once again the goal of all of this work is to create a beautiful il­lusion:that the program has its own private memory, where its own code and data reside. Behind that virtual reality lies the ugly physical truth: that many programs are actually sharing memory at the same time, as the CPU (or CPUs) switches between running one program and the next. Through virtualization, the OS (with the hardware’s help) turns the ugly machine reality into something that is a useful, powerful, and easy to use abstraction. 
14.1 Assumptions 
Our .rst attempts at virtualizing memory will be very simple,al­
most laughably so. Go ahead, laugh all you want; pretty soon itwill 
be the OS laughing at you, when you try to understand the ins and 
OPERATING SYSTEMS ARPACI-DUSSEAU DESIGN TIP:INTERPOSITION 
Interposition is a generic and powerful technique that is often used to 
great effect in computer systems. In virtualizing memory, the hard­
ware will interpose on each memory access, and translate eachvir­
tual address issued by the process to a physical address wherethe 
desired information is actually stored. However, the general tech­
nique of interposition is much more broadly applicable; indeed, al­
most any well-de.ned interface can be interposed upon, to addnew 
functionality or improve some other aspect of the system. Oneof 
the usual bene.ts of such an approach is transparency;the interpo­
sition often is done without changing the client of the interface, thus 
requiring no changes to said client. 
outs of TLBs, multi-level page tables, and other technical wonders. Don’t like the idea of the OS laughing at you? Well, you may be out of luck then; that’s just how the OS rolls. 
Speci.cally, we will assume for now that the user’s address space must be placed contiguously in physical memory. We will also as­sume, for simplicity, that the size of the address space is nottoo big; speci.cally, that it is less than the size of physical memory.Finally, we will also assume that each address space is exactly the same size. Don’t worry if these assumptions sound unrealistic; we will relax them as we go, thus achieving a realistic virtualization of memory. 

14.2 An Example 
To understand better what we need to do to implement address translation, and why we need such a mechanism, let’s look at a sim­ple example. Imagine there is a process whose address space as indicated in Figure 14.1. What we are going to examine here is a short code sequence that loads a value from memory, increments it by three, and then stores the value back into memory. You can imag­ine the C-language representation of this code might look like this: 
void func()int x; ... x=x +3;// thisis theline ofcode weare interested in 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
The compiler turns this line of code into assembly, which might 
look something like this (in x86 assembly). Use objdump on Linux 
or otool on Mac OS X to disassemble it: 
128: movl 0x0(%ebx), %eax ;load 0+ebx into eax 
132: addl $0x03, %eax ;add 3 to eax register
135: movl %eax, 0x0(%ebx) ;store eax back to mem 
This code snippet is relatively straightforward; it presumes that the address of x has been placed in the register ebx,andthen loads the value at that address into the general-purpose register eax using the movl instruction (for “longword” move). The next instruction adds 3 to eax,and the .nal instruction stores the value in eax back into memory at that same location. 
In Figure 14.1, you can see how both the code and data are laid out in the process’s address space; the three-instruction code sequence is located at address 128 (in the code section near the top), and the value of the variable x at address 15 KB (in the stack near the bottom). In the .gure, the initial value of x is 3000, as shown in its location on the stack. 
When these instructions run, from the perspective of the process, the following memory accesses take place. 
• 
Fetch instruction at address 128 

• 
Execute this instruction (load from address 15 KB) 

• 
Fetch instruction at address 132 

• 
Execute this instruction (no memory reference) 

• 
Fetch the instruction at address 135 

• 
Execute this instruction (store to address 15 KB) 


From the program’s perspective, its address space starts at ad­dress 0 and grows to a maximum of 16 KB. All memory references it generates should be within these bounds. However, to virtualize memory, the OS wishes to place this process somewhere else in phys­ical memory, not necessarily starting at address zero. Thus,we have the problem: how can we place this process somewhere else in mem­ory in a way that is transparent to the process? In other words, how can provide the illusion of a virtual address space starting at address 0, when in reality the address space of the program is located at some other physical address? 
OPERATING SYSTEMS ARPACI-DUSSEAU 
0KB 
128 

130 
133 
1KB 
2KB 
3KB 
4KB 
14KB 
15KB 
16KB 
Figure 14.1: A Process And Its Address Space 
An example of what physical memory might look like once this process’s address space has been placed in memory is found in Fig­ure 14.2. In the .gure, you can see the OS using the .rst slot of physical memory for itself, and that it has relocated the process from the example above into the slot starting at physical memory address 32 KB. The other two slots are free (16 KB-32 KB and 48 KB-64 KB). 

14.3 Dynamic (Hardware-based) Relocation 
To gain some understanding of hardware-based address transla­tion, we’ll .rst discuss its .rst incarnation. Introduced inthe .rst time-sharing machines of the late 1950’s is a simple idea referred to as base and bounds (the technique is also referred to as dynamic relocation;we’ll use both terms interchangeably) [SS74]. 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
0KB 
16KB 
32KB 
48KB 
64KB 

Figure 14.2: Physical Memory with a Single Relocated Process 
Speci.cally, we’ll need two hardware registers within each CPU: one is called the base register, and the other the bounds (sometimes called a limit register). This base-and-bounds pair is going to allow us to place the address space anywhere we’d like in physical mem­ory, and do so while ensuring that the process can only access its own address space. 
In this setup, each program is written and compiled as if it is loaded at address zero. However, when a program starts running, the OS decides where in physical memory it should be loaded and sets the base register to that value. In the example above, theOS de­cides to load the process at physical address 32 KB and thus sets the base register to this value. 
Interesting things start to happen when the process is running. 
Now, when any memory reference is generated by the process, itis 
translated by the processor in the following manner: 
physical address = virtual address + base 
Each memory reference generated by the process is a virtual ad­dress;the hardware in turn adds the contentsof the base register to this address and the result is a physical address that can be issued to the memory system. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
ASIDE:SOFTWARE-BASED RELOCATION  
In the early days, before hardware support arose, some systems per­formed a crude form of relocation purely via software methods. The basic technique is referred to as static relocation,in which a piece of software known as the loader takes an executable that is about  
to be run and rewrites its addresses to the desired offset in physical memory.  
For example, if an instruction was a load from address 1000 into a register (e.g., movl 1000, %eax), and the address space of the pro­gram was loaded starting at address 3000 (and not 0, as the program thinks), the loader would rewrite the instruction to offset each ad­dress by 3000 (e.g., movl 4000, %eax). In this way, a simple static relocation of the process’s address space is achieved.  
However, static relocation has numerous problems. First andmost importantly, it does not provide protection, as processes can gener­ate bad addresses and thus illegally access other process’s or even OS memory; in general, hardware support is likely needed for true pro­tection [WL+93]. A smaller negative is that once placed, it isdif.cult to later relocate an address space to another location [M65].  
To understand this better, let’s trace through what happens when asingle instruction is executed. Speci.cally, let’s look atone instruc­tion from our earlier sequence:  
128: movl 0x0(%ebx), %eax  
The program counter (PC) is set to 128; when the hardware needs to fetch this instruction, it .rst adds the value to the the base register value of 32 KB (32768) to get a physical address of 32896; the hard­ware then fetches the instruction from that physical address. Next, the processor begins executing the instruction. At some point, the process then issues the load from virtual address 15 KB, whichthe processor takes and again adds to the base register (32 KB), getting the .nal physical address of 47 KB and thus the desired contents. Transforming a virtual address into a physical address is exactly the technique we refer to as address translation;that is, the hard­ware takes a virtual address the process thinks it is referencing and  
THREE  
ARPACI-DUSSEAU  EASY PIECES  
(V0.5)  

transforms it into a physical address which is where the data actually resides. Because this relocation of the address happens at runtime, and because we can move address spaces even after the process has started running, the technique is often referred to as dynamic relo­cation [M65]. 
Now you might be asking: what happened to that bounds (limit) register? After all, isn’t this supposed to be the base-and-bounds ap­proach? Indeed, it is. And as you might have guessed, the bounds register is there to help with protection. Speci.cally, the processor will .rst check that the memory reference is within bounds to make sure it is legal; in the simple example above, the bounds register would always be set to 16 KB. If a process generates a virtual address that is greater than the bounds, or one that is negative, the CPU will raise an exception, and the process will likely be terminated. The point of the bounds is thus to make sure that all addresses generated by the process are legal and within the “bounds” of the process. 
We should note that the base and bounds registers are hardware structures kept on the chip (one pair per CPU). Sometimes people call the part of the processor that helps with address translation the memory management unit (MMU);aswe develop more sophisti­cated memory-management techniques, we will be adding more cir­cuitry to the MMU. 
Asmall aside about bound registers, which can be de.ned in one of two ways. In one way (as above), it holds the size of the address space, and thus the hardware checks the virtual address against it .rst before adding the base. In the second way, it holds the physical address of the end of the address space, and thus the hardware .rst adds the base and then makes sure the address is within bounds. Both methods are logically equivalent; for simplicity, we’ll usually assume that the bounds register holds the size of the address space. 
HARDWARE SUPPORT:DYNAMIC RELOCATION 
With dynamic relocation, we can see how a little hardware goes along way. Namely, a base register is used to transform virtual addresses (generated by the program) into physical addresses. A bounds (or limit)register ensures that such addresses are within the con.nes of the address space. Together, they combine to provide a simple and ef.cient virtualization of memory. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
Example Translations 
To understand address translation via base-and-bounds in more de­tail, let’s take a look at an example. Imagine a process with anad­dress space of size 4 KB (yes, unrealistically small) has beenloaded at physical address 16 KB. Here are the results of a number of address translations: 
• 
Virtual Address 0 . Physical Address 16 KB 

• 
VA1 KB . PA 17 KB 

• 
VA 3000 . PA 19384 

• 
VA 4400 . Fault (out of bounds) 


As you can see from the example, it is easy for you to simply add the base address to the virtual address (which can rightly be viewed as an offset into the address space) to get the resulting physical ad­dress. Only if the virtual address is “too big” or negative will the result be a fault (e.g., 4400 is greater than the 4 KB bounds), causing an exception to be raised and the process to be terminated. 

14.4 OS Issues 
There are a number of new OS issues that arise when using base and bounds to implement a simple virtual memory. Speci.cally, there are three critical junctures where the OS must take action to implement this base-and-bounds approach to virtualizing memory. 
First, The OS must take action when a process is created, .nd­ing space for its address space in memory. Fortunately, givenour assumptions that each address space is (a) smaller than the size of physical memory and (b) the same size, this is quite easy for the OS; it can simply view physical memory as an array of slots, and track whether each one is free or in use. When a new process is created, the OS will have to search a data structure (often called a free list)to .nd room for the new address space and then mark it used. 
An example of what physical memory might look like can be found in Figure 14.2. In the .gure, you can see the OS using the .rst slot of physical memory for itself, and that it has relocated the pro­cess from the example above into the slot starting at physicalmem­ory address 32 KB. The other two slots are free (16 KB-32 KB and 48 KB-64 KB); thus, the free list should consist of these two entries. 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
DATA STRUCTURE:THE FREE LIST 
The OS must track which parts of free memory are not in use, so 
as to be able to allocate memory to processes. Many different data 
structures can of course be used for such a task; the simplest (which 
we will assume here) is a free list,which simply isa list of the ranges 
of the physical memory which are not currently in use. 
Second, the OS must take action when a process is terminated, reclaiming all of its memory for use in other processes or the OS. Upon termination of a process, the OS thus puts its memory backon the free list, and cleans up any associated data structures asneed be. 
Third, the OS must also take action when a context switch occurs. There is only one base and bounds register on each CPU, after all, and their values differ for each running program, as each program is loaded at a different physical address in memory. Thus, theOS must save and restore the base-and-bounds pair when it switches be­tween processes. Speci.cally, when the OS decides to stop running aprocess, itmustsave the values of the base and bounds registers to memory, in some per-process structure such as the process struc­ture or process control block (PCB). Similarly, when the OS resumes arunning process (or runs it the .rst time), it must set the values of the base and bounds on the CPU to the correct values for this process. 
We should note that when a process is stopped (i.e., not running), it is possible for the OS to move an address space from one location in memory to another rather easily. To move a process’s address space, the OS .rst deschedules the process; then, the OS copies the address space from the current location to the new location; .nally, the OS updates the saved base register (in the process structure) topoint to the new location. When the process is resumed, its (new) base register is restored, and it begins running again, obliviousthat its instructions and data are now in a completely new spot in memory! 
We should also note that access to the base and bounds registers is obviously privileged.Special hardware instructions are required to access base-and-bounds registers; if a process, running in user mode, attempts to do so, the CPU will raise an exception and the OS will likely terminate the process. Only in kernel (or privileged)mode can such registers be modi.ed. Imagine the havoc a user process could 
OPERATING SYSTEMS ARPACI-DUSSEAU 
wreak1 if it could arbitrarily change the base register while running. Imagine it! And then quickly .ush such dark thoughts from your mind, as they are the ghastly stuff of which nightmares are made. 

14.5 Summary 
In this chapter, we have extended the concept of limited direct ex­ecution with a speci.c mechanism used in virtual memory, known as address translation.With address translation, the OS can control each and every memory access from a process, ensuring the accesses stay within the bounds of the address space. Key to the ef.ciency of this technique is hardware support, which performs the transla­tion quickly for each access, turning virtual addresses (theprocess’s view of memory) into physical ones (the actual view). All of this is performed in a way that is transparent to the process that has been re­located; the process has no idea that its memory references are being translated, making for a wonderful illusion. 
We have also seen one particular form of virtualization, known as base and bounds or dynamic relocation. Base-and-bounds virtu­alization is quite ef.cient,asonly a little more hardware logic is re­quired to add a base register to the virtual address and check that the address generated by the process is in bounds. Base-and-bounds also offers protection;the OS andhardware combine to ensure no pro­cess can generate memory references outside its own address space. Protection is certainly one of the most important goals of theOS; without it, the OS could not control the machine (if processeswere free to overwrite memory, they could easily do nasty things like over­write the trap table and soon take over the system). 
Unfortunately, this simple technique of dynamic relocationdoes have its inef.ciencies. For example, as you can see in Figure 14.2 (back a few pages), the relocated process is using physical memory from 32 KB to 48 KB; however, because the process stack and heap are not too big, all of the space between the two is simply wasted.This type of waste is usually called internal fragmentation,as the space inside the allocated unit is not all used (i.e., is fragmented) and thus wasted. In our current approach, although there might be enough physical memory for more processes, we are currently restricted to 
1
Is there anything other than “havoc” that can be “wreaked”? 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
placing an address space in a .xed-sized slot and thus internal frag­ 
mentation can arise2.Thus, we are going to need more sophisticated  
machinery, to try to better utilize physical memory and avoidinter­ 
nal fragmentation. Our .rst attempt will be a slight generalization  
of base and bounds known as segmentation,which we will discuss  
next.  
2Adifferent solution might instead place a .xed-sized stack within the address  
space, just below the code region, and a growing heap below that. However, this limits  
.exibility by making recursion and deeply-nested function calls challenging, and thus is  
something we hope to avoid.  
OPERATING  
SYSTEMS  ARPACI-DUSSEAU  


References 
[M65] “On Dynamic Program Relocation” 
W.C. McGee IBM Systems Journal Volume 4, Number 3, 1965, pages 184–199 
This paper is a nice summary of early work on dynamic relocation, as well as some basics on static relocation. 
[P90] “Relocating loader for MS-DOS .EXE executable .les” 
Kenneth D. A. Pillay 
Microprocessors & Microsystems archive 
Volume 14, Issue 7 (September 1990) 

An example of a relocating loader for MS-DOS. Not the .rst one,but just a relatively modern example of how such a system works. 
[SS74] “The Protection of Information in Computer Systems” 
J. Saltzer and M. Schroeder CACM, July 1974 
From this paper: “The concepts of base-and-bound register and hardware-interpreted descriptors appeared, apparently independently, between 1957 and 1959 on three projects with diverse goals. At M.I.T., McCarthy suggested the base-and-bound idea as part of the memory protection system necessary to make time-sharing feasible. IBM independentlydevelopedthe base-and-boundregis­ter as a mechanism to permit reliable multiprogramming of theStretch (7030) computersystem. At Burroughs, R. Barton suggested that hardware-interpreted descriptors would provide direct support for the naming scope rules of higher level languages in the B5000 computer system.” We found this quote on Mark Smotherman’s cool history pages [S04]; see them for more information. 
[S04] “System Call Support” Mark Smotherman, May 2004 http://www.cs.clemson.edu/ mark/syscall.html 
Aneat history of system call support. Smotherman has also collected some early history on items like interrupts and other fun aspects of computing history. See his web pages for more details. 
[WL+93] “Ef.cient Software-based Fault Isolation” Robert Wahbe, Steven Lucco, Thomas E. Anderson, Susan L. Graham SOSP ’93 
Aterri.c paper about howyou can use compiler support to boundmemory references from a program, without hardware support. The paper sparked renewed interest in software techniques for isolation of memory references. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
0KB 1KB 2KB 3KB 4KB 5KB 6KB 7KB 
16KB 
Program Code  
Stack  
Heap  
(free)  

Figure 14.3: A Fixed-Sized Stack Address Space 

Homework 
This program allows you to see how address translations are per­formed in a system with base and bounds registers. As before, there are two steps to running the program to test out your understanding of base and bounds. First, run without the -c .ag to generate a set of translations and see if you can correctly perform the addresstransla­tions yourself. Then, when done, run with the -c .ag to check your answers. 
In this homework, we will assume a slightly different address space than our canonical one with a heap and stack at opposite ends of the space. Rather, we will assume that the address space hasa code section, then a .xed-sized (small) stack, and a heap thatgrows downward right after, looking something like you see in Figure 14.3. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
In this con.guration, there is only one direction of growth, towards higher regions of the address space. 
In the .gure, the bounds register would be set to 7 KB, as that represents the end of the address space. References to any address within the bounds would be considered legal; references above this value are out of bounds and thus the hardware would raise an ex­ception. 
To run with the default .ags, type relocation.py at the com­mand line. The result should be something like this: 
prompt> ./relocation.py 
... 
Base-and-Bounds register information: 

Base : 0x00003082 (decimal 12418) 
Limit : 472 

Virtual Address Trace VA 0: 0x01ae (decimal:430) -> PA or violation? VA 1: 0x0109 (decimal:265) -> PA or violation? VA 2: 0x020b (decimal:523) -> PA or violation? VA 3: 0x019e (decimal:414) -> PA or violation? VA 4: 0x0322 (decimal:802) -> PA or violation? 
For each virtual address, either write down the physicaladdress it translates to OR write down that it is an out-of-bounds address (a segmentation violation). Forthis problem, you should assume a simple virtual address space of a given size. 
As you can see, the homework simply generates randomized vir­tual addresses. For each, you should determine whether it is in bounds, and if so, determine to which physical address it translates.Running with -c (the “compute this for me” .ag) gives us the results of these translations, i.e., whether they are valid or not, and if valid, the re­sulting physical addresses. For convenience, all numbers are given both in hex and decimal. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
prompt> ./relocation.py -c 
... 
Virtual Address Trace 

VA 0: 0x01ae (decimal:430) -> VALID: 0x00003230 (dec:12848) 
VA 1: 0x0109 (decimal:265) -> VALID: 0x0000318b (dec:12683) 
VA 2: 0x020b (decimal:523) -> SEGMENTATION VIOLATION 
VA 3: 0x019e (decimal:414) -> VALID: 0x00003220 (dec:12832) 
VA 4: 0x0322 (decimal:802) -> SEGMENTATION VIOLATION 

With a base address of 12418 (decimal), address 430 is within bounds (i.e., it is less than the limit register of 472) and thus translates to 430 added to 12418 or 12848. A few of the addresses shown above are out of bounds (523, 802), as they are in excess of the bounds. Pretty simple, no? Indeed, that is one of the beauties of base and bounds: it’s so darn simple! 
There are a few .ags you can use to control what’s going on better: 
prompt> ./relocation.py -h
Usage: relocation.py [options] 

Options:-h, --help show this help message and exit -s SEED, --seed=SEED the random seed -a ASIZE, --asize=ASIZE address space size (e.g., 16, 64k, 32m)-p PSIZE, --physmem=PSIZE physical memory size (e.g., 16, 64k) -n NUM, --addresses=NUM # of virtual addresses to generate-b BASE, --b=BASE value of base register -l LIMIT, --l=LIMIT value of limit register -c, --compute compute answers for me 
In particular, you can control the virtual address-space size (-a), the size of physical memory (-p), the number of virtual addresses to generate (-n), and the values of the base and bounds registers for this process (-b and -l,respectively). 
OPERATING SYSTEMS ARPACI-DUSSEAU 
Questions 
Now let’s explore a few questions with this homework simulator. 
• 	
Run with seeds 1, 2, and 3, and compute whether each virtual address generated by the process is in or out of bounds. If in bounds, compute the translation. 

• 	
Run with these .ags: -s0 -n 10.What value do you have set -l (the bounds register) to in order to ensure that all the generated virtual addresses are within bounds? 

• 	
Run with these .ags: -s 1 -n 10-l 100.What is the max­imum value that bounds can be set to, such that the address space still .ts into physical memory in its entirety? 

• 	
Run some of the same problems above, but with larger address spaces (-a)and physical memories (-p). 

• 	
What fraction of randomly-generated virtual addresses are valid, as a function of the value of the bounds register? Make a graph from running with different random seeds, with limit values ranging from 0 up to the maximum size of the address space. 


THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
15 




Segmentation 
So far we have been putting the entire address space of each process in memory. With the base and bounds registers, the OS can easily relocate processes to different parts of physical memory. However, you might have noticed something interesting about these address spaces of ours: there is a big chunk of “free” space right in themid­dle, between the stack and the heap. 
As you can imagine from Figure 15.1, although the space between the stack and heap is not being used by the process, it is still tak­ing up physical memory when we relocate the entire address space somewhere in physical memory; thus, the simple approach of using abase and bounds register pair to virtualize memory is wasteful. It also makes it quite hard to run a program when the entire address space doesn’t .t into memory; thus, base and bounds is not as .exi­ble as we would like. And thus, a problem: 
THE CRUX:HOW TO SUPPORT A LARGE ADDRESS SPACE 
How do we support a large address space with (potentially) a lot of free space between the stack and the heap? Note that in our ex­amples, with tiny (pretend) address spaces, the waste doesn’t seem too bad. Imagine, however, a 32-bit address space (4 GB in size); a typical program will only use megabytes of memory, but still would demand that the entire address space be resident in memory. 
155 
0KB 1KB 2KB 3KB 4KB 5KB 6KB 
14KB 15KB 16KB 
Program Code  

 
Heap  
(free)  
Stack  

Figure 15.1: An Address Space (Again) 
15.1 Segmentation: Generalized Base/Bounds 
To solve this problem, an idea was born, and it is called segmen­tation.It is quite an old idea, going at least as far back as the very early 1960’s [H61, G62]. The idea is simple: instead of havingjust one base and bounds pair in our MMU, why not have a base and bounds pair per logical segment of the address space? A segment is just a contiguous portion of the address space of a particular length, and in our canonical address space, we have three logically-different seg­ments: code, stack, and heap. What segmentation allows the OSto do is to place each one of those segments in different parts of phys­ical memory, and thus avoid .lling physical memory with unused virtual address space. 
Let’s look at an example. Assume we want to place the address space from Figure 15.1 into physical memory. With a base and bounds 
OPERATING SYSTEMS ARPACI-DUSSEAU 
0KB 
16KB 
32KB 
48KB 
64KB 

Figure 15.2: Placing Segments In Physical Memory 
pair per segment, we can place each segment independently in physi­cal memory. For example, see Figure 15.2; there you will see a 64-KB physical memory with those three segments placed within it (as well as 16KB reserved for the OS). 
As you can see in the diagram, only used memory is allocated space in physical memory, and thus large address spaces with large amounts of unused address space (which we sometimes call sparse address spaces)can be accommodated. 
The hardware structure in our MMU required to support segmen­tation is just what you’d expect: in this case, a set of three base and bounds register pairs. Table 15.1 shows the register values for the example above; each bounds register holds the size of a segment. 
Segment Base Size 
Code 32K 2K 
Heap 34K 2K 
Stack 28K 2K 
Table 15.1: Segment Register Values 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
DESIGN TIP:GENERALIZATION 
As you can see, segmentation is just a generalization of dynamic re­
location; instead of a single base and bounds register, we usea few 
(or even many). The abstract technique of generalization canthus 
be quite useful in systems design, where one good idea can be made 
slightly broader and thus solve a larger class of problems. However, 
be careful when generalizing; as Lampson warns us “Don’t general­
ize; generalizations are generally wrong.” [L83] 
You can see from the table that the code segment is placed at phys­
ical address 32KB and has a size of 2KB and the heap segment is 
placed at 34KB and also has a size of 2KB. 
Let’s do an example translation. Assume that a reference is made to virtual address 100 (which is in the code segment). When theref­erence takes place (say, on an instruction fetch), the hardware will add the base value to the offset into this segment (100 in this case) to arrive at the desired physical address: 100 + 32KB, or 32868. It will then check that the address is within bounds (100 is less than 2KB), .nd that it is, and issue the reference to physical memory address 32868. 
Now let’s look at an address in the heap, say virtual address 4200. Note that if we just add the virtual address 4200 to the base of the heap (34KB), we get a physical address of 39016, which is not the correct physical address. What we need to .rst do is extract the offset into the heap, i.e., which byte(s) in this segment the address refers to. Because the heap starts at virtual address 4KB (4096), the offset of 4200 is actually 4200 – 4096 or 104. We then take this offset (104) and add it to the base register physical address (34K or 34816) to get the desired result: 34920. 
What if we tried to refer to an illegal address, such as 7KB which is beyond the end of the heap? You can imagine what will happen: the hardware detects that the address is out of bounds, traps into the OS, likely leading to the termination of the offending process. And now you know the origin of the famous term that all C programmers learn to dread: the segmentation violation or segmentation fault. 
OPERATING SYSTEMS ARPACI-DUSSEAU ASIDE:THE SEGMENTATION FAULT 
The term segmentation fault or violation arises from a memoryac­
cess on a segmented machine to an illegal address. Humorously, the 
term persists, even on machines where there is no support for seg­
mentation at all. Or perhaps not so humorous, if you can’t .gure 
why your code keeps segfaulting. 

15.2 Which Segment Are We Referring To? 
The hardware uses segment registers to perform translations. But 
how does it know the offset into a segment? How does it know which 
segment an address refers to? 
One common approach, sometimes referred to as an explicit ap­proach, is to chop up the address space into segments based on the top few bits of the virtual address; this technique was used inthe VAX/VMS system [LL82]. In our example above, we have three seg­ments; thus we need two bits to accomplish our task. If we use the top two bits of a virtual address to select the segment, our virtual address looks like this (assuming a 16KB address space): 

In our example, then, if the top two bits are 00, the hardware knows the virtual address is in the code segment, and thus usesthe code base and bounds pair to relocate the address to the correct phys­ical location. If the top two bits are 01, the hardware knows the ad­dress is in the heap, and thus uses the heap base and bounds. Let’s take our example heap virtual address from above (4200) and trans­late it, just to make sure this is clear. The virtual address 4200, in binary form, can be seen here: 

Segment Offset 
As you can see from the picture, the top two bits (01) tell the hard­ware which segment we are referring to. The bottom 12 bits are the 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
offset into the segment: 0000 0110 1000, or hex 0x068, or 104 in deci­mal. Thus, the hardware simply takes the .rst two bits to determine which segment register to use, and then takes the next 12 bits as the offset into the segment. By adding the base register to the offset, the hardware arrives at the .nal physical address. Note the offset eases the bounds check too: we can simply check if the offset is less than the bounds; if not, the address is illegal. Thus, if base and bounds were arrays (with one entry per segment), the hardware would be doing something like this to obtain the desired physical address: 
1 // get top 2 bits of 14-bit VA 2 Segment = (VirtualAddress & SEG_MASK) >> SEG_SHIFT 3 // now get offset 4 Offset = VirtualAddress & OFFSET_MASK 5 if (Offset >= Bounds[Segment]) 6 RaiseException(PROTECTION_FAULT) 7 else 8 PhysAddr = Base[Segment] + Offset 9 Register = AccessMemory(PhysAddr) 
In our running example, we can .ll in values for the constants 
above. Speci.cally, SEG 
MASK would be set to 0x3000, SEG 
SHIFT 
to 12,and OFFSET 
MASK to 0xFFF. 
You may also have noticed that when we use the top two bits, and we only have three segments (code, heap, stack), one segment of the address space goes unused. Thus, some systems would put the code in the same segment as the heap and thus use only 1 bit to select which segment to use [LL82]. 
There are other ways for the hardware to determine which seg­
ment a particular address is in. In the implicit approach, the hard­
ware determines the segment by noticing how the address was formed. 
If, for example, the address was generated from the program counter 
(i.e., it was an instruction fetch), then the address is within the code 
segment; if the address is based off of the stack or base pointer, it 
must be in the stack segment; any other address must be in the heap. 

15.3 What About The Stack? 
Thus far, we’ve left out one important component of the address space: the stack. The stack has been relocated to physical address 28KB in the diagram above, but with one important difference: it grows backwards (in physical memory, it starts at 28KB and grows 
OPERATING SYSTEMS ARPACI-DUSSEAU 
backwards to 26KB, which correspond to virtual addresses 16KB to 14KB); translation has to proceed differently. 
The .rst thing we need is a little extra hardware support. Instead of just base and bounds values, the hardware also needs to know which way the segment grows (a bit, for example, that is set to 1 when the segment grows in the positive direction, and 0 for nega­tive). Thus, our updated view of what the hardware is trackingis found in Table 15.2. 
Segment Base Size Grows Positive? Code 32K 2K 1 Heap 34K 2K 1 Stack 28K 2K 0 
Table 15.2: Segment Registers (With Negative-Growth Support) 
With the hardware understanding that segments can grow in the negative direction, the hardware must now translate such virtual ad­dresses slightly differently. Let’s take an example stack virtual ad­dress and translate it to understand the process. 
In this example, assume we wish to access virtual address 15KB, which should map to physical address 27KB. Our virtual address, in binary form, thus looks like this: 11 1100 0000 0000 (hex 0x3C00). The hardware uses the top two bits (11) to designate the segment, but then we are left with an offset of 3KB. To obtain the correctneg­ative offset, we must subtract the maximum segment size from 3KB: in this example, a segment can be 4KB, and thus the correct negative offset is 3KB -4KB which equals -1KB. We simply add the negative offset (-1KB) to the base (28KB) to arrive at the correct physical ad­dress: 27KB. The bounds check can be calculated just by ensuring the absolute value of the negative offset is less than the sizeof the segment. 

15.4 Support for Sharing 
As support for segmentation grew, system designers soon real­ized that they could realize new types of ef.ciencies with a little more hardware support. Speci.cally, to save memory, sometimes itis use­ful to share certain memory segments between address spaces. In particular, code sharing is common and still in use in systems today. 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
To support sharing, we need a little extra support from the hard­ware, in the form of protection bits.Basic support adds a few bits per segment, indicating whether or not a program can read or write asegment, or perhaps execute code that lies within the segment. By setting a code segment to read-only, the same code can be shared across multiple processes, without worry of harming isolation; while each process still thinks that it is accessing its own privatememory, the OS is secretly sharing memory which cannot be modi.ed by the process, and thus the illusion is preserved. 
An example of the additional information tracked by the hard­ware (and OS) is shown in Figure 15.3. As you can see, the code seg­ment is set to read and execute, and thus the same physical segment in memory could be mapped into multiple virtual address spaces. 
Segment Base Size Grows Positive? Protection 
Code 32K 2K 1 Read-Execute Heap 34K 2K 1 Read-Write Stack 28K 2K 0 Read-Write 
Table 15.3: Segment Register Values (with Protection) 
With protection bits, the hardware algorithm described earlier would 
also have to change. In addition to checking whether a virtualad­
dress is within bounds, the hardware also has to check whethera 
particular access is permissable. If a user process tries to write to 
aread-only page, or execute from anon-executable page, the hard­
ware should raise an exception, and thus let the OS deal with the 
offending process. 

15.5 Fine-grained vs. Coarse-grained Segmentation 
Most of our examples thus far have focused on systems with just afew segments (i.e., code, stack, heap); we can think of this seg­mentation as coarse-grained,as it chops up the address space into relatively large, coarse chunks. However, some early systems (e.g., Multics [CV65,DD68]) were much more .exible and allowed for ad­dress spaces to be comprised of a large number smaller segments; we refer to this as .ne-grained segmentation. 
Supporting many segments requires even further hardware sup­
port, with a segment table of some kind stored in memory. Such 
segment tables usually support the creation of a very large number 
OPERATING SYSTEMS ARPACI-DUSSEAU 
of segments, and thus enable a system to use segments in more .ex­ible ways than we have thus far discussed. For example, early ma­chines like the Burroughs B5000 had support for thousands of seg­ments, and expected a compiler to chop code and data into separate segments which the OS and hardware would then support [RK68]. The thinking at the time was that by having .ne-grained segments, the OS could better learn about which segments are in use and which are not and thus utilize main memory more effectively. 

15.6 OS Support 
You now should have a basic idea as to how segmentation works. Pieces of the address space are relocated into physical memory as the system runs, and thus a huge savings of physical memory is achieved relative to our simpler approach with just a single base/bounds pair for the entire address space. Speci.cally, all the unused space be­tween the stack and the heap need not be allocated in physical mem­ory, allowing us to .t more address spaces into physical memory. 
However, segmentation does raise a number of new issues. We’ll .rst describe the new OS issues that must be addressed. The .rst is an old one: what should the OS do on a context switch? You should have a good guess by now: the segment registers must be saved and restored. Clearly, each process has its own virtual address space, and thus the OS must make sure to set up these registers correctly before letting the process run again. 
The second, and more important, issue is managing free space in physical memory. When a new address space is created, the OS has to be able to .nd space in physical memory for its segments. Previously, we assumed that each address space was the same size, and thus physical memory could be thought of as a bunch of slots where processes would .t in. Now, we have a number of segments per process, and each segment might be a different size. 
The general problem that arises is that physical memory quickly becomes full of little holes of free space, making it dif.cultto allo­cate new segments, or to grow existing ones. We call this problem external fragmentation [R69]; see Figure 15.3 (left). 
In the example, a process comes along and wishes to allocate a 
20KB segment. In that example, there is 24KB free, but not in one 
contiguous segment (rather, in three non-contiguous chunks). Thus, 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
Compacted 
0KB 
0KB 
8KB 
8KB 
16KB 
16KB 
24KB 
24KB 
32KB 
32KB 
40KB 
40KB 
48KB 
48KB 
56KB 
56KB 
64KB 
64KB 

Operating System  
Allocated  
(not in use)  

Figure 15.3: Non-compacted and Compacted Memory. 
the OS cannot satisfy the 20KB request. 
One solution to this problem would be to compact physical mem­ory by rearranging the existing segments. For example, the OScould stop whichever processes are running, copy their data to one con­tiguous region of memory, change their segment register values to point to the new physical locations, and thus have a large freeex­tent of memory with which to work. By doing so, the OS enables the new allocation request to succeed. However, compaction is expen­sive, as copying segments is memory-intensive and thus woulduse afair amountof processor time. See Figure 15.3(right) for what a compacted physical memory would look like. 
Asimpler approach is to use a free-list management algorithm that tries to keep large extents of memory available for allocation. There are literally hundreds of approaches that people have taken, including classic algorithms like best-.t (which keeps a list of free spaces and returns the one closest in size that satis.es the desired al­location to the requester), worst-.t, .rst-.t,andmore complex schemes like Knuth’s buddy algorithm [K68]. An excellent survey by Wilson et al. is a good place to start if you want to learn more about such algorithms [W+95], or you can wait until we cover some of the basics ourselves in a later chapter. Unfortunately, though, no matter how smart the algorithm, external fragmentation will still exist; thus, a good algorithm simply attempts to minimize it. 
OPERATING SYSTEMS ARPACI-DUSSEAU DESIGN TIP:IF 1000 SOLUTIONS EXIST,NO GREAT ONE DOES 
The fact that so many different algorithms exist to try to minimize 
external fragmentation is indicative of a stronger underlying truth: 
there is no one “best” way to solve the problem. Thus, we settle 
for something reasonable and hope it is good enough. The only real 
solution (as we will see in forthcoming chapters) is to avoid the prob­
lem altogether, by never allocating memory in variable-sized chunks. 

15.7 Summary 
Segmentation solves a number of problems, and helps us build amore effective virtualization of memory. Beyond just dynamic re­location, segmentation can better support sparse address spaces, by avoiding the huge potential waste of memory between logical seg­ments of the address space. It is also fast, as doing the arithmetic seg­mentation requires in hardware is easy and well-suited to hardware; the overheads of translation are minimal. A fringe bene.t arises too: code sharing. If code is placed within a separate segment, such a seg­ment could potentially be shared across multiple running programs. 
However, as we learned, allocating variable-sized segmentsin memory leads to some problems that we’d like to overcome. The .rst, as discussed above, is external fragmentation. Because seg­ments are variable-sized, free memory gets chopped up into odd-sized pieces, and thus satisfying a memory-allocation request can be dif.cult. One can try to use smart algorithms [W+95] or periodically compact memory, but the problem is fundamental and hard to avoid. 
The second and perhaps more important problem is that segmen­
tation still isn’t .exible enough to support our fully generalized, sparse 
address space. For example, if we have a large but sparsely-used 
heap all in one logical segment, the entire heap must still reside in 
memory in order to be accessed. In other words, if our model of 
how the address space is being used doesn’t exactly match how the 
underlying segmentation has been designed to support it, segmen­
tation doesn’t work very well. And thus our re.ned problem: 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
THE CRUX: HOW TO BETTER SUPPORT VIRTUAL MEMORY Although segmentation is a step forward, it is still not fullygen­eral, and further has the problem of external fragmentation.Can we build an even more general approach to virtualizing memory?  
OPERATING SYSTEMS  ARPACI-DUSSEAU  


References 
[CV65] “Introduction and Overview of the Multics System” 
F. J. Corbato and V. A. Vyssotsky Fall Joint Computer Conference, 1965 
One of .ve papers presented on Multics at the Fall Joint Computer Conference; oh to be a .y on the wall in that room that day! 
[DD68] “Virtual Memory, Processes, and Sharing in Multics” Robert C. Daley and Jack B. Dennis Communications of the ACM, Volume 11, Issue 5, May 1968 
An early paper on how to perform dynamic linking in Multics, which was way ahead of its time. Dynamic linking .nally found its way back into systems about 20 years later, as the large X-windows libraries demanded it. Some say that these large X11 libraries were MIT’s revenge for removing support for dynamic linking in early versions of UNIX! 
[G62] “Fact Segmentation” 
M. N. Green.eld Proceedings of the SJCC, Volume 21, May 1962 
Another early paper on segmentation; so early that it has no references to other work. 
[H61] “Program Organization and Record Keeping for Dynamic Storage” 
A. W. Holt Communications of the ACM, Volume 4, Issue 10, October 1961 
An incredibly early and dif.cult to read paper about segmentation and some of its uses. 
[I09] “Intel 64 and IA-32 Architectures Software Developer’s Manuals” Intel, 2009 Available: http://www.intel.com/products/processor/manuals 
Try reading about segmentation in here (Chapter 3 in Volume 3a); it’ll hurt your head, at least a little bit. 
[K68] “The Art of Computer Programming: Volume I” 
Donald Knuth. 
Addison-Wesley, 1968. 

Knuth is famous not only for his early books on the Art of Computer Programming but for his typesetting system TeX which is still a powerhouse typesetting tool used by professionals today, and indeed to typeset this very book. 
[L83] “Hints for Computer Systems Design” Butler Lampson ACM Operating Systems Review, 15:5, October 1983 
Atreasure-trove of sage advice on howto build systems. Hard to read in one sitting; take it in a little at a time, like a .ne wine, or a reference manual. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
[LL82] “Virtual Memory Management in the VAX/VMS Operating System” 
Henry M. Levy and Peter H. Lipman 
IEEE Computer, Volume 15, Number 3 (March 1982) 

Aclassic memory management system, with lots of common sensein its design. We’ll study it in more detail in a later chapter. 
[RK68] “Dynamic Storage Allocation Systems” 
B. Randell and C.J. Kuehner 
Communications of the ACM 
Volume 11(5), pages 297-306, May 1968 

Anice overview of the differences between paging and segmentation, with some historical discus­sion of various machines. 
[R69] “A note on storage fragmentation and program segmentation” 
Brian Randell. 
Communications of the ACM 
Volume 12(7), pages 365-372, July 1969 

One of the earliest papers to discuss fragmentation. 
[W+95] “Dynamic Storage Allocation: A Survey and Critical Review” 
Paul R. Wilson, Mark S. Johnstone, Michael Neely, and David Boles. 
In International Workshop on Memory Management 
Scotland, United Kingdom, September 1995 

Agreat survey paper on memory allocators. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
Homework  
This program allows you to see how address translations are per­formed in a system with segmentation. The segmentation that this system uses is pretty simple: an address space has just two segments; further, the top bit of the virtual address generated by the process determines which segment the address is in: 0 for segment 0 (where, say, code and the heap would reside) and 1 for segment 1 (where the stack lives). Segment 0 grows in a positive direction (towards higher addresses), whereas segment 1 grows in the negative direction. With segmentation, as you might recall, there is a base/limitpair of registers per segment. Thus, in this problem, there are twobase/limit pairs. The segment-0 base tells which physical address the top of seg­ment 0 has been placed in physical memory and the limit tells how big the segment is; the segment-1 base tells where the bottom of seg­ment 1 has been placed in physical memory and the corresponding limit also tells us how big the segment is (or how far it grows inthe negative direction). As before, there are two steps to running the program to test out your understanding of segmentation. First, run without the -c .ag to generate a set of translations and see if you can correctly perform the address translations yourself. Then, when done, run withthe -c .ag to check your answers. To run with the default .ags, type segmentation.py at the com­mand line1.The result should be something like the following:  
ARG seed 0 ARG address space size1k ARG phys mem size 16k  
Segment register information:  
Segment 0 base (grows positive) : 0x00001aea (decimal 6890) Segment 0 limit : 472  
Segment 1 base (grows negative) : 0x00001254 (decimal 4692) Segment 1 limit : 450  
Virtual Address Trace  
1If this doesn’t work, try ./segmentation.py,or possibly try python ./segmentation.py.If these don’t work, email us!  
ARPACI-DUSSEAU  THREE EASY PIECES (V0.5)  

VA 0: 0x0000020b (decimal: 523) --> PA or violation? VA 1: 0x0000019e (decimal: 414) --> PA or violation? VA 2: 0x00000322 (decimal: 802) --> PA or violation? VA 3: 0x00000136 (decimal: 310) --> PA or violation? VA 4: 0x000001e8 (decimal: 488) --> PA or violation? 
For each virtual address, either write down the physicaladdress it translates to OR write down that it is an out-of-bounds address (a segmentation violation). Forthis problem, you should assume a simple address space with two segments: the top bit of the virtual address can thus be used to check whether the virtual address is in segment 0 (topbit=0) or segment 1 (topbit=1). Note thatthe base/limit pairs given to you grow in different directions, depending on the segment, i.e., segment 0grows in the positive direction, whereas segment 1 inthe negative. 
Then, after you have computed the translations in the virtualad­
dress trace, run the program again with the -c .ag. You will see the 
following (not including the redundant information): 
Virtual Address Trace 
VA 0: 0x0000020b (decimal: 523) --> SEG VIOLATION (SEG1) VA 1: 0x0000019e (decimal: 414) --> VALID SEG0: 0x00001c88 VA 2: 0x00000322 (decimal: 802) --> VALID SEG1: 0x00001176 VA 3: 0x00000136 (decimal: 310) --> VALID SEG0: 0x00001c20 VA 4: 0x000001e8 (decimal: 488) --> SEG VIOLATION (SEG0) 
As you can see, with -c,the program translates the addresses for 
you, and hence you can check if you understand how a system using 
segmentation translates addresses. 
Of course, there are some parameters you can use to give your­self different problems. One particularly important parameter is the -s or -seed parameter, which lets you generate different problems by passing in a different random seed. Of course, make sure to use the same random seed when you are generating a problem and then solving it. 
There are also some parameters you can use to play with different-
sized address spaces and physical memories. For example, to exper­
iment with segmentation in a tiny system, you might type: 
segmentation.py -s 100 -a 16 -p 32 
which will yield: 
OPERATING SYSTEMS ARPACI-DUSSEAU 
ARG seed 0 ARG address space size 16ARG phys mem size 32 
Segment register information: 
Segment 0 base 
Segment 0 limit 

Segment 1 base 
Segment 1 limit 

(grows positive) 	: : 
(grows negative) : 
Virtual Address Trace 
VA 0: 0x0000000c (decimal: 12) 
VA 1: 0x00000008 (decimal: 8)
VA 2: 0x00000001 (decimal: 1)
VA 3: 0x00000007 (decimal: 7)
VA 4: 0x00000000 (decimal: 0) 

: 
0x00000018 (decimal 24) 4 
0x00000012 (decimal 18) 
5 
--> PA or --> PA or --> PA or --> PA or --> PA or violation? violation? violation? violation? violation? 
The parameters tell the program to use a random seed of 100, and to generate virtual addresses for a 16-byte address space (-a 16) placed somewhere in a 32-byte physical memory (-p 32). As you  
can see, the resulting virtual addresses are tiny (12, 8, 1, 7,and0).  
As you can also see, the program picks tiny base register and limit  
values, as appropriate. Run with -c to see the answers.  
This example should also show you exactly what each base pair  
means. For example, segment 0’s base is set to a physical address of  
24 (decimal) and is of size 4 bytes. Thus, virtual addresses 0, 1, 2, and  
3are in segment0 and valid, and map to physical addresses 24, 25,  
26, and 27, respectively.  
Slightly more tricky is the negative-direction-growing segment 1.  
In the tiny example above, segment 1’s base register is set to physical  
address 18, with a size of 5 bytes. That means that the last .ve bytes  
of the virtual address space, in this case 11, 12, 13, 14, and 15, are  
valid virtual addresses, and that they map to physical addresses 13,  
14, 15, 16, and 17, respectively. Note you can specify bigger values by tacking a k, m,or even g  
onto the values you pass in with the -a or -p .ags, as in “kilobytes”,  
“megabytes”, and “gigabytes”, respectively. Thus, if you wanted to  
do some translations with a 1-MB address space set in a 32-MB phys­ 
ical memory, you might type:  
segmentation.py -a 1m -p 32m  
THREE  
ARPACI-DUSSEAU  EASY PIECES  
(V0.5)  

If you want to get even more speci.c, you can set the base register 
and limit register values yourself, with the --b0, --l0, --b1,and 
--l1 .ags. 
There is also one more interesting .ag: -A (or --addresses). This .ag lets you pass in a comma-separated list of virtual addresses (in decimal) for the program to translate, which can be usefulto test your knowledge in different ways. For example, running: 
segmentation.py -A 0,1,2 -c 
would run the simulation for virtual addresses 0, 1, and 2. Finally, you can always run: 
segmentation.py -h 
Doing so gets you a complete list of .ags and options, as usual. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
Questions 
• 	First let’s use a tiny address space to translate some addresses. Here’s a simple set of parameters with a few different random seeds; can you translate the addresses? 
segmentation.py -a 128 -p 512 -b 0 -l 20 -B 512 -L 20 -s 0 segmentation.py -a 128 -p 512 -b 0 -l 20 -B 512 -L 20 -s 1 segmentation.py -a 128 -p 512 -b 0 -l 20 -B 512 -L 20 -s 2 
• 	
Now, let’s see if we understand this tiny address space we’ve constructed (using the parameters from the question above). What is the highest legal virtual address in segment 0? What about the lowest legal virtual address in segment 1? What are the lowest and highest illegal addresses in this entire address space? Finally, how would you run segmentation.py with the -A .ag to test if you are right? 

• 	
Let’s say we have a tiny 16-byte address space in a 128-byte physical memory. What base and bounds would you set up so as to get the simulator to generate the following translation results for the speci.ed address stream: valid, valid, violation, ..., violation, valid, valid? Assume the following parameters: 


segmentation.py -a 16 -p 128
-A 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 
--b0 ? --l0 ? --b1 ? --l1 ? 

• 	
Assuming we want to generate a problem where roughly 90% of the randomly-generated virtual addresses are valid (i.e., not segmentation violations). How should you con.gure the sim­ulator to do so? Which parameters are important? 

• 	
Can you run the simulator such that no virtual addresses are valid? How? 


THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
16 




Free-Space Management (INCOMPLETE) 
In this chapter, we take a small detour from our discussion of virtual­izing memory to discuss a fundamental aspect of any memory man­agement system, whether it be a malloc library (managing pages of aprocess’s heap) or the OS itself (managing portions of the address space of a process). Speci.cally, we will discuss the issues surround­ing free-space management. 
Let’s make the problem more speci.c. Managing free space can certainly be easy, as we will see when we discuss the concept of pag­ing.It is easy when the space you are managing is divided into .xed-sized units. In such a case, you can keep a list of these .xed-sizes units; when a client requests one of them, return the .rst entry. 
Where free-space management becomes more dif.cult and inter­esting is when the free space you are managing consists of variable-sized units; this always happens in a user-level memory-allocation library (as in malloc() and free())and also in an OS managing physical memory when using segmentation to implement virtual memory. In either case, the problem that exists is known as external fragmentation:the free space ends up being chopped up into little pieces of different sizes and is thus fragmented; subsequentrequests may fail because there is no single contiguous space that can satisfy the request, even though the total amount of free space exceeds the size of the request. The .gure below shows an example; a request for 15 bytes will fail even though there are 20 bytes free as the 20 bytes are non-contiguous. 
[0.... 9 | 10 .... 19| 20 .... 29 ] 
free used free 

175 
And thus we arrive at the problem addressed in this chapter: 
CRUX:HOW TO MANAGE FREE SPACE 
How should free space be managed, when satisfying variable-
sized requests? What strategies can be used to minimize fragmenta­
tion? What are the performance differences of alternate approaches? 
16.1 Assumptions 
Most of this discussion will focus on the great history of alloca­tors found in user-level memory-allocation libraries. We draw on Wilson’s excellent survey [W+95] but encourage interested readers to go to the source itself for more details (it is nearly 80 pages long). 
We assume a basic interface such as that provided by malloc() and free().Speci.cally, void *malloc(size tsize) takes a single parameter size,which isthe number of bytesrequested by the application; it hands back a pointer (of no particular type, or a void pointer in C lingo) to a region of that size (or greater). The com­plementary routine void free(void *ptr) takes a pointer and frees the corresponding chunk. Note the implication of the interface: the user, when freeing the space, does not inform the library of its size; thus, the library must be able to .gure out how big a chunkof memory is when handed just a pointer to it (we’ll discuss how todo this below). 
The space that this library manages is known historically as theheap,andthe generic data structure used to manage free space in the heap is some kind of free list.This structure contains references to all of the free chunks of space in the managed region of memory. Of course, this data structure need not be a list per se,but just some kind of data structure to track free space. 
We further assume that primarily we are concerned with external fragmentation,as described above. Allocators could of course also have the problem of internal fragmentation;if an allocator hands out chunks of memory bigger than that requested, any unasked for space in such a chunk is considered internal fragmentation and is another example of space waste. However, for the sake of simplicity, we’ll mostly focus on external fragmentation. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

We’ll also assume that once memory is handed out to a client, it cannot be relocated to another location in memory. For example, if a program calls malloc() and is given a pointer to some space within the heap, that memory region is essentially “owned” by the program (and cannot be moved by the library) until the program returnsit via acorresponding call to free().Thus, no compaction of free space 
is possible, which would be useful to combat fragmentation1.Com­paction could, however, be used in the OS to deal with fragmentation when implementing segmentation;see the chapter on segmentation for details. 
Finally, we’ll assume that the allocator manages a contiguous re­gion of bytes. In some cases, an allocator could ask for that region to grow; for example, a user-level memory-allocation library might call into the kernel to grow the heap (via a system call such as sbrk) when it runs out of space. However, for simplicity, we’ll justassume that the region is a single .xed size throughout its life. 

16.2 Low-level Mechanisms 
Before delving into some policy details, we’ll .rst cover some common mechanisms used in most allocators. First, we’ll discuss the basics of splitting and coalescing, common techniques inmost any allocator. Second, we’ll show how one can track the size ofal­located regions quickly and with relative ease. Finally, we’ll discuss how to build a simple list inside the free space to keep track ofwhat is free and what isn’t. 
Splitting and Coalescing 
Afree list contains a set of elements that describe the free space still remaining in the heap. Thus, assume the following 30-byte heap: 
[0.... 9 | 10 .... 19| 20 .... 29 ] 
free used free 

1
Once you hand a pointer to a chunk of memory to a C program, it is generally dif­.cult to determine all references (pointers) to that region,which may be stored in other variables or even in registers at a given point in execution. This may not be the case in more strongly-typed, garbage-collected languages, which would thus enable com­paction as a technique to combat fragmentation. 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
The free list for this heap would have two elements on it. One 
entry describes the .rst 10-byte free segment (bytes 0-9), and one 
entry describes the other free segment (bytes 20-29): 
head->[addr:0,len:10]->[addr:20,len:10]->NULL 
As described above, a request for anything greater than 10 bytes will fail (returning NULL); there just isn’t a single contiguous chunk of memory of that size available. A request for exactly that size (10 bytes) could be satis.ed easily by either of the free chunks. But what happens if the request is for something smaller than 10 bytes? 
Assume we have a request for just a single byte of memory. In this case, the allocator will perform an action known as splitting:it will .nd a free chunk of memory that can satisfy the request and split it into two. The .rst chunk it will return to the caller; the second chunk will remain on the list. Thus, in our example above, if a request for 1byte were made, and the allocator decided to use the second ofthe two elements on the list to satisfy the request, the call to malloc() would return 20 (the address of the 1-byte allocated region) and the list would end up looking like this: 
head->[addr:0,len:10]->[addr:21,len:9]->NULL 
In the picture, you can see the list basically stays intact; the only 
change is that the free region now starts at 21 instead of 20, and the length of that free region is now just 92.Thus, the split is commonly used in allocators when requests are smaller than the size of any par­ticular free chunk. 
Acorollary mechanism found in many allocators is known as co­alescing of free space. Take our example from above once more: 
[0 .... 9 | 10 .... 19 | 20 .... 29 ] free used free 
Given this (tiny) heap, what happens when an application calls free(10), thus returning the space in the middle of the heap? If we simply add this free space back into our list without too much think­ing, we might end up with a list that looks like this: 
2
This discussion assumes that there are no headers, an unrealistic but simplifying assumption we make for now. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
head->[addr:10,len:10]->[addr:0,len:10]->[addr:20,len:10]->NULL 
Note the problem: while the entire heap is now free, it is seem­ingly divided into three chunks of 10 bytes each. Thus, if a user re­quests 20 bytes, a simple list traversal will not .nd such a free chunk, and return failure. 
What allocators do in order to avoid this problem is coalesce free space when a chunk of memory is freed. The idea is simple: when returning a free chunk in memory, look carefully at the addresses of the chunk you are returning as well as the nearby chunks of free space; if the newly-freed space sits right next to one (or two,as in this example) existing free chunks, merge them into a single larger free chunk. Thus, with coalescing, our .nal list should look like this: 
head->[addr:0,len:30]->NULL 
Indeed, this is what the heap list looked like at .rst, before any al­locations were made. With coalescing, an allocator can better ensure that large free extents are available for the application. 

Tracking The Size Of Allocated Regions 
You might have noticed that the interface to free(void *ptr) does not take a size parameter; thus it is assumed that given a pointer, the malloc library can quickly determine the size of the region ofmem­ory being freed and thus incorporate the space back into the free list. 
To accomplish this task, most allocators store a little bit ofextra information in a header block which is kept in memory, usually just before the handed-out chunk of memory. Let’s look at an example again (Figure 16.1); in this example, we are examining an allocated block of size 20 bytes, pointed to by ptr (imagine the user called malloc() and stored the results in ptr,e.g., ptr = malloc(20);). 
The header minimally contains the size of the allocated region (in this case, 20); it may also contain additional pointers to speed up deallocation, a magic number to provide additional integrity check­ing, and other information. Let’s assume a simple header which con­tains the size of the region and a magic number, like this: typedef struct __header_t { 
int size; 
int magic;}header_t; 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
| header | \ 
| header | |----the header used by the malloc library 
| header | / 

ptr -> 	---------------\ | ||| ||| | |----the 20 bytes handed to the user | ||| || ---------------/ 
Figure 16.1: An Allocated Region Plus Header 
The example above would thus look like what you see in Figure 
16.2. When the user calls free(ptr),the library then uses simple pointer arithmetic to .gure out where the header begins: 
void free(void *ptr) { header_t *hptr = (void *)ptr -sizeof(header_t);} 
After obtaining such a pointer to the header, the library can easily determine whether the magic number matches the expected value as asanity check (assert(hptr->magic == 1234567))and calcu­late the total size of the newly-freed region via simple math (hptr->size+sizeof(header)). Note the small but critical detail in the last sentence: the size of the free region is the size of the header plus the size of the space allocated to the user. Thus, when a user requests N bytes of memory, the library does not search for a free chunk ofsize N;rather, it searches for a free chunk of size N plus the size of the header. 
---------------<-hptr|size : 20||magic:1234567| 
ptr -> 	---------------\ | ||| ||| | |----the 20 bytes handed to the user | ||| || ---------------/ 
Figure 16.2: Speci.c Contents Of The Header 
OPERATING SYSTEMS ARPACI-DUSSEAU 

Embedding A Free List 
Thus far we have treated our simple free list as a conceptual entity; it is just a list describing the free chunks of memory in the heap. But how do we build such a list inside the free space itself? 
In a more typical list, when allocating a new node, you would just call malloc() when you need space for the node. Unfortunately, within the memory-allocation library, you can’t do this! Instead, you need to build the list inside the free space itself. Don’t worry if this sounds a little weird; it is, but not so weird that you can’t do it! 
Assume we have a 4096-byte chunk of memory to manage (i.e., the heap is 4KB). To manage this as a free list, we .rst have to ini­tialize said list; initially, the list should have one entry,of size 4096. Here is the description of a node of the list: typedef struct __node_t { 
int size; 
struct __node_t *next; }node_t; 
Now let’s look at some code that initializes the heap and puts the .rst element of the free list inside that space. We are assuming that the heap is built within some free space acquired via a call to the system call mmap();this isnot the only way to build such a heap but serves us well in this example. Here is the code: 
// mmap() gives us a pointer to the beginning of a chunk of free space node_t *head = mmap(NULL, 4096, PROT_READ|PROT_WRITE,
MAP_ANON|MAP_PRIVATE, -1, 0); head->size = 4096; head->next = NULL; 
After running this code, the status of the list is that it has a single entry, of size 4096. The head pointer contains the beginning address of this range; let’s assume it is 16KB (though any virtual address would be .ne). Visually, the heap looks like this: 
head -> 	--------------­|size: 4096| 
|next: 0| [virtual address: 16KB] // this is where the ’size’ field goes // and ’next’: note that NULL is just 0 
|  |  
...  
|  |  

// The rest of the 4096 bytes 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
Now, let’s imagine that a chunk of memory is requested, say of size 100. To service this request, the library will .rst .nd a chunk that is large enough to accommodate the request; because there is only one free chunk (4096), this chunk will be chosen. Then, the chunk will be split into two: one chunk big enough to service the request (and header, as described above), and the remaining free chunk. As­suming an 8-byte header (an integer size and an integer magic num­ber), the space in the heap now looks like this: 
---------------[virtual address: 16KB] |size : 100| 
|magic:1234567| ptr -> ---------------\ | || ... | --the 100-bytes now allocated | ||head -> ---------------/|size: 3988| 
|next: 0| 
|| ... // The rest of the free 3988 bytes 
|| 
Thus, upon the request for 100 bytes, the library allocated 108 bytes out of the existing one free chunk, returns a pointer (marked ptr in the .gure above) to it, stashes the header information imme­diately before the allocated space for later use upon free(),and shrinks the one free node in the list to 3988 bytes (4096 minus 108). 
Now let’s look at the heap when there are three allocated regions, 
each of 100 bytes (or 108 including the header). The visualization 
looks like what you see in Figure 16.3. 
As you can see, the .rst 324 bytes of the heap are now allocated, and thus we see three headers in that space as well as three 100­byte regions being used by the calling program. The free list remains uninteresting: just a single node, but now only 3772 bytes in size after the three splits. But what happens when the calling program returns some memory via free()? 
In this example, the application returns the middle chunk of allo­
cated memory, by calling free(16500) (the value 16500 is arrived 
upon by adding the start of the memory region, 16384, to the 108 
OPERATING SYSTEMS ARPACI-DUSSEAU 
---------------[virtual address: 16KB] 
|size : 100| 

|magic:1234567| 
---------------\ 
| || 

... | --100-bytes still allocated 
| || 
---------------/
|size : 100| 

|magic:1234567| 
sptr -> ---------------\ 
| || 

... | --100-bytes still allocated 
| | | (but about to be freed) 
---------------/
|size : 100| 

|magic:1234567| 
---------------\ 
| || 

... | --100-bytes still allocated | ||head -> ---------------/|size: 3772| 
|next: 0| 
|| ... // The rest of the free 3772 bytes || 
Figure 16.3: Free Space With Three Chunks Allocated 
of the previous chunk and the 8 bytes of the header for this chunk). This value is shown in the previous diagram by the pointer sptr. 
The library immediately .gures out the size of the free region, and then adds the free chunk back onto the free list. Assuming we insert at the head of the free list, the space now looks like this: 
And now we have a list that starts with a small free chunk (108 bytes, pointed to by the head of the list) and a large free chunk(3772 bytes). Our list .nally has more than one element on it! And yes, the free space is fragmented, an unfortunate but common occurrence. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
---------------[virtual address: 16KB] |size : 100| 
|magic:1234567| ---------------\ | || 
... | --100-bytes still allocated | ||head -> ---------------/|size : 108| 
|next : 16708| -----------------------------------­sptr -> ---------------||| | ... (now free) ||| | ---------------||size : 100| | ---------------||magic:1234567| | ---------------\ |||| | ... | --100-bytes still allocated |||| | ---------------/ ||size: 3772| <----------------------------------­
|next: 0| -> still points to NULL 
|| ... // The rest of the free 3772 bytes 
|| 
Figure 16.4: Free Space With Two Chunks Allocated 
One last example: let’s assume now that the last two in-use chunks 
are freed. Without coalescing, you might end up with a free list that 
looks like what you see in Figure 16.5. 
As you can see from the .gure, we now have a big mess! Why? Simple, we forgot to coalesce the list. Although all of the memory is free, it is chopped up into pieces, thus appearing as a fragmented memory despite not being one. The solution is simple: go through the list and merge neighboring chunks; when .nished, we’ll beback to one big free space again. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
---------------[virtual address: 16KB] 
|size : 100| <--------------­---------------|
|next : 16492| ------------| 
---------------| |
| | || 

... (now free) | || | || ---------------| ||size : 108| <----------| | ---------------||next : 16708| -----------------------------------­---------------| |||| | 
... (now free) | |
||| |
head -> ---------------| |
|size : 100| | | 
---------------| |
|next : 16384| ----------------| 
---------------|
|| | 
... (now free) ||| | ---------------||size: 3772| <----------------------------------­
|next: 0| -> still points to NULL 
|| ... // The rest of the free 3772 bytes || 
Figure 16.5: Free Space With Two Chunks Allocated 

Growing The Heap 
We should discuss one last mechanism found within many allocation libraries. Speci.cally, what should you do if the heap runs out of space? The simplest approach is just to fail. In some cases this is the only option, and thus returning NULL is an honorable approach. Don’t feel bad! You tried, and though you failed, you fought the good .ght. 
Most traditional allocators start with a small-sized heap and then request more memory from the OS when they run out. Typically, this 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
means they make some kind of system call (e.g., sbrk in most UNIX systems) to grow the heap, and then allocate the new chunks from there. To service the sbrk request, the OS .nds free physical pages, maps them into the address space of the requesting process, and then returns the value of the end of the new heap; at that point, a larger heap is available, and the request can be successfully serviced. 


16.3 Basic Strategies 
Now that we have some machinery under our belt, let’s go over some basic strategies for managing free space. These approaches are mostly based on pretty simple policies that you could think upyour­self; try it before reading and see if you come up with all of thealter­natives (or maybe some new ones!). 
The ideal allocator is both fast and minimizes fragmentation. Un­fortunately, because the stream of allocation and free requests can be arbitrary (after all, they are determined by the programmer), any particular strategy can do quite badly given the wrong set of inputs. Thus, we will not describe a “best” approach, but rather talk about some basics and discuss their pros and cons. 
Best Fit 
The best .t strategy is quite simple: .rst, search through the free list and .nd chunks of free memory that are as big or bigger than the requested size. Then, return the one that is the smallest in that group of candidates; this is the so called best-.t chunk (it could becalled smallest .t too). One pass through the free list is enough to .nd the correct block to return. 
The intuition behind best .t is simple: by returning a block that is close to what the user asks, best .t tries to reduce wasted space. However, there is a cost; naive implementations pay a heavy perfor­mance penalty when performing an exhaustive search for the correct free block. 

Worst Fit 
The worst .t approach is the opposite of best .t; .nd the largest chunk and return the requested amount; keep the remaining (large) 
OPERATING SYSTEMS ARPACI-DUSSEAU 
chunk on the free list. Worst .t tries to thus leave big chunks free in­stead of lots of small chunks that can arise from a best-.t approach. Once again, however, a full search of free space is required, and thus this approach can be costly. Worse, most studies show that it per­forms badly, leading to excess fragmentation while still having high overheads. 

First Fit 
The .rst .t method simply .nds the .rst block that is big enough and returns the requested amount to the user. As before, the remaining free space is kept free for subsequent requests. 
First .t has the advantage of speed – no exhaustive search of all the free spaces are necessary – but sometimes this leads to pollu­tion of the beginning of the free list with a number of small objects. Thus, how the allocator manages the free list’s order becomesan is­sue. One approach is to use address-based ordering;by keeping the list ordered by the address of the free space, coalescing becomes eas­ier, and fragmentation tends to be reduced. 

Next Fit 
Instead of always beginning the .rst-.t search at the beginning of the list, the next .t algorithm keeps an extra pointer to the location within the list where one was looking last. The idea is to spread the searches for free space throughout the list more uniformly, thus avoided heaving splintering of the beginning of the list. Theperfor­mance of such an approach is quite similar to .rst .t, as an exhaus­tive search is once again avoided. 

Examples 


16.4 Other Approaches 
Beyond the basic approaches described above, there have been ahost of suggested techniques and algorithms to improve memory allocation in some way. We list a few of them here for your consid­eration (i.e., to make you think about a little more than just best-.t allocation). 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
Segregated Lists 
One interesting approach that has been around for some time isthe use of segregated lists.The basic idea is simple: if a particular appli­cation has one (or a few) popular-sized request that it makes,keep a separate list just to manage objects of that size; all other requests are forwarded to a more general memory allocator. 
The bene.ts of such an approach are obvious. By having a chunk of memory dedicated for one particular size of requests, fragementa­tion is much less of a concern; moreover, allocation and free requests can be served quite quickly when they are of the right size, as no complicated search of a list is required. 
Just like any good idea, this approach introduces new complica­tions into a system as well. For example, how much memory should one dedicate to the pool of memory that serves specialized requests of a given size, as opposed to the general pool? One particularallo­cator, the slab allocator by Jeff Bonwick (which was designed for use in the Solaris kernel), handles this issue in a rather nice way. 
Speci.cally, when the kernel boots up, it allocates a number of object caches for kernel objects that are likely to be requested fre­quently (such as locks, .le-system inodes, etc.); the objectcaches thus are each segregated free lists of a given size and serve memoryallo­cation and free requests quickly. When a given cache is running low on free space, it requests some slabs of memory from a more general memory allocator (the total amount requested being a multiple of the page size and the object in question). Conversely, when the reference counts of the objects within a given slab all go to zero, the general allocator can reclaim them from the specialized allocator, which is often done when the VM system needs more memory. 
The slab allocator also goes beyond most segregated list approaches 
by keeping free objects on the lists in a pre-initialized state. Bon-
wick shows that initialization and destruction of data structures is 
costly [B+00]; by keeping freed objects in a particular list in their 
initialized state, the slab allocator thus avoids frequent initializa­
tion/destruction cycles per object and thus lowers overheads notice­
ably. 

Buddy Allocation 
Because coalescing is critical for an allocator, 
OPERATING SYSTEMS ARPACI-DUSSEAU 

Other Ideas 
Scaling. Fast coalescing. Deferred coalescing. Multiprocessor awareness [B+00]. 
16.5 Summary 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
16.6  References  
[B+00] “Hoard: A Scalable Memory Allocator for Multithreaded Applications”  
Emery D. Berger, Kathryn S. McKinley, Robert D. Blumofe, and Paul R. Wilson  
ASPLOS-IX, November 2000  
Berger and company’s excellent allocator for multiprocessor systems. Beyond just being a fun  
paper, also used in practice!  
[B94] “The Slab Allocator: An Object-Caching Kernel Memory Allocator” Jeff Bonwick  
USENIX ’94  
xxx  
[W+95] “Dynamic Storage Allocation: A Survey and Critical Review”  
Paul R. Wilson, Mark S. Johnstone, Michael Neely, David Boles  
International Workshop on Memory Management  
Kinross, Scotland, September 1995  
An excellent and far-reaching survey of many facets of memoryallocation. Far too muchdetail to  
go into in this tiny chapter!  
OPERATING  
SYSTEMS  ARPACI-DUSSEAU  

17 




Paging: Introduction 
Remember our goal: to virtualize memory. Segmentation (a gen­eralization of dynamic relocation) helped us do this, but hassome problems; in particular, managing free space becomes quite apain as memory becomes fragmented. Thus, we’d like to .nd a different solution. 
(page 0 of the address space) 
(page 1) 
(page 2) 
(page 3) 
Figure 17.1: A Simple 64-byte Address Space 
Thus comes along the idea of paging [KE+62,L78]. Instead of splitting up our address space into three logical segments (each of variable size), we split up our address space into .xed-sizedunits we call a page.Here in Figure 17.1 an example of a tiny address space, 64 bytes total in size, with 16 byte pages (real addressspaces are much bigger, of course, commonly 32 bits and thus 4-GB of ad­dress space, or even 64 bits). 
Thus, we have an address space that is split into four pages (0 through 3). With paging, physical memory is also split into some 
191 

0 16 32 48 64 80 96 112 128 page frame 0 of physical memory page frame 1 page frame 2 page frame 3 page frame 4 page frame 5 page frame 6 page frame 7 
reserved for OS  
(unused)  
page 3 of AS  
page 0 of AS  
(unused)  
page 2 of AS  
(unused)  
page 1 of AS  

Figure 17.2: 64-Byte Address Space Placed In Physical Memory 
number of pages as well; we sometimes will call each page of physi­cal memory a page frame.For an example, let’s examine Figure 17.2. 
Paging, as we will see, has a number of advantages over our pre­vious approaches. Probably the most important improvement will be .exibility:with a fully-developed paging approach, the system will be able to support the abstraction of an address space effectively, re­gardless of how the processes uses the address space; we won’t, for example, have to make assumptions about how the heap and stack grow and how they are used. 
Another advantage is the simplicity of free-space management that paging affords. For example, when the OS wishes to place our tiny 64-byte address space from above into our 8-page physical memory, it simply .nds four free pages; perhaps the OS keeps a free list of all free pages for this, and just grabs the .rst four free pages offof this list. In the example above, the OS has placed virtual page 0 of the address space (AS) in physical page 3, virtual page 1 of the AS on physical page 7, page 2 on page 5, and page 3 on page 2. 
To record where each virtual page of the operating system is placed 
in physical memory, the operating system keeps a per-process data 
structure known as a page table.The major role of the page table 
is to store address translations for each of the virtual pages of the 
address space, thus letting us know where in physical memory they 
live. For our simple example above, the page table would thus have 
the following entries: 
OPERATING SYSTEMS ARPACI-DUSSEAU 
Virtual Page Number Physical Page Frame 
03 
17 
25 
32 
As we said before, it is important to remember that this is a per-process data structure1;If another process were to run in our example above, the OS would have to manage a different page table for it, as its virtual pages obviously map to different physical pages (modulo any sharing going on). 
Now, we know enough to perform an address-translation exam­ple. Let’s imagine the process with that tiny address space (64 bytes) is performing a memory access: 
movl <virtual address>, %eax 
Speci.cally, let’s pay attention to the explicit load of the data at<virtual address> into the register eax (and thus ignore the in­struction fetch that must have happened prior). 
To translate this virtual address that the process generated, we have to .rst split it into two components: the virtual page number (VPN),andthe offset within the page. For this example, because the virtual address space of the process is 64 bytes, we need 6 bitstotal 
for our virtual address (26 =64). Thus, our virtual address: 
Va5  Va4  Va3  Va2  Va1  Va0  

where Va5 is the highest-order bit of the virtual address, andVa0 the lowest order bit. Because we know the page size (16 bytes),we can further divide the virtual address as follows: 
VPN offset 
Va5  Va4  Va3  Va2  Va1  Va0  

The page size is 16 bytes in a 64-byte address space; thus we need  
to be able to select 4 pages, and the top 2 bits of the address do just  
1This is generally true for most of the page table structures wewill discuss; however,  
for some page tables, such as the inverted page table,there is one table for all processes.  
THREE  
ARPACI-DUSSEAU  EASY PIECES  
(V0.5)  


PFN offset 
Figure 17.3: The Address Translation Process 
that. Thus, we have a 2-bit virtual page number (VPN). The remain­ing bits tell us which byte of the page we are interested in, 4 bits in this case; we call this the offset. 
When a process generates a virtual address, the OS and hard­ware must combine to translate this virtual address into a meaning­ful physical address. For example, let us assume the load above was to virtual address 21: 
movl 21, %eax 
Turning “21” into binary form, we get “010101”, and thus we can examine this virtual address and see how it breaks down into a vir­tual page number (VPN) and offset: 
VPN offset 
0  1  0  1  0  1  

Thus, the virtual address “21” is on the 5th (“0101”th) byte ofvir­tual page “01” (or 1). With our virtual page number, we can now index our page table and .nd which physical page that virtual page 1resides within. In the page table above the physical page number (PPN) (a.k.a. physical frame number or PFN) is 7 (binary 111).Thus, we can translate this virtual address by replacing the VPN with the PFN and then issue the load to physical memory (Figure 17.3). 
OPERATING SYSTEMS ARPACI-DUSSEAU 
Note the offset stays the same (i.e., it is not translated), because the offset just tells us which byte within the page we want. Our .nal physical address is 1110101 (117 in decimal), and is exactly where we want our load to fetch data from (Figure 17.2). 
DATA STRUCTURE:THE PAGE TABLE 
One of the most important data structures in the memory manage­
ment subsystem of a modern OS is the page table.In general, a page 
table stores virtual-to-physical address translations,thus letting the 
system know where each page of an address space actually resides in 
physical memory. Because each address space requires such transla­
tions, in general there is one page table per process in the system. The 
exact structure of the page table is either determined by the hardware 
(older systems) or can be more .exibly managed by the OS (modern 
systems). 
17.1 Where Are Page Tables Stored? 
Page tables can get awfully large, much bigger than the small seg­ment table or base/bounds pair we have discussed previously.For example, imagine a typical 32-bit address space, with 4-KB pages. This virtual address splits into a 20-bit VPN and 12-bit offset (recall that 10 bits would be needed for a 1-KB page size, and just add two more to get to 4 KB). 
A20-bit VPN implies that there are 220 translations that the OS would have to manage for each process (that’s roughly a million); assuming we need 4 bytes per page table entry (PTE) to hold the physical translation plus any other useful stuff, we get an immense 4MB of memory needed for each page table! That is pretty big. Now imagine there are 100 processes running: this means the OS would need 400MB of memory just for all those address translations! 
Because they are so big, we don’t keep any special on-chip hard­ware in the MMU to store the page table of the currently-running process. Instead, we store the page table for each process in memory somewhere. Let’s assume for now that the page tables live in physi­cal memory that the OS manages. In Figure 17.4 is a picture of what that might look like. 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
0 16 32 48 64 80 96 112 128 page frame 0 of physical memory page frame 1 page frame 2 page frame 3 page frame 4 page frame 5 page frame 6 page frame 7 
page table: 3 7 5 2  
(unused)  
page 3 of AS  
page 0 of AS  
(unused)  
page 2 of AS  
(unused)  
page 1 of AS  

Figure 17.4: Example: Page Table in Kernel Physical Memory 

17.2 What’s Actually In The Page Table? 
Asmall aside about page table organization. The page table isjust adata structure that is used to map virtual addresses (or really, vir­tual page numbers) to physical addresses (physical page numbers). Thus, any data structure could work. The simplest form is called a linear page table.It is just an array. The OS indexes the array by the VPN, and thus looks up the page-table entry (PTE) at that index in order to .nd the desired PFN. For now, we will assume this lin­ear page table structure; in later chapters, we will make use of more advanced data structures to help solve some problems with paging. 
As for the contents of each PTE, we have a number of different bits in there worth understanding at some level. A valid bit is common to indicate whether the particular translation is valid; forexample, when a program starts running, it will have code and heap at one end of its address space, and the stack at the other. All the unused space in-between will be marked invalid,and if the process triesto access such memory, it will generate a trap to the OS which willlikely terminate the process. Thus, the valid bit is crucial for supporting a sparse address space; by simply marking all the unused pages in the address space invalid, we remove the need to allocate physical frames for those pages and thus save a great deal of memory. 
We also might have protection bits,indicating whether the page could be read from, written to, or executed from (e.g., a code page). Again, accessing a page in a way not allowed by these bits will gen­erate a trap to the OS. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
There are a couple of other bits that are important but we won’t talk about much for now. A present bit indicates whether this page is in physical memory or on disk (swapped out); we will understand this in more detail when we study how to move parts of the address space to disk and back in order to support address spaces that are larger than physical memory and allow for the pages of processes that aren’t actively being run to be swapped out. A dirty bit is also common, indicating whether the page has been modi.ed since itwas brought into memory. 
31302928272625242322212019181716151413121110 
9 8 7 6 5 4 3 2 1 0 
PFN 
G
PAT
D
A
PCD
PWT
U/S
R/W
P 

Figure 17.5: An x86 Page Table Entry (PTE) 
A reference bit (a.k.a. accessed bit)is sometimes used to track whether apage has been accessed, and is useful in determining which pages are popular and thus should be kept in memory; such knowledge is critical during page replacement,a topic we will study in great detail in subsequent chapters. 
Figure 17.5 shows an example page table entry from the x86 ar­
chitecture [I09]. It contains a present bit (P); a read/writebit (R/W) 
which determines if writes are allowed to this page; a user/supervisor 
bit (U/S) which determines if user-mode processes can accessthe 
page; a few bits (PWT, PCD, PAT, and G) that determine how hard­
ware caching works for these pages; an accessed bit (A) and a dirty 
bit (D); and .nally, the PFN itself. 

17.3 Paging: Also Too Slow 
With page tables in memory, we already know that they might be 
too big. Turns out they can slow things down too. For example, take 
our simple instruction: 
movl 21, %eax 
Again, let’s just examine the explicit reference to address 21 and not worry about the instruction fetch. In this example, we will as­sume the hardware performs the translation for us. To fetch the de­sired data, the system must .rst translate the virtual address (21) into the correct physical address (117). Thus, before issuing the load 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
to address 117, the system must .rst fetch the proper page table en­try from the process’s page table, perform the translation, and then .nally get the desired data from physical memory. 
To do so, the hardware must know where the page table is for the currently-running process. Let’s assume for now that a single page-table base register contains the physical address of the starting location of the page table. To .nd the location of the desired PTE, the hardware will thus perform the following functions: 
VPN = (VirtualAddress & VPN_MASK) >> SHIFT PTEAddr = PageTableBaseRegister + (VPN * sizeof(PTE)) 
In our example, VPN 
MASK would be set to 0x30 (hex 30, or bi­nary 110000) which picks out the VPN bits from the full virtualad­dress; SHIFT is set to 4 (the number of bits in the offset), suchthat we move the VPN bits down to form the correct integer virtual page number. For example, with virtual address 21 (010101), and masking turns this value into 010000; the shift turns it into 01, or virtual page 1, as desired. We then use this value as an index into the array of PTEs pointed to by the page table base register. 
Once this physical address is known, the hardware can fetch the PTE from memory, extract the PFN, and concatenate it with the offset from the virtual address to form the desired physical address. Specif­ically, you can think of the PFN being left-shifted by SHIFT, and then logically OR’d with the offset to form the .nal address as follows: 
offset = VirtualAddress & OFFSET_MASK PhysAddr = (PFN << SHIFT) | offset 
Finally, the hardware can fetch the desired data from memory and 
put it into register eax.The program has now succeeded at loading 
avalue from memory! 
To summarize, we now describe the initial protocol for what hap­pens on each memory reference. Figure 17.6 shows the basic ap­proach. For every memory reference (whether an instruction fetch or an explicit load or store), paging requires us to perform one extra memory reference in order to .rst fetch the translation from the page table. That is a lot of work! Extra memory references are costly, and in this case will likely slow down the process by a factor of twoor more. 
And now you can hopefully see that there are two real problems that we must solve. Without careful design of both hardware and 
OPERATING SYSTEMS ARPACI-DUSSEAU 
1 // Extract the VPN from the virtual address 
2 VPN = (VirtualAddress & VPN_MASK) >> SHIFT 
3 
4 // Form the address of the page-table entry (PTE) 
5 PTEAddr = PTBR + (VPN * sizeof(PTE)) 
6 
7 // Fetch the PTE 
8 PTE = AccessMemory(PTEAddr) 
9 

10 // Check if process can access the page 
11 if (PTE.Valid == False) 
12 RaiseException(SEGMENTATION_FAULT) 
13 else if (CanAccess(PTE.ProtectBits) == False) 
14 RaiseException(PROTECTION_FAULT) 
15 else 
16 // Access is OK: form physical address and fetch it 
17 offset = VirtualAddress & OFFSET_MASK 
18 PhysAddr = (PTE.PFN << PFN_SHIFT) | offset 
19 Register = AccessMemory(PhysAddr) 
Figure 17.6: Accessing Memory With Paging 
software, page tables will cause the system to run too slowly,as well as take up too much memory. While seemingly a great solution for our memory virtualization needs, these two crucial problemsmust .rst be overcome; the next two chapters show us how to do so. 
THE CRUX: HOW TO MAKE PAGING FASTER AND PAGE TABLES SMALLER 
Paging, as we’ve described, has two big problems: the page tables are too big,and the address translation process is too slow.Thus, how can the OS, in tandem with the hardware, speed up translation? How can the OS and hardware reduce the exorbitant memory de­mands of paging? 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 

References 
[KE+62] “One-level Storage System” 
T. Kilburn, and D.B.G. Edwards and M.J. Lanigan and F.H. Sumner IRE Trans. EC-11, 2 (1962), pp. 223-235 (Reprinted in Bell and Newell, “Computer Structurers: Readings and Examples” McGraw-Hill, New York, 1971). 
The Atlas pioneered the idea of dividing memory into .xed-sized pages and in many senses was an early form of the memory-management ideas we see in modern computer systems. 
[I09] “Intel 64 and IA-32 Architectures Software Developer’s Manuals” Intel, 2009 Available: http://www.intel.com/products/processor/manuals 
In particular, pay attention to “Volume 3A: System Programming Guide Part 1” and “Volume 3B: System Programming Guide Part 2” 
[L78] “The Manchester Mark I and atlas: a historical perspective” 
S. H. Lavington 
Communications of the ACM archive 
Volume 21, Issue 1 (January 1978), pp. 4-12 
Special issue on computer architecture 

This paper is a great retrospective of some of the history of the development of some important 
computer systems. As we sometimes forget in the US, many of these new ideas came from over­
seas. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

Homework 
In this homework, you will use a simple program, which is known as paging-linear-translate.py,to see if you understand how simple virtual-to-physical address translation works withlinearpage tables. To run the program, remember to either type just the name of the program ./paging-linear-translate.py or possibly this python paging-linear-translate.py.When you run it with the [-h] (help) .ag, you see: 
Usage: paging-linear-translate.py [options] 
Options:-h, --help show this help message and exit -s SEED, --seed=SEED the random seed -a ASIZE, --asize=ASIZE 
address space size (e.g., 16, 64k, ...)-p PSIZE, --physmem=PSIZEphysical memory size (e.g., 16, 64k, ...)-P PAGESIZE, --pagesize=PAGESIZE
page size (e.g., 4k, 8k, ...) -n NUM, --addresses=NUM number of virtual addresses to generate -u USED, --used=USED percent of address space that is used -v verbose mode -c compute answers for me 
First, run the program without any arguments: 
ARG seed 0 ARG address space size 16kARG phys mem size 64k ARG page size 4kARG verbose False 
The format of the page table is simple:
The high-order (left-most) bit is the VALID bit.If the bit is 1, the rest of the entry is the PFN.If the bit is 0, the page is not valid. 
Use verbose mode (-v) if you want to print the VPN # by each entry of the page table. 
Page Table (from entry 0 down to the max size)0x8000000c 0x00000000 0x00000000 0x80000006 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
Virtual Address Trace VA 0: 0x00003229 (decimal: 12841) --> PA or invalid? VA 1: 0x00001369 (decimal: 4969) --> PA or invalid? VA 2: 0x00001e80 (decimal: 7808) --> PA or invalid? VA 3: 0x00002556 (decimal: 9558) --> PA or invalid? VA 4: 0x00003a1e (decimal: 14878) --> PA or invalid? 
For each virtual address, write down the physical address ittranslates to OR write down that it is an out-of-bounds address (e.g., a segmentation fault). 
As you can see, what the program provides for you is a page table for a particular process (remember, in a real system with linear page tables, there is one page table per process;here we just focus on one process, its address space, and thus a single page table). Thepage table tells you, for each virtual page number (VPN) of the address space, that the virtual page is mapped to a particular physical frame number (PFN) and thus valid, or not valid. 
The format of the page-table entry is simple: the left-most (high­order) bit is the valid bit; the remaining bits, if valid is 1, is the PFN. 
In the example above, the page table maps VPN 0 to PFN 0xc 
(decimal 12), VPN 3 to PFN 0x6 (decimal 6), and leaves the othertwo 
virtual pages, 1 and 2, as not valid. 
Because the page table is a linear array, what is printed above is a replica of what you would see in memory if you looked at the bits yourself. However, it is sometimes easier to use this simulator if you run with the verbose .ag (-v); this .ag also prints out the VPN (index) into the page table. From the example above, run with the -v .ag: 
Page Table (from entry 0 down to the max size)
[ 
0] 0x8000000c 

[ 
1] 0x00000000 

[ 
2] 0x00000000 

[ 
3] 0x80000006 


Your job, then, is to use this page table to translate the virtual ad­dresses given to you in the trace to physical addresses. Let’slook at the .rst one: VA 0x3229. To translate this virtual address into aphysical address, we .rsthave to break itup into its constituent components: a virtual page number and an offset.We do this by noting down the size of the address space and the page size. In this exam­ple, the address space is set to 16KB (a very small address space) and 
OPERATING SYSTEMS ARPACI-DUSSEAU 
the page size is 4KB. Thus, we know that there are 14 bits in the vir­tual address, and that the offset is 12 bits, leaving 2 bits forthe VPN. Thus, with our address 0x3229, which is binary 11 0010 0010 1001, we know the top two bits specify the VPN. Thus, 0x3229 is on virtual page 3 with an offset of 0x229. 
We next look in the page table to see if VPN 3 is valid and mapped to some physical frame or invalid, and we see that it is indeed valid (the high bit is 1) and mapped to physical page 6. Thus, we can form our .nal physical address by taking the physical page 6 and adding it onto the offset, as follows: 0x6000 (the physical page, shifted into the proper spot) OR 0x0229 (the offset), yielding the .nal physical address: 0x6229. Thus, we can see that virtual address 0x3229trans­lates to physical address 0x6229 in this example. 
To see the rest of the solutions (after you have computed them yourself!), just run with the -c .ag (as always): 
... 
VA 0: 00003229 (decimal: 12841) --> 00006229 (25129) [VPN 3] 
VA 1: 00001369 (decimal: 4969) --> Invalid (VPN 1 not valid)
VA 2: 00001e80 (decimal: 7808) --> Invalid (VPN 1 not valid)
VA 3: 00002556 (decimal: 9558) --> Invalid (VPN 2 not valid)
VA 4: 00003a1e (decimal: 14878) --> 00006a1e (27166) [VPN 3] 

Of course, you can change many of these parameters to make more interesting problems. Run the program with the -h .ag to see what options there are: 
• 	
The -s .ag changes the random seed and thus generates dif­ferent page table values as well as different virtual addresses to translate. 

• 
The -a .ag changes the size of the address space. 

• 
The -p .ag changes the size of physical memory. 

• 
The -P .ag changes the size of a page. 

• 	
The -n .ag can be used to generate more addresses to translate (instead of the default 5). 

• 	
The -u .ag changes the fraction of mappings that are valid, from 0that roughly 1/2 of the pages in the virtual address space will be valid. 

• 	
The -v .ag prints out the VPN numbers to make your life eas­ier. 


THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
Questions 
• 	Before doing any translations, let’s use the simulator to study how linear page tables change size given different parameters. Compute the size of linear page tables as different parameters change. Some suggested inputs are below; by using the -v flag,you can see how many page-table entriesare .lled. 
First, to understand how linear page table size changes as the address space grows: 
paging-linear-translate.py -P 1k -a 1m -p 512m -v -n 0 paging-linear-translate.py -P 1k -a 2m -p 512m -v -n 0 paging-linear-translate.py -P 1k -a 4m -p 512m -v -n 0 
Then, to understand how linear page table size changes as page size grows: 
paging-linear-translate.py -P 1k -a 1m -p 512m -v -n 0 paging-linear-translate.py -P 2k -a 1m -p 512m -v -n 0 paging-linear-translate.py -P 4k -a 1m -p 512m -v -n 0 
Before running any of these, try to think about the expected trends. How should page-table size change as the address space grows? As the page size grows? Why shouldn’t we just use re­ally big pages in general? 
• 	Now let’s do some translations. Start with some small exam­ples, and change the number of pages that are allocated to the address space with the -u flag.For example: 
paging-linear-translate.py -P 1k -a 16k -p 32k -v -u 0 paging-linear-translate.py -P 1k -a 16k -p 32k -v -u 25 paging-linear-translate.py -P 1k -a 16k -p 32k -v -u 50 paging-linear-translate.py -P 1k -a 16k -p 32k -v -u 75 paging-linear-translate.py -P 1k -a 16k -p 32k -v -u 100 
What happens as you increase the percentage of pages that are allocated in each address space? 
• 	Now let’s try some different random seeds, and some differ­ent (and sometimes quite crazy) address-space parameters, for variety: 
OPERATING SYSTEMS ARPACI-DUSSEAU 
paging-linear-translate.py -P 8 -a 32 -p 1024 -v -s 1 paging-linear-translate.py -P 8k -a 32k -p 1m -v -s 2 paging-linear-translate.py -P 1m -a 256m -p 512m -v -s 3 
Which of these parameter combinations are unrealistic? Why? 
• 	Use the program to try out some other problems. Can you .nd the limits of where the program doesn’t work anymore? For example, what happens if the address-space size is bigger than physical memory? 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
18 




Paging: Faster Translations (TLBs) 
When we want to make things fast, the OS needs some help. And help usually comes from one place: the hardware. To speed address translation, we are going to add what is called (for historical reasons [CP78]) a translation-lookaside buffer,or TLB [C68,C95]. A TLB is part of the chip’s memory-management unit (MMU), and is simply ahardware cache of popular virtual-to-physical address translations; thus, a better name would be an address-translation cache.Upon each virtual memory reference, the hardware .rst checks the TLB to see if the desired translation is held therein; if so, the translation is performed (quickly) without having to consult the page table (which has all translations). Because of their tremendous performance im­pact, TLBs in a real sense make virtual memory possible [C95]. 
Figure 18.1 shows how hardware might handle a virtual address translation (assuming a simple linear page table and a hardware-managed TLB). In the common case (lines 3–9), we are hoping that a translation will be found in the TLB (a TLB hit)and thus the trans­lation will be quite fast (done in hardware near the processing core). In the less common case (lines 10–19), the translation won’t be in the cache (a TLB miss), and the system will have to consult the page table in main memory, update the TLB, and retry the memory refer­ence. 
207 
DESIGN TIP:CACHING 
Caching is one of the most fundamental performance techniques in computer systems, one that is used again and again to make the “common-case fast” [HP06]. The idea behind hardware caches is to take advantage of locality in instruction and data references. There are usually two types of locality: temporal locality and spatial lo­cality.With temporal locality, the idea is that an instruction or data item that has been recently accessed will likely be re-accessed soon in the future. Think of loop variables or instructions in a loop; they are accessed repeatedly over time. With spatial locality, the idea is that if a program accesses memory at address x,it will likely soon access memory near x.Imagine here streaming through an array of some kind, accessing one element and then the next. Of course,these properties depend on the exact nature of the program, and thusare not hard-and-fast laws but more like rules of thumb. 
Hardware caches, whether for instructions, data, or addresstrans­lations (as in our TLB) take advantage of locality by keeping copies of memory in small, fast on-chip memory. Instead of having to go to a (slow) memory to satisfy a request, the processor can .rstcheck if a nearby copy exists in a cache; if it does, the processor canaccess it quickly (i.e., in a few cycles) and avoid spending the costly time it takes to access memory (many nanoseconds). 
You might be wondering: if caches (like the TLB) are so great, why don’t we just make bigger caches and keep all of our data in them? Unfortunately, this is where we run into more fundamental laws like those of physics. If you want a fast cache, it has to be small, asis­sues like the speed-of-light and other physical constraintsbecome relevant. Any large cache by de.nition is slow, and thus defeats the purpose. Thus, we are stuck with small, fast caches; the question that remains is how to best use them to improve performance. 
18.1 Who Handles the Miss? 
One question that we must answer: who handles a TLB miss? 
Two answers are possible: the hardware, or the software (OS).In 
the olden days, the hardware had complex instruction sets (some-
OPERATING SYSTEMS ARPACI-DUSSEAU 
1 VPN = (VirtualAddress & VPN_MASK) >> SHIFT 
2 (Success, TlbEntry) = TLB_Lookup(VPN) 
3 if (Success == True) // TLB Hit 
4 if (CanAccess(TlbEntry.ProtectBits) == True) 
5 Offset = VirtualAddress & OFFSET_MASK 
6 PhysAddr = (TlbEntry.PFN << SHIFT) | Offset 
7 Register = AccessMemory(PhysAddr) 
8 else 
9 RaiseException(PROTECTION_FAULT) 

10 else // TLB Miss 
11 PTEAddr = PTBR + (VPN * sizeof(PTE)) 
12 PTE = AccessMemory(PTEAddr) 
13 if (PTE.Valid == False) 
14 RaiseException(SEGMENTATION_FAULT) 
15 else if (CanAccess(PTE.ProtectBits) == False) 
16 RaiseException(PROTECTION_FAULT) 
17 else 
18 TLB_Insert(VPN, PTE.PFN, PTE.ProtectBits) 
19 RetryInstruction() 
Figure 18.1: TLB Control Flow Algorithm 
times called CISC,for complex-instruction set computers) and the people who built the hardware didn’t much trust those sneaky OS people. Thus, the hardware would handle the TLB miss entirely. To do this, the hardware has to know exactly where the page tables are located in memory (via a page-table base register,used in line 11 in Figure 18.1), as well as their exact format;on a miss, the hard­ware would “walk” the page table, .nd the correct page-table entry and extract the desired translation, update the TLB with the transla­tion,andretrytheinstruction. Anexampleofan“older”architecture that has hardware-managed TLBs is the Intel x86 architecture, which uses a .xed multi-level page table (see the next chapter for details); the current page table is pointed to by the CR3 register [I09]. 
More modern architectures (e.g., MIPS R10k [H93] or Sun’s SPARC v9 
[WG00], both RISC or reduced-instruction set computers) have what 
is known as a software-managed TLB.On a TLBmiss, the hardware 
simply raises an exception (line 11 in Figure 18.2), which pauses the 
current instruction stream, raises the privilege level to kernel mode, 
and jumps to a trap handler.As you might guess, this trap handler is 
code within the OS that is written with the express purpose of han­
dling TLB misses. When run, the code will lookup the translation 
in the page table, use special “privileged” instructions to update the 
TLB, and return from the trap; at this point, the hardware retries the 
instruction (resulting in a TLB hit). 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
1 VPN = (VirtualAddress & VPN_MASK) >> SHIFT 2 (Success, TlbEntry) = TLB_Lookup(VPN) 3 if (Success == True) // TLB Hit 4 if (CanAccess(TlbEntry.ProtectBits) == True) 5 Offset = VirtualAddress & OFFSET_MASK 6 PhysAddr = (TlbEntry.PFN << SHIFT) | Offset 7 Register = AccessMemory(PhysAddr) 8 else 9 RaiseException(PROTECTION_FAULT) 
10 else // TLB Miss 
11 RaiseException(TLB_MISS) 

Figure 18.2: TLB Control Flow Algorithm (OS Handled) 
Let’s discuss a couple of important details. First, the return-from­trap instruction needs to be a little different than the return-from­trap we saw before when servicing a system call. In the latter case, the return-from-trap should resume execution at the instruction after the trap into the OS, just as a return from a procedure call returns to the instruction immediately following the call into the procedure. In the former case, when returning from a TLB miss-handling trap, the hardware must resume execution at the instruction that caused the trap; this retry thus lets the instruction run again, this time result­ing in a TLB hit. Thus, depending on how a trap or exception was caused, the hardware must save a different PC when trapping into the OS, in order to resume properly when the time to do so arrives. 
Second, when running the TLB miss-handling code, the OS needs to be extra careful not to cause an in.nite chain of TLB misses to occur. Many solutions exist; for example, you could keep TLB miss handlers in physical memory (where they are unmapped and not subject to address translation), or reserve some entries in the TLB for permanently-valid translations and use some of those permanent translation slots for the handler code itself; these wired translations always hit in the TLB. 
The primary advantage of the software-managed approach is .ex­ibility:the OS can use any data structure it wants to implement the page table, without necessitating hardware change. Anotheradvan­tage is simplicity;as you can see in the TLB control .ow (line 11 in Figure 18.2, in contrast to lines 11–19 in Figure 18.1), the hardware doesn’t have to do much on a miss; it raises an exception, and the OS TLB miss handler does the rest. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
ASIDE:RISC VS.CISC 
In the 1980’s, a great battle took place in the computer architec­ture community. On one side was the CISC camp, which stood for Complex Instruction Set Computing;on the other side was RISC, for Reduced Instruction Set Computing [PS81]. The RISC side was spear-headed by David Patterson at Berkeley and John Hennessy at Stanford (who are also co-authors of some famous books [HP06]), al­though later John Cocke was recognized with a Turing award forhis earliest work on RISC [CM00]. 
CISC instruction sets tend to have a lot of instructions in them, and each instruction is relatively powerful. For example, you might see a string copy, which takes two pointers and a length and copies bytes from source to destination. The idea behind CISC was that instructions should be high-level primitives, to make the assembly language itself easier to use, and to make code more compact. 
RISC instruction sets are exactly the opposite. A key observation behind RISC is that instruction sets are really compiler targets, and all compilers really want are a few simple primitives that they can use to generate high-performance code. Thus, RISC proponents ar­gued, let’s rip out as much from the hardware as possible (especially the microcode), and make what’s left simple, uniform, and fast. 
In the early days, RISC chips made a huge impact, as they were noticeably faster [BC91]; many papers were written; a few companies were formed (e.g., MIPS and Sun). However, as time progressed, CISC manufacturers such as Intel incorporated many RISC tech­niques into the core of their processors, for example by adding early pipeline stages that transformed complex instructions intomicro­instructions which could then be processed in a RISC-like manner. These innovations, plus a growing number of transistors on each chip, allowed CISC to remain competitive. The end result is that the debate died down, and today both types of processors can be made to run fast. 

18.2 TLB Contents: What’s In There? 
Let’s look at the contents of the hardware TLB in more detail. A typical TLB might have 32, 64, or 128 entries and be what is called 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
fully associative.Basically, this just means that any given translation can be anywhere in the TLB, and that the hardware will search the entire TLB in parallel to .nd the desired translation. A typical TLB entry might look like this: 
VPN PFN other bits Note that both the VPN and PFN are present in each entry, as atranslation could end up in any of these locations; the hardware searches the entries in parallel to see if there is a match. More interesting are the “other bits”. For example, the TLB com­monly has a valid bit, which says whether the entry has a valid trans­lation or not. Also common are protection bits, which determine how a page can be accessed (as in the page table). For example, code pages might be marked read and execute,whereas heap pages might be marked read and write.There may also be a few other .elds, in­cluding an address-space identi.er,a dirty bit,and so forth; see below for more information. 

18.3 TLB Issue: Context Switches 
With TLBs, a new issue arises when switching between processes (and hence address spaces). Speci.cally, the contents of theTLB con­tain virtual-to-physical translations that are only valid for the current running process; these translations are not meaningful for other pro­cesses. Thus, when switching to run another process, the hardware or OS or both must be careful. 
To understand this better, let’s look at an example. When one process (P1) is running, it accesses the TLB with translations that are valid for it. Assume here that the 0th virtual page of process P1 might be mapped to physical frame 10. Another process may also be ready in the system (P2), and the OS might be context-switching between it and P1; assume the 0th virtual page of P2 is mapped to physical frame 17. If entries for both processes were in the TLB, it might look 
like this: VPN 0 — 0 — 
OPERATING 
PFN 
10 — 17 — 
valid 
1 0 1 0 
prot 
rwx — rwx — 
SYSTEMS ARPACI-DUSSEAU 
In the TLB above, we clearly have a problem: VPN 0 translates to either PFN 10 (P1) or PFN 17 (P2), but the hardware can’t distinguish which entry is meant for which process. Thus, we need to do some more work in order for the TLB to correctly and ef.ciently support virtualization across multiple processes. And thus, a crux: 
THE CRUX: 
HOW TO MANAGE TLB CONTENTS ON A CONTEXT SWITCH When context-switching between processes, the translations in the TLB for the last process are not meaningful to the about-to-be-run process. What should the hardware or OS do in order to solve this problem? 
There are a number of possible solutions. One approach is to sim­ply .ush the TLB on context switches, thus emptying it before run­ning the next process. On a software-based system, this couldbe ac­complished with an explicit (and privileged) hardware instruction; with a hardware-managed TLB, the .ush could be enacted when the page-table base register is changed (the OS must change the PTBR on acontext switch anyhow). In either case, the .ush operation simply sets all valid bits to 0. 
By .ushing the TLB on each context switch, we now have a work­ing solution, as a process will never accidentally encounterthe wrong translations in the TLB. However, there is a cost: each time a process runs, it must incur a fair number of TLB misses as it touches data and code pages. If the OS is switching between processes frequently, this cost may be noticeable. 
To overcome this cost, some systems add a little extra hardware support to enable sharing of the TLB across context switches.In particular, the hardware could provide an address space identi.er (ASID).eld in the TLB. You can think ofthe ASID as a process iden­ti.er (PID), but usually it has fewer bits than that (say 8 for the ASID instead of the full 32 bits for a PID). 
If we take our example TLB from above and add ASIDs (and a few other .elds), we can observe that two identical VPNs for different processes can readily share the TLB; only the ASID .eld is needed to differentiate the two translations. With a few extra bits in each TLB entry, the OS and hardware can combine to enable entries from different processes’s page tables share the TLB: 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
VPN  PFN  valid  prot  ASID  
0  10  1  rwx  1  
—  —  0  —  —  
0  17  1  rwx  2  
—  —  0  —  —  

Thus, with address-space identi.ers, the TLB can hold transla­tions from different processes at the same time without any confu­sion. Of course, the hardware also needs to know which processis currently running in order to perform translations, and thusthe OS must, on a context switch, set some privileged register to theASID of the currently-running process. 
As an aside, you may also notice another case where two entries of the TLB are remarkably similar. In this example, there are two entries for two different processes at two different VPNs that point to the same physical page: 
VPN  PFN  valid  prot  ASID  
10  101  1  r-x  1  
—  —  0  —  —  
50  101  1  r-x  2  
—  —  0  —  —  

This situation might arise, for example, when two processes share apage (acode page, for example). In the example above, process 1 is sharing physical page 101 with process 2; P1 maps this page into the 10th page of its address space, whereas P2 maps it to the 50th page of its AS. Sharing of code pages (in binaries, or shared libraries) is useful as it reduces the number of physical pages in use.  
18.4  Issue: Replacement Policy  
As with any cache, and thus also with the TLB, one more issue that we must consider is cache replacement.Speci.cally, when we are installing a new entry in the TLB, we have to replace an old one, and thus the question: which one to replace?  
OPERATING SYSTEMS  ARPACI-DUSSEAU  

THE CRUX:TLB REPLACEMENT POLICY 
Which TLB entry should be replaced when we add a new TLB en­
try? The goal, of course, being to minimize the miss rate (or increase 
hit rate)and thus improve performance. 
We will study such policies in some detail when we tackle the 
problem of swapping pages to disk in a virtual memory system. Here 
we’ll just highlight a few of typical policies. 
One common approach is to evict the least-recently-used or LRU entry. The idea here is to take advantage of locality in the memory-reference stream; thus, it is likely that an entry that has notrecently been used is a good candidate for eviction as (perhaps) it won’t soon be referenced again. Another typical approach is to use a random policy. Randomness sometimes makes a bad decision but has the nice property that there are not any weird corner case behaviors that can cause pessimal behavior, e.g., think of a loop accessing n +1 pages, a TLB of size n,andan LRU replacement policy. 

18.5 Real Code 
To make sure you understand how the TLB works, let’s look at some C code that accesses a large array named a.Figure 18.3 shows the code snippet of interest. Let’s assume that integers in the array a are 4 bytes each, and that the page size is 4 KB. How many TLB misses does this code cause? 
To answer this question, we have to .rst make some further as­sumptions. For now, let us assume that we ignore the instruction fetches that obviously must execute in order to run the code; these too could cause TLB misses (likely one, or perhaps two if the code straddle two consecutive virtual pages), but let’s just focus on data accesses for now. Let’s also ignore accesses to the loop variable i; after all, it will likely be stored in a register by a smart compiler dur­ing the looping. Thus, the only accesses we are interested in are the accesses to the array a. 
Assuming the .rst integer of a is on the beginning of a page, 
we can assume this code will .rst generate a virtual address for the 
.rst integer, then the second, and so on. Let’s assume the .rstaccess 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
// assume a[] is an array of 4-byte integers 
int i; 
for (i = 0; i < size; i++) 

a[i] = i; 
Figure 18.3: TLB Hits/Misses During Array Access. 
to the page causes a TLB miss, because that page has never been referenced before. The next references to integers on that same page cause TLB hits. In fact, from the loop above, we should expect the following repeating pattern: a TLB miss to the .rst integer inthe array, followed by 1023 TLB hits for the remaining integers onthe page. You could thus compute the TLB hit rate for this code snippet: 
1023 
1024 ,or roughly 99.9%. 
Aquick question for you: how would you change the above code in order to generate a hit rate of 50%? How about 0%? Hint: it is easy; think about changing the amount you increment the variable i by, or the value of size. 

18.6 An Example 
Finally, let’s brie.y look at a real TLB and what is in it. This ex­ample is taken from the MIPS R4000 [H93], which is a great example of a modern system that uses software-managed TLBs. All 64 bits of this TLB entry can be seen in Figure 18.4. 

Figure 18.4: A MIPS TLB Entry. 
The MIPS R4000 supports a 32-bit address space with 4KB pages. Thus, we would expect a 20-bit VPN and 12-bit offset in our typical virtual address. However, as you can see in the TLB, there are only 19 bits for the VPN; as it turns out, user addresses will only comefrom half the address space (the rest reserved for the kernel) and hence only 19 bits of VPN are needed. The VPN translates to up to a 24-bit physical frame number (PFN), and hence can support systems with up to 64GB of (physical) main memory (224 4KB pages). 
There are a few other interesting bits in the MIPS TLB. We see a 
OPERATING SYSTEMS ARPACI-DUSSEAU 
global bit (G), which is used for pages that are globally-shared among processes. Thus, if the global bit is set, the ASID is ignored.We also see the 8-bit ASID,which the OS can use to distinguish between address spaces (as described above). One question for you: what should the OS do if there are more than 256 (28)processes running at a time? Finally, we see 3 Coherence (C) bits, which determine how apage is cached by the hardware (a bitbeyond the scope of these notes); a dirty bit which is marked when the page has been written to (we’ll see the use of this later); a valid bit which tells the hardware if there is a valid translation present in the entry. There is also a page mask .eld (not shown), which supports multiple page sizes; we’ll see later why having larger pages might be useful. Finally, some of the 64 bits are unused (shaded gray in the diagram). 
MIPS TLBs usually have 32 or 64 of these entries, most of which are used by user processes as they run. However, a few are reserved for the OS. A wired register can be set by the OS to tell the hardware how many slots of the TLB to reserve for the OS; the OS uses these reserved mappings for code and data that it wants to access during critical times, where a TLB miss would be problematic (e.g., while running the TLB miss handler). 
Because the MIPS TLB is software managed, there needs to be in­structions to update the TLB. The MIPS provides four such instruc­tions: TLBP,which probesthe TLBto see if a particular translation is in there; TLBR,which readsthe contents of a TLB entry into regis­ters; TLBWI,which replaces a speci.c TLB entry; and TLBWR,which replaces a random TLB entry. The OS uses these instructions toman­age the TLB’s contents. It is of course critical that these instructions are privileged;imagine what a user process coulddo if it couldmod­
ify the contents of the TLB!1 

18.7 Summary 
We have seen how hardware can help us make address translation faster. By providing a small, dedicated on-chip TLB as an address-translation cache, most memory references will hopefully behandled without having to access the page table in main memory. Thus, in the 
1
Answer: anything under the sun, including take over the machine, run arbitrary jobs or its own “OS”, or make the Sun disappear. 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
DESIGN TIP:RAM ISN’TALWAYS RAM 
The term random-access memory,or RAM,impliesthat you can ac­
cess any part of RAM just as quickly as another. While it is generally 
good to think of RAM in this way, you can probably see that be­
cause of hardware/OS features such as the TLB, accessing a particu­
lar page of memory may be more costly than you think, particularly 
if that page isn’t currently mapped by your TLB. Thus, it is always 
good to remember the implementation tip: RAM isn’t always RAM. 
Sometimes randomly accessing your address space, particular if the 
number of pages accessed exceeds the TLB coverage, can lead tose­
vere performance penalties. 
common case, the performance of the program will be almost as if memory isn’t being virtualized at all, an excellent achievement for an operating system, and certainly essential to the use of paging in modern systems. 
However, TLBs do not make the world rosy for every program that exists. In particular, if the number of pages a program accesses in a short period of time exceeds the number of pages that .t into the TLB, the program will generate a large number of TLB misses, and thus run quite a bit more slowly. We refer to this phenomenon as exceeding the TLB coverage,and it can be quite a problem for certain systems. One solution, as we’ll discuss in the next chapter, is to include support for larger page sizes; by mapping key data struc­tures into regions of the program’s address space that are mapped by larger pages, the effective coverage of the TLB can be increased. Support for large pages is often exploited by programs such asa database management system (a DBMS), which have certain data structures that are both large and randomly-accessed. 
One other TLB issue worth mentioning: TLB access can easily be­come a bottleneck in the CPU pipeline, in particular with whatis called a physically-indexed cache.With such a cache, address trans­lation has to take place before the cache is accessed, which can slow things down quite a bit. Because of this potential problem, people have looked into all sorts of clever ways to access caches with virtual addresses, thus avoiding the expensive step of translation in the case of a cache hit. Such a virtually-indexed cache solves some perfor­mance problems, but introduces new issues into hardware design as well. See Wiggins’s .ne survey for more details [W03]. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

References 
[BC91] “Performance from Architecture: Comparing a RISC anda CISC with Similar Hardware Organization” 
D. Bhandarkar and Douglas W. Clark Communications of the ACM, September 1991 
Agreat and fair comparison between RISC and CISC. The bottom line: on similar hardware, RISC was about a factor of three better in performance. 
[CM00] “The evolution of RISC technology at IBM” John Cocke and V. Markstein IBM Journal of Research and Development, 44:1/2 
Asummary of the ideas and work behind the IBM 801, which many consider the .rst true RISC microprocessor. 
[C95] “The Core of the Black Canyon Computer Corporation” John Couleur IEEE Annals of History of Computing, 17:4, 1995 
In this fascinating historical note, Couleur talks about howhe invented the TLB in 1964 while working for GE, and the fortuitous collaboration that thus ensued with the Project MAC folks at MIT. 
[CG68] “Shared-access Data Processing System” John F. Couleur and Edward L. Glaser Patent 3412382, November 1968 
The patent that contains the idea for an associative memory tostore address translations. The idea, according to Couleur, came in 1964. 
[CP78] “The architecture of the IBM System/370” 
R.P. Case and A. Padegs Communications of the ACM. 21:1, 73-96, January 1978 
Perhaps the .rst paper to use the term translation lookaside buffer.The name arises from the historical name for a cache, which was a lookaside buffer as called by those developing the Atlas system at the University of Manchester; a cache of address translations thus became a translation lookaside buffer.Even though the term lookaside buffer fell out of favor, TLB seems to have stuck, for whatever reason. 
[H93] “MIPS R4000 Microprocessor User’s Manual”. 
Joe Heinrich, Prentice-Hall, June 1993 
Available: http://cag.csail.mit.edu/raw/ 
documents/R4400 Uman book Ed2.pdf 




THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
[HP06] “Computer Architecture: A Quantitative Approach” John Hennessy and David Patterson Morgan-Kaufmann, 2006 
Agreat book about computer architecture. Even better if you can .nd the classic .rst edition. 
[I09] “Intel 64 and IA-32 Architectures Software Developer’s Manuals” Intel, 2009 Available: http://www.intel.com/products/processor/manuals 
In particular, pay attention to “Volume 3A: System Programming Guide Part 1” and “Volume 3B: System Programming Guide Part 2” 
[PS81] “RISC-I: A Reduced Instruction Set VLSI Computer” 
D.A. Patterson and C.H. Sequin 
ISCA ’81, Minneapolis, May 1981 

The paper that introduced the term RISC, and started the avalanche of research into simplifying computer chips for performance. 
[SB92] “CPU Performance Evaluation and Execution Time Prediction Using Narrow Spectrum Benchmarking” Rafael H. Saavedra-Barrera EECS Department, University of California, Berkeley Technical Report No. UCB/CSD-92-684, February 1992 www.eecs.berkeley.edu/Pubs/TechRpts/1992/CSD-92-684.pdf 
Agreat dissertation about how to predict execution time of applications by breaking them down into constituent pieces and knowing the cost of each piece. Probably the most interesting part that comes out of this work is the tool to measure details of the cache hierarchy (described in Chapter 5). Make sure to check out the wonderful diagrams therein. 
[W03] “A Survey on the Interaction Between Caching, Translation and Protection” Adam Wiggins University of New South Wales TR UNSW-CSE-TR-0321, August, 2003 
An excellent survey of how TLBs interact with other parts of the CPU pipeline, namely hardware caches. 
[WG00] “The SPARC Architecture Manual: Version 9” David L. Weaver and Tom Germond, September 2000 SPARC International, San Jose, California Available: http://www.sparc.org/standards/SPARCV9.pdf 
OPERATING SYSTEMS ARPACI-DUSSEAU 
Homework  
In this homework, you are to measure the size and cost of access­ 
ing a TLB. The idea is based on work by Saavedra-Barrera [SB92],  
who developed a simple but beautiful method to measure numerous  
aspects of cache hierarchies, all with a very simple user-level pro­ 
gram. Read his work for more details.  
The basic idea is to access some number of pages within large data  
structure (e.g., an array) and to time those accesses. For example,  
let’s say the TLB size of a machine happens to be 4 (which would  
be very small, but useful for the purposes of this discussion). If you  
write a program that touches 4 or fewer pages, each access should be  
aTLB hit, and thus relatively fast. However, once you touch 5 pages  
or more, repeatedly in a loop, each access will suddenly jump in cost,  
to that of a TLB miss.  
The basic code to loop through an array once should look like this:  
int jump = PAGESIZE / sizeof(int); for (i = 0; i < NUMPAGES * jump; i += jump) { a[i] += 1; }  
In this loop, one integer per page of the the array a is updated, up to the number of pages speci.ed by NUMPAGES.By timing such a  
loop repeatedly (say, a few hundred million times in another loop  
around this one, or however many loops are needed to run for a  
few seconds), you can time how long each access takes (on aver­age). By looking for jumps in cost as NUMPAGES increases, you can  
roughly determine how big the .rst-level TLB is, determine whether  
asecond-level TLB exists (and how big itis if itdoes), and in general  
get a good sense of how TLB hits and misses can affect performance.  
Here is an example graph:  
As you can see in the graph, when just a few pages are accessed (8  
or fewer), the average access time is roughly 5 nanoseconds. When  
16 or more pages are accessed, there is a sudden jump to about 20  
nanoseconds per access. A .nal jump in cost occurs at around 1024  
pages, at which point each access takes around 70 nanoseconds. From  
this data, we can conclude that there is a two-level TLB hierarchy; the  
.rst is quite small (probably holding between 8 and 16 entries); the  
second is larger but slower (holding roughly 512 entries). The over- 
THREE  
ARPACI-DUSSEAU  EASY PIECES  
(V0.5)  

TLB Size Measurement 
Time Per Access (ns) 
80 
60 
40 
20 
0 
1 4 16 64 256 1024 Number Of Pages 
Figure 18.5: Discovering TLB Sizes and Miss Costs. 
all difference between hits in the .rst-level TLB and misses is quite large, roughly a factor of fourteen. TLB performance matters! 
OPERATING SYSTEMS ARPACI-DUSSEAU 
Questions 
• 	
For timing, you’ll need to use a timer such as that made avail­able by gettimeofday().How precise is such a timer? How long does an operation have to take in order for you to time it precisely? (this will help determine how many times, in a loop, you’ll have to repeat a page access in order to time it success­fully) 

• 	
Write the program, called tlb.c,that can roughly measure the cost of accessing each page. Inputs to the program should be: the number of pages to touch and the number of trials. 

• 	
Now write a script in your favorite scripting language (csh, python, etc.) to run this program, while varying the number of pages accessed from 1 up to a few thousand, perhaps in­crementing by a factor of two per iteration. Run the script on different machines and gather some data. How many trials are needed to get reliable measurements? 

• 	
Next, graph the results, making a graph that looks similar to the one above. Use a good tool like ploticus.Visualiza­tion usually makes the data much easier to digest; why do you think that is? 

• 	
One thing to watch out for is compiler optimzation. Com­pilers do all sorts of clever things, including removing loops which increment values that no other part of the program sub­sequently uses. How can you ensure the compiler does not remove the main loop above from your TLB size estimator? 

• 	
Another thing to watch out for is the fact that most systems today ship with multiple CPUs, and each CPU, of course, has its own TLB hierarchy. To really get good measurements, you have to run your code on just one CPU, instead of letting the scheduler bounce it from one CPU to the next. How can you do that? (hint: look up “pinning a thread” on Google for some clues) What will happen if you don’t do this, and the code moves from one CPU to the other? 

• 	
Another issue that might arise relates to initialization. Ifyou don’t initialize the array a above before accessing it, the .rst time you access it will be very expensive, due to initial access costs such as demand zeroing. Will this affect your code and its timing? What can you do to counterbalance these potential costs? 


THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
19 




Paging: Smaller Tables 
We now tackle the second problem that paging introduces: pageta­
bles are too big and thus consume too much memory. We start out with a linear page table. As you might recall1,linear page tables get pretty big. Assume again a 32-bit address space (232 bytes), with 4KB (212 byte) pages and a 4-byte page-table entry. An address space 
thus has roughly 1 million virtual pages in it ( 232 ); multiply by the 
212 
page-table size and you see that our page table is 4MB in size. Recall also: we usually have one page table for every process in the system! Thus, with a hundred active processes (not uncommon on a modern system), we will be allocating 400MB of memory just for page tables! Thus, we are in search of some techniques to reduce this heavy bur­den. There are a lot of them, so let’s get going. 
19.1 Simple Solution: Bigger Pages 
We could reduce the size of the page table in one simple way: use bigger pages. Take our 32-bit address space again, but this time assume 16KB pages. We would thus have an 18-bit VPN plus a 14-bit offset. Assuming the same size for each PTE (4 bytes), we now have 
218 
entries in our linear page table and thus a total size of 1MB per page table, a factor of four reduction in size of the page table(not surprisingly, the reduction exactly mirrors the factor of four increase in page size). 
1
Or might not; this paging thing is getting out of control! 
225 
ASIDE:MULTIPLE PAGE SIZES 
As an aside, do note that many architectures (e.g., MIPS, SPARC, x86-64) now support multiple page sizes. Usually, a small (4KB or 8KB) page size is used. However, if a “smart” application requests it, a single large page (e.g., of size 4MB) can be used for a speci.c portion of the address space, enabling such applications to place a frequently-used (and large) data structure in such a space while con­suming only a single TLB entry. This type of large page usage is common in database management systems and other high-end com­mercial applications. The main reason for multiple page sizes is not to save page table space, however; it is to reduce pressure on the TLB, enabling a program to access more of its address space without suffering from too many TLB misses. 
The major problem with this approach, however, is that big pages lead to waste within each page, a problem known as internal frag­mentation (as the waste is internal to the unit of allocation). Ap­plications thus end up allocating pages but only using littlebits and pieces of each, and memory quickly .lls up with these overly-large pages. Thus, most systems use relatively small page sizes in the com­mon case: 4KB (as in x86) or 8KB (as in SPARCv9). Our problem will not be solved so simply, alas. 

19.2 Hybrid Approach: Paging and Segments 
Whenever you have two reasonable but different approaches to something in life, you should always examine the combinationof the two to see if you can obtain the best of both worlds. We call such a combination a hybrid.For example, why eat just chocolate or plain peanut butter when you can instead combine the two in a lovely hybrid known as the Reese’s Peanut Butter Cup [M28]? 
Years ago, the creators of Multics, and in particular Jack Dennis, chanced upon such an idea in the construction of the Multics virtual memory system [M07]. Speci.cally, Dennis had the idea of combin­ing paging and segmentation in order to reduce the memory over­head of page tables. We can see why this might work by examininga typical linear page table in more detail. Assume we have an address 
OPERATING SYSTEMS ARPACI-DUSSEAU 
Virtual Address Space Physical Memory 

Figure 19.1: A 16-KB address space with 1-KB pages 
space in which the used portions of the heap and stack are small. For the example, we use a tiny 16KB address space with 1KB pages (Figure 19.1); the page table for this address space is in Table 19.1. 
This example assumes the single code page (VPN 0) is mapped to physical page 10, the single heap page (VPN 4) to physical page23, and the two stack pages at the other end of the address space (VPNs 14 and 15) are mapped to physical pages 28 and 4, respectively.As you can see from the picture, most of the page table is unused, full of invalid entries. What a waste! And this is for a tiny 16KB address space. Imagine the page table of a 32-bit address space and allthe potential wasted space in there! Actually, don’t imagine such a thing; it’s far too gruesome. 
Thus, our hybrid approach: instead of having a single page table for the entire address space of the process, why not have one per logical segment? In this example, we might thus have three page tables, one for the code, heap, and stack parts of the address space. 
Now, remember with segmentation, we had a base register that told us where each segment lived in physical memory, and a bound or limit register that told us the size of said segment. In our hybrid, we still have those structures in the MMU; here, we use the basenot 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
PFN valid prot present dirty 
10  1  r-x  1  0  
- 0  —  - - 
- 0  —  - - 
- 0  —  - - 
23  1  rw­ 1  1  
- 0  —  - - 
- 0  —  - - 
- 0  —  - - 
- 0  —  - - 
- 0  —  - - 
- 0  —  - - 
- 0  —  - - 
- 0  —  - - 
- 0  —  - - 
28  1  rw­ 1  1  
4  1  rw­ 1  1  

Table 19.1: A Page Table for 16-KB Address Space 
to point to the segment itself but rather to hold the physical address of the page table of that segment. The bounds register is used to indicate the end of the page table (i.e., how many valid pages it has). 
Let’s do a simple example to clarify. Assume a 32-bit virtual ad­dress space with 4KB pages, and an address space split into four seg­ments. We’ll only use three segments for this example: one forcode, one for heap, and one for stack. 
To determine which segment an address refers to, we’ll use the top two bits of the address space. Let’s assume 00 is the unused segment, with 01 for code, 10 for the heap, and 11 for the stack.Thus, avirtual address looks like this: 

In the hardware, assume that there are thus three base/bounds pairs, one each for code, heap, and stack. When a process is running, the base register for each of these segments contains the physical ad­dress of a linear page table for that segment; thus, each process in the system now has three page tables associated with it. On a context switch, these registers must be changed to re.ect the location of the page tables of the newly-running process. 
On a TLB miss (assuming a hardware-managed TLB), the hard­
ware uses the segment bits (SN)to determine which base and bounds 
pair to use. The hardware then takes the physical address therein and 
OPERATING SYSTEMS ARPACI-DUSSEAU 
DESIGN TIP:USE HYBRIDS When you have two good and seemingly opposing ideas, you should always see if you can combine them into a hybrid that manages to achieve the best of both worlds. Hybrid corn species, for example, are known to be more robust than any naturally-occurring species. Of course, not all hybrids are a good idea; see the Zeedonk (or Zon­key), which is a cross of a Zebra and a Donkey. If you don’t believe such a creature exists, look it up, and prepare to be amazed. 
combines it with the VPN as follows to form the address of the page table entry (PTE): 
SN = (VirtualAddress & SEG_MASK) >> SN_SHIFT VPN = (VirtualAddress & VPN_MASK) >> VPN_SHIFT AddressOfPTE = Base[SN] + (VPN * sizeof(PTE)) 
This sequence should look familiar; it is virtually identical to what we saw before with linear page tables. The only difference, ofcourse, is the use of one of three segment base registers instead of thesingle page table base register. 
The critical difference in our hybrid scheme is the presence of a bounds register per segment; each bounds register holds the value of the maximum valid page in the segment. For example, if the code segment is using its .rst three pages (0, 1, and 2), the code segment page table will only have three entries allocated to it and thebounds register will be set to 3; memory accesses beyond the end of theseg­ment will generate an exception and likely lead to the termination of the process. In this manner, our hybrid approach realizes a signi.­cant memory savings compared to the linear page table; unallocated pages between the stack and the heap no longer take up space in a page table (just to mark them as not valid). 
However, as you might notice, this approach is not without prob­lems. First, it still requires us to use segmentation; as we discussed before, segmentation is not quite as .exible as we would like.Sec­ond, this hybrid causes external fragmentation to arise again. While most of memory is managed in page-sized units, page tables nowcan be of arbitrary size (in multiples of PTEs). Thus, .nding freespace for them in memory is somewhat complicated. For these reasons, peo­ple continued to look for better approaches to implementing smaller page tables. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
Linear Page Table Multi-level Page Table PTBR 201 PDBR 200 PFN 
PFN 200 
valid
prot

PFN 201


The Page Directory 

[Page 1 of PT: Not Allocated] 
[Page 2 of PT: Not Allocated] 

Figure 19.2: Linear (Left) and Multi-Level (Right) Page Tables 

19.3 Multi-level Page Tables 
Adifferent approach doesn’t rely on segmentation but attacks the same problem: how to get rid of all those invalid regions in thepage table instead of keeping them all in memory? We call this approach a multi-level page table,asit turnsthe linear page table into some­thing like a tree. This approach is so effective that many modern systems employ it (e.g., x86). We now describe this approach in de­tail. 
The basic idea behind a multi-level page table is simple. First, chop up the page table into page-sized units; then, if an entire page of page-table entries (PTEs) is invalid, don’t allocate thatpage of the page table at all. To track whether a page of the page table is valid (and if valid, where it is in memory), use a new structure, called the page directory.The page directory thus either can be used to tell you where a page of the page table is, or that the entire page of the page table contains no valid pages. 
Figure 19.2 shows an example. On the left of the .gure is the clas­sic linear page table; even though most of the middle regions of the address space are not valid, we still have to have page-table space allocated for those regions (i.e., the middle two pages of thepage table). On the right is a multi-level page table. The page directory marks just two pages of the page table as valid (the .rst and last); 
OPERATING SYSTEMS ARPACI-DUSSEAU 
DESIGN TIP:TIME-SPACE TRADE-OFFS When building a data structure, one should always consider time-space trade-offs in its construction. Usually, if you wish to make access to a particular data structure faster, you will have topay a space-usage penalty for the structure. 
thus, just those two pages of the page table reside in memory. And thus you can see one way to visualize what a multi-level table is do­ing: it just makes parts of the linear page table disappear (freeing those frames for other uses), and tracks which pages of the page ta­ble are allocated with the page directory. 
The page directory, in a simple two-level table, contains oneentry per page of the page table. It consists of a number of page direc­tory entries (PDE).APDEhas a valid bit and a page frame number (PFN), similar to a PTE. However, as hinted at above, the meaning of this valid bit is slightly different: if the PDE entry is valid, it means that at least one of the pages of the page table that the entry points to (via the PFN) is valid (i.e., in at least one PTE on that page pointed to by this PDE, the valid bit in that PTE is set to 1). If the PDE entry is not valid, the rest of the PDE is not de.ned. 
Multi-level page tables have some obvious advantages over ap­proaches we’ve seen thus far. First, and perhaps most obviously, the multi-level table only allocates page-table space in proportion to the amount of address space you are using; thus it is generally compact and supports sparse address spaces. 
Second, if carefully constructed, each portion of the page table .ts neatly within a page, making it easier to manage memory; the OS can simply grab the next free page when it needs to allocate or grow a page table. Contrast this to a simple (non-paged) linear page 
table2,which is just an array of PTEs indexed by VPN; with such astructure, the entire linear page table mustreside contiguously in physical memory. For a large page table (say 4MB), .nding such alarge chunk of unused contiguous free physical memory can be quite a challenge. With a multi-level structure, we add a level of 
2We are making some assumptions here, in particular that all page tables reside in their entirety in physical memory (i.e., they are not swappedto disk); we’ll soon relax this assumption. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
indirection through use of the page directory, which points to pieces of the page table; that indirection allows us to place page-table pages wherever we would like in physical memory. 
It should be noted that there is a cost to multi-level tables; on a TLB miss, two loads from memory will be required to get the right translation information from the page table (one for the pagedirec­tory, and one for the PTE itself), in contrast to just one load with a linear page table. Thus, the multi-level table is a small example of a time-space trade-off.We wanted smaller tables (and got them), but not for free; although in the common case (TLB hit), performance is obviously identical, a TLB miss suffers from a higher cost with this smaller table. 
Another obvious negative is complexity.Whether it is the hard­ware or OS handling the page-table lookup (on a TLB miss), do­ing so is undoubtedly more involved than a simple linear page-table lookup. Often we are willing to increase complexity in order to im­prove performance or reduce overheads; in the case of a multi-level table, we make page-table lookups more complicated in order to save valuable memory. 
DESIGN TIP:BE WARY OF COMPLEXITY 
System designers should be wary of adding complexity into their 
system. What a good systems builder does is implement the least 
complex system that achieves the task at hand. For example, ifdisk 
space is abundant, you shouldn’t design a .le system that works 
hard to use as few bytes as possible; similarly, if processorsare fast, 
it is better to write a clean and understandable module withinthe 
OS than perhaps the most CPU-optimized, hand-assembled codefor 
the task at hand. Be wary of needless complexity, in prematurely-
optimized code or other forms; such approaches make systems 
harder to understand, maintain, and debug. As Antoine de Saint-
Exupery famously wrote: “Perfection is .nally attained not when 
there is no longer anything to add, but when there is no longer any­
thing to take away.” 
OPERATING SYSTEMS ARPACI-DUSSEAU 
0000 0000 0000 0001 0000 0010 0000 0011 0000 0100 0000 0101 0000 0110 0000 0111 
................ 


1111 1100 (free) 1111 1101 (free) 1111 1110 stack 1111 1111 stack 
Figure 19.3: A 16-KB address space with 64-byte pages 
ADetailedMulti-LevelExample 
To understand the idea behind multi-level page tables better, let’s do an example. Imagine a small address space of size 16 KB, with 64­byte pages. Thus, we have a 14-bit virtual address space, with8 bits for the VPN and 6 bits for the offset. A linear page table would have 
28 (256) entries, even if only a small portion of the address space is in use. Figure 19.3 presents one example of such an address space. 
In this example, virtual pages 0 and 1 are for code, virtual pages 4and 5 for the heap, and virtual pages 254 and 255 for the stack;the rest of the pages of the address space are unused. 
To build a two-level page table for this address space, we start with our full linear page table and break it up into page-sizedunits. Recall our full table (in this example) has 256 entries; assume each PTE is 4 bytes in size. Thus, our page table is 1KB (256 × 4bytes) in size. Given that we have 64-byte pages, the 1-KB page table canbe divided into 16 64-byte pages; each page can hold 16 PTEs. 
What we need to understand now is how to take a VPN and use it to index .rst into the page directory and then into the page of the page table. Remember that each is an array of entries; thus, all we need to .gure out is how to construct the index for each from pieces of the VPN. 
Let’s .rst consider indexing into the page directory. Our page ta­ble in this example is small: it consists of 256 entries, spread across 16 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
pages. The page directory needs one entry per page of the page ta­ble; thus, the page directory has 16 entries. Because the directory has 16 entries, we need four bits of the VPN to index into the directory; we use the top four bits of the VPN, as follows: 
VPN offset 
13  12  11  10  9  8  7  6  5  4  3  2  1  0  


Page Directory Index 
Once we extract the page-directory index (PDIndex for short) from the VPN, we can use it to .nd the address of the page-directory entry (PDE) with a simple index calculation: PDEAddr = PageDirBase+(PDIndex * sizeof(PDE)).This results in our page directory, which we can now examine to make further progress in our transla­tion. 
If the page-directory entry is marked invalid, we know that the access is invalid, and thus raise an exception. If, however, the PDE is valid, we have more work to do. Speci.cally, we now have to fetch the page-table entry (PTE) from the page of the page table pointed to by this page-directory entry. To .nd this PTE, we have to indexinto the portion of the page table using the remaining bits of the VPN: 
VPN offset 
13  12  11  10  9  8  7  6  5  4  3  2  1  0  


Page Directory Index  Page Table Index  
This page-table index (PTIndex for short) can then be used to index into the page table itself, giving us the address of our PTE:  
PTEAddr = (PDE.PFN << SHIFT) + (PTIndex * sizeof(PTE))  
Note that the page-frame number (PFN) obtained from the page-directory entry must be left-shifted into place before combining it with the page-table index to form the address of the PTE. To see if this all makes sense, we’ll now .ll in a multi-level page table with some actual values, and translate a single virtualaddress.  
OPERATING SYSTEMS  ARPACI-DUSSEAU  

pointer to 
page of PT valid? 
100 1 
—— 0 
—— 0 
—— 0 
—— 0 
—— 0 
—— 0 
—— 0 
—— 0 
—— 0 
—— 0 
—— 0 
—— 0 
—— 0 
—— 0 
101 1 
Table 19.2: A Page Directory 
Let’s begin with the page directory for this example, found in Table 
19.2. 
In the .gure, you can see that each page directory entry (PDE) describes something about a page of the page table for the address space. In this example, we have two valid regions in the address space (at the beginning and end), and a number of invalid mappings in-between. 
In physical page 100 (the physical frame number of the 0th page of the page table), we have the .rst page of 16 page table entries for the .rst 16 VPNs in the address space. See Table 19.3 for the contents of this portion of the page table. 
This page of the page table contains the mappings for the .rst 16 VPNs; in our example, VPNs 0 and 1 are valid (the code segment),as are 4 and 5 (the heap). Thus, the table has mapping informationfor each of those pages. The rest of the entries are marked invalid. 
The other valid page of page table is found inside PFN 101. This page contains mappings for the last 16 VPNs of the address space; see Table 19.4 for details. 
In the example, VPNs 254 and 255 (the stack) have valid map­pings. Hopefully, what we can see from this example is how much space savings are possible with a multi-level indexed structure. In 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
PFN valid prot 101 r-x 231 r-x 
–0 
— 

–0 
— 801 rw­591 rw­

–0 
— 

–0 
— 

–0 
— 

–0 
— 

–0 
— 

–0 
— 

–0 
— 

–0 
— 

–0 
— 

–0 
— 


Table 19.3: The First Page of the Page Table (PFN 100) 
this example, instead of allocating the full sixteen pages for a linear page table, we allocate only three:one for the page directory, and two for the chunks of the page table that have valid mappings. The savings for large (32-bit or 64-bit) address spaces could obviously be much greater. 
Finally, let’s use this information in order to perform a translation. 
Here is an address that refers to the 0th byte of VPN 254: 0x3F80,or 
11 1111 1000 0000 in binary. 
Recall that we will use the top 4 bits of the VPN to index into the page directory. Thus, 1111 will choose the last (15th, if you start at the 0th) entry of the page directory above. This points us to a valid page of the page table located at address 101.We then use the next 4 bits of the VPN (1110)to index into that page ofthe page table and .nd the desired PTE. 1110 is the next-to-last (14th) entry on the page, and tells us that page 254 of our virtual address space is mapped at physical page 55. By concatenating PFN=55 (or hex 0x37)with offset=000000, we can thus form our desired physical addressand issue the request to the memory system: PhysAddr = (PTE.PFN << SHIFT) + offset = 00 1101 1100 0000 = 0x0DC0. 
You should now have some idea of how to construct a two-level page table, using a page directory which points to pages of thepage table. Unfortunately, however, our work is not done. As we’llnow discuss, sometimes two levels of page table is not enough! 
OPERATING SYSTEMS ARPACI-DUSSEAU 
PFN valid prot 
–0 — –0 — –0 — –0 — –0 — –0 — –0 — –0 — –0 — –0 — –0 — –0 — –0 — –0 — 551 rw­451 rw-
Table 19.4: The Last Page of the Page Table (PFN 101) 

More Than Two Levels 
In our example thus far, we’ve assumed that multi-level page tables only have two levels: a page directory and then pieces of the page table. In some cases, a deeper tree is possible (and indeed, needed). 
Let’s take a simple example and use it to show why a deeper multi-level table can be useful. In this example, assume we have a30-bitvirtual address space, and asmall (512byte) page. Thus our virtual address has a 21-bit virtual page number component and a 9-bit offset. 
Remember our goal in constructing a multi-level page table: to make each piece of the page table .t within a single page. Thus far, we’ve only considered the page table itself; however, what ifthe page directory gets too big? 
To determine how many levels are needed in a multi-level table to make all pieces of the page table .t within a page, we start by determining how many page-table entries .t within a page. Given our page size of 512 bytes, and assuming a PTE size of 4 bytes, you should see that you can .t 128 PTEs on a single page. When we index into a page of the page table, we can thus conclude we’ll need the least signi.cant 7 bits (log2128)of the VPN as an index: 
What you also might notice from the diagram above is how many bits are left into the (large) page directory: 14. If our page directory 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
VPN offset 

Page Directory Index Page Table Index 
has 214 entries, it spans not one page but 128, and thus our goal of making every piece of the multi-level page table .t into a pagevan­ishes. 
To remedy this problem, we build a further level of the tree, by splitting the page directory itself into multiple pages, andthen adding another page directory on top of that, to point to the pages of the page directory. We can thus split up our virtual address as follows: 
VPN offset 
2928272625242322212019181716151413121110 9 8 7 6 5 4 3 2 1 0 
PD Index 0 PD Index 1 Page Table Index 
Now, when indexing the upper-level page directory, we use the very top bits of the virtual address (PD Index 0 in the diagram); this index can be used to fetch the page-directory entry from the top-level page directory. If valid, the second level of the page directory is consulted by combining the physical frame number from the top-level PDE and the next part of the VPN (PD Index 1). Finally, if valid, the PTE address can be formed by using the page-table in­dex combined with the address from the second-level PDE. Whew! That’s a lot of work. And all just to look something up in a multi­level table. 

The Translation Process: Remember the TLB 
To summarize the entire process of address translation usinga two-level page table, we once again present the control .ow in algorith­mic form (Figure 19.4). The .gure shows what happens in hardware (assuming a hardware-managed TLB) upon every memory reference. 
As you can see from the .gure, before any of the complicated multi-level page table access occurs, the hardware .rst checks the TLB; upon a hit, the physical address is formed directly without ac­cessing the page table at all, as before. Only upon a TLB miss does 
OPERATING SYSTEMS ARPACI-DUSSEAU 
1 VPN = (VirtualAddress & VPN_MASK) >> SHIFT 
2 (Success, TlbEntry) = TLB_Lookup(VPN) 
3 if (Success == True) // TLB Hit 
4 if (CanAccess(TlbEntry.ProtectBits) == True) 
5 Offset = VirtualAddress & OFFSET_MASK 
6 PhysAddr = (TlbEntry.PFN << SHIFT) | Offset 
7 Register = AccessMemory(PhysAddr) 
8 else 
9 RaiseException(PROTECTION_FAULT) 

10 else // TLB Miss 
11 // first, get page directory entry 
12 PDIndex = (VPN & PD_MASK) >> PD_SHIFT 
13 PDEAddr = PDBR + (PDIndex * sizeof(PDE)) 
14 PDE = AccessMemory(PDEAddr) 
15 if (PDE.Valid == False) 
16 RaiseException(SEGMENTATION_FAULT) 
17 else 
18 // PDE is valid: now fetch PTE from page table 
19 PTIndex = (VPN & PT_MASK) 
20 PTEAddr = (PDE.PFN << SHIFT) + (PTIndex * sizeof(PTE)) 
21 PTE = AccessMemory(PTEAddr) 
22 if (PTE.Valid == False) 
23 RaiseException(SEGMENTATION_FAULT) 
24 else if (CanAccess(PTE.ProtectBits) == False) 
25 RaiseException(PROTECTION_FAULT) 
26 else 
27 TLB_Insert(VPN, PTE.PFN, PTE.ProtectBits) 
28 RetryInstruction() 

Figure 19.4: Multi-level Page Table Control Flow 
the hardware need to perform the full multi-level lookup. On this path, you can see the cost of our traditional two-level page table: two additional memory accesses are needed to look up a valid trans­lation. 


19.4 Inverted Page Tables 
An even more extreme space savings in the world of page tables is found with inverted page tables.Here, instead of having many page tables (one per process of the system), we keep a single page table that has an entry for each physical page of the system. The entry tells us which process is using this page, and which virtual page of that process maps to this physical page. 
Finding the correct entry is now a matter of searching through this data structure. A linear scan would be expensive, and thus a 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
hash table is often built over the base structure to speed lookups. The PowerPC is one example of such an architecture [JM98]. 
More generally, inverted page tables illustrate what we’ve said from the beginning: page tables are just data structures. Youcan do lots of crazy things with data structures, making them smaller or bigger, making them slower or faster. Multi-level and inverted page tables are just two examples of the many things one could do. 

19.5 Swapping the Page Tables to Disk 
Finally, we discuss the relaxation of one .nal assumption. Thus far, we have assumed that page tables reside in kernel-owned phys­ical memory. Even with our many tricks to reduce the size of page tables, it is still possible, however, that they may be too bigto .t into memory all at once. Thus, some systems place such page tables in kernel virtual memory,thereby allowing the system to swap some of these page tables to disk when memory pressure gets a littletight. We’ll talk more about this in future notes, once we understandhow to move pages in and out of memory in more detail. 

19.6 Paging: Summary 
We have now seen how real page tables are built; not necessar­ily just as linear arrays but as more complex data structures.The trade-offs such tables present are in time and space – the bigger the table, the faster a TLB miss can be serviced, as well as the converse –and thus the right choice of structure depends strongly on the con­straints of the given environment. In a memory-constrained system (like many older systems), small structures make sense; in a system with a reasonable amount of memory and with workloads that ac­tively use a large number of pages, a bigger table that speeds up TLB misses might be the right choice. With software-managed TLBs, the entire space of data structures opens up to the delight of the operat­ing system innovator (hint: that’s you). 
OPERATING SYSTEMS ARPACI-DUSSEAU 

References 
[JM98] “Virtual Memory: Issues of Implementation” Bruce Jacob and Trevor Mudge IEEE Computer, June 1998 
An excellent survey of a number of different systems and theirapproach to virtualizing memory. Plenty of details on x86, PowerPC, MIPS, and other architectures. 
[LL82] “Virtual Memory Management in the VAX/VMS Operating System” Hank Levy and P. Lipman IEEE Computer, Vol. 15, No. 3, March 1982 
Aterri.c paper about a real virtual memory manager in a classic operating system, VMS. So terri.c, in fact, that we’ll use it to review everything we’velearned about virtual memory thus far a few chapters from now. 
[M28] “Reese’s Peanut Butter Cups” Mars Candy Corporation. 
Apparently these .ne confections were invented in 1928 by Harry Burnett Reese, a former dairy farmer and shipping foreman for one Milton S. Hershey. At least, that is what it says on Wikipedia. If true, Hershey and Reese probably hated each other’s guts, as any two chocolate barons should. 
[M07] “Multics: History” 
Available: http://www.multicians.org/history.html 

This amazing web site provides a huge amount of history on the Multics system, certainly one of the most in.uential systems in OS history. The quote from therein: “Jack Dennis of MIT contributed in.uential architectural ideas to the beginning of Multics, especially the idea of com­bining paging and segmentation.” (from Section 1.2.1) 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

Homework 
This fun little homework tests if you understand how a multi­level page table works. And yes, there is some debate over the use of the term “fun” in the previous sentence. The program is called: paging-multilevel-translate.py. 
Some basic assumptions: 
• 
The page size is an unrealistically-small 32 bytes 

• 	
The virtual address space for the process in question (assume there is only one) is 1024 pages, or 32 KB 

• 
physical memory consists of 128 pages 


Thus, a virtual address needs 15 bits (5 for the offset, 10 for the VPN). A physical address requires 12 bits (5 offset, 7 for the PFN). 
The system assumes a multi-level page table. Thus, the upper .ve bits of a virtual address are used to index into a page directory; the page directory entry (PDE), if valid, points to a page of the page table. Each page table page holds 32 page-table entries (PTEs). Each PTE, if valid, holds the desired translation (physical framenumber, or PFN) of the virtual page in question. 
The format of a PTE is thus: 
VALID | PFN6 ... PFN0 
and is thus 8 bits or 1 byte. 
The format of a PDE is essentially identical: 
VALID | PT6 ... PT0 
You are given two pieces of information to begin with. 
First, you are given the value of the page directory base register 
(PDBR), which tells you which page the page directory is located 
upon. 
Second, you are given a complete dump of each page of memory. Apage dump looks like this: 
page0: 08 00 01 15 11 1d 1d 1c 01 17 1514 16 1b 13 0b ... page1: 19 05 1e 13 02 16 1e 0c 15 09 0616 00 19 10 03 ... page2: 1d 07 11 1b 12 05 07 1e 09 1a 1817 16 18 1a 01 ... ... 
OPERATING SYSTEMS ARPACI-DUSSEAU 
which shows the 32 bytes found on pages 0, 1, 2, and so forth. The .rst byte (0th byte) on page 0 has the value 0x08, the second is 0x00, the third 0x01, and so forth. 
You are then given a list of virtual addresses to translate. 
Use the PDBR to .nd the relevant page table entries for this vir­tual page. Then .nd if it is valid. If so, use the translation toform a .nal physical address. Using this address, you can .nd the VALUE that the memory reference is looking for. 
Of course, the virtual address may not be valid and thus generate afault. 
Some useful options: 
-s SEED, --seed=SEED the random seed -n NUM, --addresses=NUM number of virtual addresses to generate -c, --solve compute answers for me 
Change the seed to get different problems, as always. 
Change the number of virtual addresses generated to do more translations for a given memory dump. 
Use -c (or –solve) to show the solutions. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
Questions 
• 	
With a linear page table, you need a single register to locate the page table, assuming that hardware does the lookup upon aTLB miss. How many registers do you need to locate atwo­level page table? A three-level table? 

• 	
Use the simulator to perform translations given random seeds 0, 1, and 2, and check your answers using the -c .ag. How many memory references are needed to perform each lookup? 

• 	
Given your understanding of how cache memory works, how do you think memory references to the page table will behave in the cache? Will they lead to lots of cache hits (and thus fast accesses?) Or lots of misses (and thus slow accesses)? 


OPERATING SYSTEMS ARPACI-DUSSEAU 
20 




Beyond Physical Memory: Mechanisms 
Thus far, we’ve assumed that an address space is unrealistically small and .ts into physical memory. In fact, we’ve been assuming that ev­ery address space of every running process .ts into memory. We will now relax these big assumptions, and assume that we wish to sup­port many concurrently-running large address spaces. 
To do so, we require an additional level in the memory hierarchy. Thus far, we have assumed that all pages reside in physical memory. However, to support large address spaces, the OS will need a place to stash away portions of address spaces that currently aren’t in great demand. In general, the characteristics of such a location are that it should have more capacity than memory; as a result, it is generally slower (if it were faster, we would just use it as memory, no?).In modern systems, this role is usually served by a hard disk drive. Thus, in our memory hierarchy, big and slow hard drives sit at the bottom, with memory just above. And thus we arrive at the crux of the problem: 
THE CRUX:HOW TO GO BEYOND PHYSICAL MEMORY How can the OS make use of a larger, slower device to transparently provide the illusion of a large virtual address space? 
One question you might have: why do we want to support a sin­gle large address space for a process? Once again, the answer is convenience and ease of use. With a large address space, you don’t have to worry about if there is room enough in memory for your 
245 
ASIDE:STORAGE TECHNOLOGIES 
We’ll delve much more deeply into how I/O devices actually work 
later (see the chapter on I/O devices). So be patient! And of course 
the slower device need not be a hard disk, but could be something 
more modern such as a Flash-based SSD. We’ll talk about those 
things too. For now, just assume we have a big and relatively-slow 
device which we can use to help us build the illusion of a very large 
virtual memory, even bigger than physical memory itself. 
program’s data structures; rather, you just write the program natu­rally, allocating memory as needed. It is a powerful illusionthat the OS provides, and makes your life vastly simpler. You’re welcome! A contrast is found in older systems that used memory overlays,which required programmers to manually move pieces of code or data in and out of memory as they were needed [D97]. Try imagining what this would be like: before calling a function or accessing some data, you need to .rst arrange for the code or data to be in memory; yuck! 
Beyond just a single process, the addition of swap space allows the OS to support the illusion of a large virtual memory for multi­ple concurrently-running processes. The invention of multiprogram­ming (running multiple programs “at once”, to better utilizethe ma­chine) almost demanded the ability to page out some pages, as early machines clearly could not hold all the pages needed by all processes at once. Thus, the combination of multiprogramming and ease-of­use leads us to want to support using more memory than is physi­cally available. It is something that all modern VM systems do; it is now something we will learn more about. 
20.1 Swap Space 
The .rst thing we will need to do is to reserve some space on the disk for moving pages back and forth. In operating systems, we generally refer to such space as swap space,because we swap pages out of memory to it and swap pages into memory from it. Thus, we will simply assume that the OS can read from and write to the swap space, in page-sized units. To do so, the OS will need to have remem­ber the disk address of a given page. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
The size of the swap space is important, as ultimately it deter­
mines the maximum number of memory pages that can be in use by 
asystem at agiven time. Letus assume for simplicity that itissimply 
very large for now. 
In the tiny example (Figure 20.1), you can see a little exampleof 
a4-page physical memory and an 8-page swap space. In the exam­
ple, three processes (Proc 0, Proc 1, and Proc 2) are actively sharing 
physical memory; each of the three, however, only have some oftheir 
valid pages in memory, with the rest located in swap space on disk. 
Afourth process (Proc 3) has all of its pages swapped out to disk, 
and thus clearly isn’t currently running. One block of swap remains 
free. Even from this tiny example, hopefully you can see how using 
swap space allows the system to pretend that memory is larger than 
it actually is. 
PFN 0 PFN 1 PFN 2 PFN 3 
Physical Memory 
Block 0 Block 1 Block 2 Block 3 Block 4 Block 5 Block 6 Block 7 
Proc 0  Proc 1  Proc 1  Proc 2  
[VPN 0]  [VPN 2]  [VPN 3]  [VPN 0]  

Swap Space 
Proc 0 [VPN 1]  Proc 0 [VPN 2]  [Free]  Proc 1 [VPN 0]  Proc 1 [VPN 1]  Proc 3 [VPN 0]  Proc 2 [VPN 1]  Proc 3 [VPN 1]  

Figure 20.1: Physical Memory and Swap Space 

20.2 The Present Bit 
Now that we have some space on the disk, we need to add some machinery higher up in the system in order to support swapping pages to and from the disk. Let us assume, for simplicity, thatwe have a system with a hardware-managed TLB. 
Recall .rst what happens on a memory reference. The running process generates virtual memory references (for instruction fetches, or data accesses), and, in this case, the hardware translatesthem into physical addresses before fetching the desired data from memory. 
Remember that the hardware .rst extracts the VPN from the vir­
tual address, checks the TLB for a match (a TLB hit), and if a hit, 
produces the resulting physical address and fetches it from memory. 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
This is hopefully the common case, as it is fast (requiring no addi­tional memory accesses). 
If the VPN is not found in the TLB (i.e., a TLB miss), the hardware locates the page table in memory (using the page table base register) and looks up the page table entry (PTE) for this page using the VPN as an index. If the page is valid and present in physical memory, the hardware extracts the PFN from the PTE, installs it in the TLB,and retries the instruction, this time generating a TLB hit; so far, so good. 
If we wish to allow pages to be swapped to disk, however, we must add even more machinery. Speci.cally, when the hardware looks in the PTE, it may .nd that the page is not present in physical memory. The way the hardware (or the OS, in a software-managed TLB approach) determines this is through a new piece of information in each page-table entry, known as the present bit.If the present bit is set to one, it means the page is present in physical memory and everything proceeds as above; if it is set to zero, the page is not in memory but rather on disk somewhere. The act of accessing a page that is not in physical memory is referred to as a page fault. 
Upon a page fault, the OS is invoked to service the page fault. A 
particular piece of code, known as a page-fault handler,runs, and 
must service the page fault, as we now describe. 

20.3 The Page Fault 
Recall that with TLB misses, we have two types of systems: hardware-
managed TLBs (where the hardware looks in the page table to .nd 
the desired translation) and software-managed TLBs (where the OS 
does). In either type of system, if a page is not present, the OSis 
put in charge to handle the page fault. The appropriately-named OS 
page-fault handler runs to determine what to do. Virtually all sys­
tems handle page faults in software; even with a hardware-managed 
TLB, the hardware trusts the OS to manage this important duty. 
If a page is not present and has been swapped to disk, the OS will need to swap the page into memory in order to service the page fault. Thus, a question arises: how will the OS know where to .nd the desired page? In many systems, the page table is a natural place to store such information. Thus, the OS could use the bits in the PTE normally used for data such as the PFN of the page for a disk address. When the OS receives a page fault for a page, it looks in the 
OPERATING SYSTEMS ARPACI-DUSSEAU ASIDE:WHY HARDWARE DOESN’T HANDLE PAGE FAULTS 
We know from our experience with the TLB that hardware design­
ers are loathe to trust the OS to do much of anything. So why do 
they trust the OS to handle a page fault? There are a few main rea­
sons. First, page faults to disk are slow;even if the OS takes a long 
time to handle a fault, executing tons of instructions, the disk opera­
tion itself is traditionally so slow that the extra overheadsof running 
software are minimal. Second, to be able to handle a page fault, the 
hardware would have to understand swap space, how to issue I/Os 
to the disk, and a lot of other details which it currently doesn’t know 
much about. Thus, for both reasons of performance and simplicity, 
the OS handles page faults, and even hardware types can be happy. 
PTE to .nd the address, and issues the request to disk to fetch the page into memory. 
When the disk I/O completes, the OS will then update the page table to mark the page as present, update the PFN .eld of the page-table entry (PTE) to record the in-memory location of the newly-fetched page, and retry the instruction. This next attempt may gen­erate a TLB miss, which would then be serviced and update the TLB with the translation (one could alternately update the TLB upon when servicing the page fault, to avoid this step). Finally, alast restart would .nd the translation in the TLB and thus proceed to fetch the desired data or instruction from memory at the translated physical address. 
Note that while the I/O is in .ight, the process will be in the blocked state. Thus, the OS will be free to run other ready processes while the page fault is being serviced. Because I/O is expensive, this overlap of the I/O (page fault) of one process and the execution of another is yet another way a multiprogrammed system can make the most effective use of its hardware. 

20.4 What If Memory Is Full? 
In the process described above, you may notice that we assumed 
there is plenty of free memory in which to page in apage from swap 
space. Of course, this may not be the case; memory may be full (or 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
close to it). Thus, the OS might like to .rst page out one or more pages to make room for the new page(s) the OS is about to bring in. The process of picking a page to kick out, or replace is known as the page-replacement policy. 
As it turns out, a lot of thought has been put into creating a good page-replacement policy, as kicking out the wrong page can exact agreat cost on program performance. Making the wrong decision can cause a program to run at disk-like speeds instead of memory-like speeds; in current technology that means a program couldrun 10,000 or 100,000 times slower. Thus, such a policy is something we should study in some detail; indeed, that is exactly what we will do in the next chapter. For now, it is good enough to understand that such a policy exists, built on top of the mechanisms describedhere. 

20.5 Page Fault Control Flow 
With all of this knowledge in place, we can now roughly sketch the complete control .ow of memory access. In other words, when somebody asks you “what happens when a program fetches some data from memory?”, you should have a pretty good idea of all the different possibilities. See the control .ow in Figures 20.2and 20.3 for more details; the .rst .gure shows what the hardware does during translation, and the second what the OS does upon a page fault. 
From the hardware control .ow diagram in Figure 20.2, notice that there are now three important cases to understand when a TLB miss occurs. First, that the page was both present and valid (Lines 18–21); in this case, the TLB miss handler can simply grab the PFN from the PTE, retry the instruction (this time resulting in a TLB hit), and thus continue as described (many times) before. In the second case (Lines 22–23), the page fault handler must be run; although this was a legitimate page for the process to access (it is valid, after all), it is not present in physical memory. Third (and .nally), the access could be to an invalid page, due for example to a bug in the program (Lines 13–14). In this case, no other bits in the PTE really matter; the hardware traps this invalid access, and the OS trap handler runs, likely terminating the offending process. 
From the software control .ow in Figure 20.3, we can see what 
the OS roughly must do in order to service the page fault. First, the 
OS must .nd a physical frame for the soon-to-be-faulted-in page to 
OPERATING SYSTEMS ARPACI-DUSSEAU 
1 VPN = (VirtualAddress & VPN_MASK) >> SHIFT 
2 (Success, TlbEntry) = TLB_Lookup(VPN) 
3 if (Success == True) // TLB Hit 
4 if (CanAccess(TlbEntry.ProtectBits) == True) 
5 Offset = VirtualAddress & OFFSET_MASK 
6 PhysAddr = (TlbEntry.PFN << SHIFT) | Offset 
7 Register = AccessMemory(PhysAddr) 
8 else 
9 RaiseException(PROTECTION_FAULT) 

10 else // TLB Miss 
11 PTEAddr = PTBR + (VPN * sizeof(PTE)) 
12 PTE = AccessMemory(PTEAddr) 
13 if (PTE.Valid == False) 
14 RaiseException(SEGMENTATION_FAULT) 
15 else 
16 if (CanAccess(PTE.ProtectBits) == False) 
17 RaiseException(PROTECTION_FAULT) 
18 else if (PTE.Present == True) 
19 // assuming hardware-managed TLB 
20 TLB_Insert(VPN, PTE.PFN, PTE.ProtectBits) 
21 RetryInstruction() 
22 else if (PTE.Present == False) 
23 RaiseException(PAGE_FAULT) 

Figure 20.2: Page-Fault Control Flow Algorithm (Hardware) 
reside within; if there is no such page, we’ll have to wait for the re­placement algorithm to run and kick some pages out of memory, thus freeing them for use here. With a physical frame in hand, the handler then issues the I/O request to read in the page from swap space.Fi­nally, when that slow operation completes, the OS updates thepage table and retries the instruction. The retry will result in a TLB miss, and then, upon another retry, a TLB hit, at which point the hardware will (.nally!) be able to access the desired memory item. 

20.6 When Replacements Really Occur 
Thus far, the way we’ve described how replacements occur as­sumes that the OS waits until memory is entirely full, and onlythen replaces (evicts) a page to make room for some other page. As you can imagine, this is a little bit unrealistic, and there are many reasons for the OS to keep a small portion of memory free more proactively. 
To keep a small amount of memory free, most operating systems thus have some kind of high watermark (HW )and low watermark (LW )to help decide when to start evicting pages from memory. How 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
1 PFN = FindFreePhysicalPage() 2 if (PFN == -1) // no free page found 3 PFN = EvictPage() // run replacement algorithm 4 DiskRead(PTE.DiskAddr, pfn) // sleep (waiting for I/O) 5 PTE.present = True // update page table with present 6 PTE.PFN = PFN // bit and translation (PFN) 7 RetryInstruction() // retry instruction 
Figure 20.3: Page-Fault Control Flow Algorithm (Software) 
this works is as follows: when the OS notices that there are fewer than LW pages available, a background thread that is responsible for freeing memory runs. The thread evicts pages until there are HW pages available. The background thread, sometimes called the swap 
daemon or page daemon1,then goes to sleep, happy that is has freed some memory for running processes and the OS to use. 
By performing a number of replacements at once, new perfor­mance optimizations become possible. For example, many systems will cluster or group anumber of pages and write them outatonce to the swap partition, thus increasing the ef.ciency of the disk[LL82]; as we will see later when we discuss disks in more detail, such clus­tering reduces seek and rotational overheads of a disk and thus in­creases performance noticeably. 
To work with the background paging thread, the control .ow in Figure 20.3 should be modi.ed slightly; instead of performing a re­placement directly, the algorithm would instead simply check if there are any free pages available. If not, it would signal that the back­ground paging thread that free pages are needed; when the thread frees up some pages, it would re-awaken the original thread, which could then page in the desired page and go about its work. 

20.7 Summary 
In this brief chapter, we have introduced the notion of accessing more memory than is physically present within a system. To do so requires more complexity in page-table structures, as a present bit must be included to tell us whether the page is present in memory or not. When not, the OS page-fault handler runs to service the page 
1The word “daemon”, usually pronounced “demon”, is an old termfor a back­ground thread or process that does something useful. Turns out (once again!) that the source of the term is Multics [CS94]. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
DESIGN TIP:DO WORK IN THE BACKGROUND When you have some work to do, it is often a good idea to do it in the background to increase ef.ciency and to allow for grouping of operations. Operating systems often do work in the background; for example, many systems buffer .le writes in memory before actually writing the data to disk. Doing so has many possible bene.ts: in­creased disk ef.ciency, as the disk may now receive many writes at once and thus better be able to schedule them; improved latency of writes, as the application thinks the writes completed quitequickly; the possibility of work reduction, as the writes may need never to go to disk (i.e., if the .le is deleted); and better use of idle time,as the background work may possibly be done when the system is other­wise idle, thus better utilizing the hardware [G+95]. 
fault,and thus arrange for the transfer of the desired page from disk to memory. 
Recall, importantly (and amazingly!), that these actions all take place transparently to the process. As far as it is concerned, it is just accessing its own private, contiguous virtual memory. Behind the scenes, pages are placed in arbitrary (non-contiguous) locations in physical memory, and sometimes they are not even present in mem­ory(!), requiring a fetch from disk. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

References 
[CS94] “Take Our Word For It” 
F. Corbato and R. Steinberg Available: http://www.takeourword.com/TOW146/page4.html 
Richard Steinberg writes: “Someone has asked me the origin ofthe word daemon as it applies to 
computing. Best I can tell based on my research, the word was .rst used by people on your team 
at Project MAC using the IBM 7094 in 1963.” Professor Corbato replies: “Our use of the word 
daemon was inspired by the Maxwell’s daemon of physics and thermodynamics (my background 
is in physics). Maxwell’s daemon was an imaginary agent whichhelped sort molecules of different 
speeds and worked tirelessly in the background. We fancifully began to use the word daemon to 
describe background processes which worked tirelessly to perform system chores.” 

[D97] “Before Memory Was Virtual” 
Peter Denning 
From In the Beginning: Recollections of Software Pioneers,Wiley, November 1997 
An excellent historical piece by one of the pioneers of virtual memory and working sets. 

[G+95] “Idleness is not sloth” Richard Golding, Peter Bosch, Carl Staelin, Tim Sullivan, John Wilkes USENIX ATC ’95, New Orleans, Louisiana 
Afun and easy-to-read discussion of how idle time can be better used in systems, with lots of good examples. 
[LL82] “Virtual Memory Management in the VAX/VMS Operating System” Hank Levy and P. Lipman IEEE Computer, Vol. 15, No. 3, March 1982 
Not the .rst place where such clustering was used, but a clear and simple explanation of how such a mechanism works. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
21 



Beyond Physical Memory: Policies 
In a virtual memory manager, life is easy when you have a lot of free memory. A page fault occurs, you .nd a free page on the free-page list, and assign it to the faulting page. Hey, Operating System, congratulations! You did it again. 
Unfortunately, things get a little more interesting when little mem­ory is free. In such a case, this memory pressure forces the OS to start paging out pages to make room for actively-used pages. Deciding which page (or pages) to evict is encapsulated within the replace­ment policy of the OS; historically, it was one of the most important decisions the early virtual memory systems made, as older systems had little physical memory. Minimally, it is an interesting set of poli­cies worth knowing a little more about. And thus our problem: 
THE CRUX:HOW TO DECIDE WHICH PAGE TO EVICT How can the OS decide which page (or pages) to evict from memory? This decision is made by the replacement policy of the system,which usually follows some general principles (discussed below) but also includes certain tweaks to avoid corner-case behaviors. 
255 
21.1 Cache Management 
Before diving into policies, we .rst describe the problem we are trying to solve in more detail. Given that main memory holds some subset of all the pages in the system, it can rightly be viewed as a cache for virtual memory pages in the system. Thus, our goal in picking a replacement policy for this cache is to minimize thenumber of cache misses;that is, to minimize the number of times that we have to go to disk to fetch the desired page. Alternately, one can view our goal as maximizing the number of cache hits,the number of times a page that is accessed is found in memory. 
Knowing the number of cache hits and misses let us calculate the average memory access time (AMAT)for a program (a metric com­puter architects compute for hardware caches [HP06]). Speci.cally, given these values, we can compute the AMAT of a program as fol­lows: (Hit% · TM )+(Miss% · TD ),where TM is the cost of accessing memory, and TD the cost of accessing disk. 
For example, let us imagine a machine with a (tiny) address space: 4KB, with 256-byte pages. Thus, a virtual address has two compo­nents: a 4-bit VPN (the most-signi.cant bits) and an 8-bit offset (the least-signi.cant bits). Thus, a process in this example can access 24 or 16 total virtual pages. In this example, the process generates the following memory references (i.e., virtual addresses): 0x000, 0x100, 0x200, 0x300, 0x400, 0x500, 0x600, 0x700, 0x800, 0x900. These virtual addresses refer to the .rst byte of each of the .rst ten pages ofthe ad­dress space (the page number being the .rst hex digit of each virtual address). 
Let us further assume that every page except virtual page 3 are already in memory. Thus, our sequence of memory references will encounter the following behavior: hit, hit, hit, miss, hit, hit, hit, hit, hit, hit. We can compute the hit rate (the percent of references found in memory): 90%, as 9 out of 10 references are in memory. The miss rate is obviously 10%. 
To calculate AMAT, we simply need to know the cost of access­ing memory and the cost of accessing disk. Assuming the cost of accessing memory (TM )is around 100 nanoseconds, and the cost of accessing disk (TD)is about 10 milliseconds, we have the following AMAT: 0.9 · 100ns +0.1 · 10ms,which is 90ns +1ms,or 1.00009 ms, or about 1 millisecond. If our hit rate had instead been 99.9%,the result is quite different: AMAT is 10.1 microseconds, or roughly 100 
OPERATING SYSTEMS ARPACI-DUSSEAU 
times faster. As the hit rate approaches 100%, AMAT approaches 100 nanoseconds. 
Unfortunately, as you can see in this example, the cost of disk access is so high in modern systems that even a tiny miss rate will quickly dominate the overall AMAT of running programs. Clearly, we need to avoid as many misses as possible or run slowly, at the rate of the disk. One way to help with this is to carefully develop a smart policy, as we now do. 

21.2 The Optimal Replacement Policy 
To better understand how a particular replacement policy works, it would be nice to compare it to the best possible replacementpol­icy. As it turns out, such an optimal policy was developed by Be-lady many years ago [B66] (he originally called it MIN). The opti­mal replacement policy leads to the fewest number of misses overall. Belady showed that a simple (but, unfortunately, dif.cult toimple­ment!) approach that replaces the page that will be accessed furthest in the future is the optimal policy, resulting in the fewest-possible cache misses. 
Hopefully, the intuition behind the optimal policy makes sense. Think about it like this: if you have to throw out some page, whynot throw out the one that is needed the furthest from now? By doing so, you are essentially saying that all the other pages in the cache are more important than the one furthest out. The reason this is true is simple: you will refer to the other pages before you refer to the one furthest out. 
Let’s trace through a simple example to understand the decisions the optimal policy makes. Assume a program accesses the follow­ing stream of virtual pages: 0, 1, 2, 0, 1, 3, 0, 3, 1, 2, 1. Table 21.1 shows what the optimal policy would do for this reference stream, assuming a cache that .ts only three pages. 
In the table, you can see the following actions. Not surprisingly, the .rst three accesses are misses, as the cache begins in an empty state; such a miss is sometimes referred to as a cold-start miss (or compulsory miss). Then we refer again to pages 0 and 1, which both hit in the cache. Finally, we reach another miss (to page 3), but this time the cache is full; a replacement must take place! Which begs the question: which page should we replace? With the optimal policy, 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
Resulting Access Hit/Miss? Evict Cache State 
0 Miss 0 1 Miss 0, 1 2 Miss 0, 1, 2 0 Hit 0,1,2 1 Hit 0,1,2 3 Miss 2 0,1,3 0 Hit 0,1,3 3 Hit 0,1,3 1 Hit 0,1,3 2 Miss 3 0,1,2 1 Hit 0,1,2 
Table 21.1: Tracing the Optimal Policy we examine the future for each page currently in the cache (0, 1, and 2), and see that 0 is accessed almost immediately, 1 is accessed a little later, and 2 is accessed furthest in the future. Thus the optimal policy has an easy choice: evict page 2, resulting in pages 0, 1, and 3 in the cache. The next three references are hits, but then we get to page 2, which we evicted long ago, and suffer another miss. Here the optimal policy again examines the future for each page in the cache (0, 1, and 3), and sees that as long as it doesn’t evict page 1 (which is about to be accessed), we’ll be OK. The example shows page 3 getting evicted, although 0 would have been a .ne choice too. Finally,we hit on page 1 and the trace completes. We can also calculate a hit rate for the cache given this stream. 
Hits 6
With 6 hits and 5 misses, the hit rate is which is or
Hits+Misses 6+5 
54.6%. You can also compute the hit rate modulo compulsory misses (i.e., ignore the .rst miss to any given page), resulting in a more im­pressive 85.7% hit rate. 
Unfortunately, as we saw before in the development of scheduling 
policies, the future is not generally known; you can’t build the opti­mal policy for a general-purpose operating system1.Thus, in devel­oping a real, deployable policy, we will have to focus on approaches that .nd some other way to decide which page to evict. The optimal policy will thus serve only as a comparison point, to know how close we are to “perfect”. 
1
If you can, let us know! We can become rich together. Or, like the scientists who “discovered” cold fusion, widely scorned and mocked. 
OPERATING SYSTEMS ARPACI-DUSSEAU ASIDE:TYPES OF CACHE MISSES 
In the computer architecture world, architects sometimes .nd it use­
ful to characterize misses by type, into one of three categories: com­
pulsory, capacity, and con.ict misses, sometimes called the Three 
C’s [H87]. A compulsory miss (or cold-start miss [EF78]) occurs be­
cause the cache is empty to begin with and this is the .rst reference 
to the item; in contrast, a capacity miss occurs because the cache ran 
out of space and had to evict an item to bring a new item into the 
cache. The third type of miss (a con.ict miss)arises in hardware be­
cause of limits on where an item can be placed in a hardware cache, 
due to something known as set-associativity;it doesnot arise in the 
OS page cache because such caches are always fully-associative,i.e., 
there are no restrictions on where in memory a page can be placed. 
See H&P for details [HP06]. 

21.3 A Simple Policy: FIFO 
Many early systems avoided the complexity of trying to approach optimal and employed very simple replacement policies. For exam­ple, some systems used FIFO (.rst-in, .rst-out) replacement, where pages were simply placed in a queue when they enter the system; when a replacement occurs, the page on the tail of the queue (the “.rst-in” page) is evicted. FIFO has one great strength: it isquite simple to implement. 
Let’s examine how FIFO does on our example reference stream from above. Table 21.2 shows the results. We again begin our trace with three compulsory misses to pages 0, 1, and 2, and then hit on both 0 and 1. Next, page 3 is referenced, causing a miss; the replace­ment decision is easy with FIFO: pick the page that was the .rst-one in (the cache state in the table is kept in FIFO order, with the .rst-in page on the left), which is page 0. Unfortunately, our next access is to page 0(!), thus causing another miss, and another replacement (of page 1). We then hit on page 3, but miss on 1 and 2, and .nally hit on 3 to .nish. 
Comparing FIFO to optimal, FIFO does notably worse: a 36.4% hit rate (or 57.1% excluding compulsory misses). FIFO simplycan’t determine the importance of blocks: even though page 0 had been accessed a number of times, FIFO still kicks it out, simply because it was the .rst one brought into memory. 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
Resulting Access Hit/Miss? Evict Cache State 
0 Miss First-in. 0 1 Miss First-in. 0, 1 2 Miss First-in. 0, 1, 2 0 Hit First-in. 0, 1, 2 1 Hit First-in. 0, 1, 2 3 Miss 0 First-in. 1, 2, 3 0 Miss 1 First-in. 2, 3, 0 3 Hit First-in. 2, 3, 0 1 Miss 2 First-in. 3, 0, 1 2 Miss 3 First-in. 0, 1, 2 1 Hit First-in. 0, 1, 2 
Table 21.2: Tracing the FIFO Policy 

21.4 Another Simple Policy: Random 
Another similar replacement policy is Random, which simply picks 
arandom page to replace under memory pressure. Random has 
properties similar to FIFO; it is simple to implement, but it doesn’t 
really try to be too intelligent in picking which blocks to evict. Let’s 
look at how Random does on our famous example reference stream 
(see Table 21.3). 
Resulting Access Hit/Miss? Evict Cache State 
0 Miss 0 1 Miss 0, 1 2 Miss 0, 1, 2 0 Hit 0,1,2 1 Hit 0,1,2 3 Miss 0 1,2,3 0 Miss 1 2,3,0 3 Hit 2,3,0 1 Miss 3 2,0,1 2 Hit 2,0,1 1 Hit 2,0,1 
Table 21.3: Tracing the Random Policy 
Of course, how Random does depends entirely upon how lucky (or unlucky) Random gets in its choices. In the example above,Ran­dom does a little better than FIFO, and a little worse than optimal. In fact, we can run the Random experiment thousands of times and determine how it does in general. Figure 21.1 shows how many hits Random achieves over 10,000 trials, each with a different random seed. As you can see, sometimes (just over 40% of the time), Random 
OPERATING SYSTEMS ARPACI-DUSSEAU 
is as good as optimal, achieving 6 hits on the example trace; some­times it does much worse, achieving 2 hits or fewer. How Random does depends on the luck of the draw. 
Frequency 
50 40 30 20 10 0 

Number of Hits 
Figure 21.1: Random Performance over 10,000 Trials 
ASIDE:TYPES OF LOCALITY There are two types of locality that programs tend to exhibit.The .rst is known as spatial locality,which statesthat if a page P is accessed, it is likely the pages around it (say P - 1or P +1)will also likely be accessed. The second is temporal locality,which statesthat pages that have been accessed in the near past are likely to be accessed again in the near future. The assumption of the presence of these types of locality plays a large role in the caching hierarchies of hard­ware systems, which deploy many levels of instruction, data,and address-translation caching to help programs run fast when such lo­cality exists. 
Of course, the principle of locality,asit isoften called,isno hard­and-fast rule that all programs must obey. Indeed, some programs access memory (or disk) in rather random fashion and don’t exhibit much or any locality in their access streams. Thus, while locality is agood thing to keep in mind while designing caches of any kind (hardware or software), it does not guarantee success. Rather, it is a heuristic that often proves useful in the design of computer systems. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

21.5 Using History: LRU 
Unfortunately, any policy as simple as FIFO or Random is likely to have a common problem: it might kick out an important page, one that is about to be referenced again. FIFO kicks out the page that was .rst brought in; if this happens to be a page with importantcode or data structures upon it, it gets thrown out anyhow, even though it will soon be paged back in. Thus, FIFO, Random, and similar policies are not likely to approach optimal; something smarter is needed. 
As we did with scheduling policy, to improve our guess at the future, we once again lean on the past and use history as our guide. For example, if a program has accessed a page in the near past, it is likely to access it again in the near future. 
Resulting Access Hit/Miss? Evict Cache State 
0 Miss LRU. 0 1 Miss LRU. 0, 1 2 Miss LRU. 0, 1, 2 0 Hit LRU. 1, 2, 0 1 Hit LRU. 2, 0, 1 3 Miss 2 LRU. 0, 1, 3 0 Hit LRU. 1, 3, 0 3 Hit LRU. 1, 0, 3 1 Hit LRU. 0, 3, 1 2 Miss 0 LRU. 3, 1, 2 1 Hit LRU. 3, 2, 1 
Table 21.4: Tracing the LRU Policy 
One type of historical information a page-replacement policy could 
use is frequency;if a page has been accessedmany times,perhapsit 
should not be replaced as it clearly has some value. An even more 
commonly-used property of a page is its recency of access; the more 
recently a page has been accessed, perhaps the more likely it will be 
accessed again. 
This family of policies is based on what people refer to as the prin­ciple of locality [D70], which basically is just an observation about programs and their behavior. What this principle says, quitesim­ply, is that programs tend to access certain code sequences and data structures quite frequently; we should thus try to use history to .g­ure out which pages are important, and keep those pages in memory when it comes to eviction time. 
And thus, a family of simple historically-based algorithms are born. The Least-Frequently-Used (LFU)policy replaces the least-
OPERATING SYSTEMS ARPACI-DUSSEAU 
ASIDE:BELADY’S ANOMALY Some year’sago, Belady (of the optimal policy) and colleagues found an interesting reference stream that behaved a little unexpectedly [BNS69]. The memory-reference stream: 1, 2, 3, 4, 1, 2, 5, 1, 2,3,4, 5. The replacement policy they were studying was FIFO. And now, the interesting part: how the cache hit rate changed when moving from acache size of 3to 4pages. 
In general, you would expect the cache hit rate to increase (get better) when the cache gets larger, right? But in this case, when running FIFO, it turns out that with a cache of size 3, 3 hits (9 misses) take place, but with a larger cache of size 4, only 2 hits (10 misses)occur. If you don’t believe it, calculate the hits and misses yourself! This odd behavior is generally referred to as Belady’s Anomaly (to the chagrin of his co-authors). 
Some other policies, such as LRU, don’t suffer from this problem. Can you guess why? As it turns out, LRU (and some other policies) have what is known as a stack property [M+70]. For algorithms that have the stack property, a cache of size N +1naturally includes the contents of a cache of size N which uses the same replacement al­gorithm. Thus, when increasing the cache size, you will neversee a decrease in hit rate – only an increase or at worst the same hit rate. FIFO and Random (among others) clearly do not have a stack prop­erty, and thus are susceptible to anomalous behavior. 
frequently-used page when an eviction must take place. Similarly, the Least-Recently-Used (LRU)policy replaces the least-recently­used page. These algorithms are easy to remember: once you know the name, you know exactly what it does. 
To better understand this, let’s examine how LRU does on our example reference stream. Table 21.4 shows the results. Fromthe ta­ble, you can see how LRU can use history to do better than stateless policies such as Random or FIFO. In the example, LRU evicts page 2when it .rsthas to replace apage, because 0 and 1 have been ac­cessed more recently. It then replaces page 0 because 1 and 3 have been accessed more recently. In both cases, LRU’s decision, based on history, turns out to be correct, and the next references are thus hits. Thus, in our simple example, LRU does as well as possible, matching optimal in its performance. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
We should also note that the opposites of these algorithms ex­ist: Most-Frequently-Used (MFU)and Most-Recently-Used (MRU). However, in most cases (though not all!), these policies do not work well, as they ignore the locality most programs exhibit instead of em­bracing it. 

21.6 Workload Examples 
Let’s look at a few more examples in order to better understand 
how some of these policies behave. We’ll look at more complex 
workloads instead just a small trace of references. 
Our .rst workload has no locality, which means that each refer­ence is to a random page within the set of accessed pages. In this simple example, the workload accesses 100 unique pages over time, choosing the next page to refer to at random; overall, 10,000 pages are accessed. In the experiment, we vary the cache size from very small (1 page) to enough to hold all the unique pages (100 page), in order to see how each policy behaves over the range of cache sizes. 
Figure 21.2 plots the results of the experiment for the optimal, LRU, Random, and FIFO policies. The y-axis of the .gure shows the hit rate that each policy achieves; the x-axis varies the cache size as described above. 
We can draw a number of conclusions from the graph. First, when there is no locality in the workload, it doesn’t matter much which re­alistic policy you are using; LRU, FIFO, and Random all perform the same, with the hit rate exactly determined by the size of the cache. Second, when the cache is large enough to .t the entire workload, it also doesn’t matter which policy you use; all policies (even opti­mal) converge to a 100% hit rate when all the referenced blocks.t in cache. Finally, you can see that optimal performs noticeablybetter than the realistic policies; peeking into the future, if it were possible, does a much better job of replacement. 
The next workload we examine is called the “80-20” workload, because it has locality within it. Speci.cally, 80% of the references are made to 20% of the pages (you might call these pages “hot”); the remaining 20% of the references are made to the remaining 80% of the pages (perhaps these are the “cold” pages). In our workload, there are a total 100 unique pages again; thus, “hot” pages (0–19) are referred to most of the time, and “cold” pages (20-99) the remain-
OPERATING SYSTEMS ARPACI-DUSSEAU 
Hit Rate 
The No-Locality Workload The 80-20 Workload 
100% 

100% 
80% 
80% 

60% 60% 
40% 40% 
20% 20% 
0% 
0% 
The Looping-Sequential Workload 

0  20 40 60 80 Cache Size (Blocks)  100  
Figure 21.2: The No-Locality, 80-20, and Looping Workloads  
der. Figure 21.2 also shows how the policies perform with the 80-20 workload. As you can see from the .gure, while both random and FIFO do reasonably well, LRU does better, as it is more likely to hold onto the hot pages; as those pages have been referred to frequentlyin the past, they are likely to be referred to again in the near future. Optimal once again does better, showing that LRU’s historical information is not perfect. You might now be wondering: is LRU’s improvement over Ran­dom and FIFO really that big of a deal? The answer, as usual, is “it depends.” If each miss is very costly (not uncommon), then even a  
ARPACI-DUSSEAU  THREE EASY PIECES (V0.5)  

small increase in hit rate (reduction in miss rate) can make a huge difference on performance. If misses are not so costly, then of course the bene.ts possible with LRU are not nearly as important. 
Let’s look at one .nal workload. We call this one the “looping se­quential” workload, as in it, we refer to 50 pages in sequence,start­ing at 0, then 1, ..., up to page 49, and then we loop, repeating those accesses, for a total of 10,000 accesses to 50 unique pages. The last graph in Figure 21.2 shows the behavior of the policies under this workload. 
This workload, common in many applications (including impor­
tant commercial applications such as databases [CD85]), represents a 
worst-case for both LRU and FIFO. These algorithms, under a looping-
sequential workload, kick out older pages; unfortunately, due to the 
looping nature of the workload, these older pages are going tobe ac­
cessed sooner than the pages that the policies prefer to keep in cache. 
Indeed, even with a cache of size 49, a looping-sequential workload 
of 50 pages results in a 0% hit rate. Interestingly, Random fares no­
tably better, not quite approaching optimal, but at least achieving a 
non-zero hit rate. 
DESIGN TIP:COMPARING AGAINST OPTIMAL IS USEFUL 
Although optimal is not very practical as a real policy, it is incredibly 
useful as a comparison point in simulation or other studies. Saying 
that your fancy new algorithm has a 80% hit rate isn’t meaningful 
in isolation; saying that optimal achieves an 82% hit rate (and thus 
your new approach is quite close to optimal) makes the result more 
meaningful and gives it context. Thus, in any study you perform, 
knowing what the optimal is lets you perform a better comparison, 
showing how much improvement is still possible, and also whenyou 
can stop making your policy better, because it is close enough to the 
ideal [AD03]. 

21.7 Implementing Historical Algorithms 
As you can see, an algorithm such as LRU can generally do a bet­ter job than simpler policies like FIFO or Random, which may throw out important pages. Unfortunately, historical policies present us with a new challenge: how do we implement them? 
OPERATING SYSTEMS ARPACI-DUSSEAU 
Let’s take, for example, LRU. To implement it perfectly, we need to do a lot of work. Speci.cally, upon each page access (i.e., each mem­ory access, whether an instruction fetch or a load or store), we must update some data structure to move this page to the front of thelist (i.e., the MRU side). Contrast this to FIFO, where the FIFO list of pages is only accessed when a page is evicted (by removing the .rst-in page) or when a new page is added to the list (to the last-in side). To keep track of which pages have been least-and most-recently used, the system has to do some accounting work on every memory reference. Clearly, without great care, such accounting could greatly reduce performance. 
One method that could help speed this up is to add a little bit of hardware support. For example, a machine could update, on each page access, a time .eld in memory (for example, this could be in the per-process page table, or just in some separate array in memory, with one entry per physical page of the system). Thus, when a page is accessed, the time .eld would be set, by hardware, to the current time. Then, when replacing a page, the OS could simply scan allthe time .elds in the system to .nd the least-recently-used page. 
Unfortunately, as the number of pages in a system grows, scan­ning a huge array of times just to .nd the absolute least-recently­used page is prohibitively expensive. Imagine a modern machine with 4GB of memory, chopped into 4KB pages. This machine has 1 million pages, and thus .nding the LRU page will take a long time, even at modern CPU speeds. Which begs the question: do we re­ally need to .nd the absolute oldest page to replace? Can we instead survive with an approximation? 
CRUX:HOW TO IMPLEMENT AN LRU REPLACEMENT POLICY Given that it will be expensive to implement perfect LRU, can we 
approximate it in some way, and still obtain the desired behavior? 

21.8 Approximating LRU 
As it turns out, the answer is yes: approximating LRU is more 
feasible from a computational-overhead standpoint, and indeed it 
is what many modern systems do. The idea requires some hardware 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
The 80-20 Workload 
100% 
80% 
60% 
40% 
20% 
0% 
Cache Size (Blocks) 
Figure 21.3: The 80-20 Workload with Clock 
support, in the form of a use bit (sometimes called the reference bit), the .rst of which was implemented in the .rst system with paging, the Atlas one-level store [KE+62]. There is one use bit per page of the system, and the use bits live in memory somewhere (they could be in the per-process page tables, for example, or just in an array somewhere). Whenever a page is referenced (i.e., read or written), the use bit is set by hardware to 1. The hardware never clears the bit, though (i.e., sets it to 0); that is the responsibility of the OS. 
How does the OS employ the use bit to approximate LRU? Well, there could be a lot of ways, but with the clock algorithm [C69], one simple approach was suggested. Imagine all the pages of the system arranged in a circular list. A clock hand points to some particular page to begin with (it doesn’t really matter which). When a replace­ment must occur, the OS checks if the currently-pointed to page P has a use bit of 1 or 0. If 1, this implies that page P was recently used and thus is not agood candidate for replacement. Thus, the clock hand is incremented to the next page P +1,and the use bit for P set to 0 (cleared). The algorithm continues until it .nds a usebit that is set to 0, implying this page has not been recently used (or, in the worst case, that all pages have been and that we have now searched through the entire set of pages, clearing all the bits). 
Note that this approach is not the only way to employ a use bit to approximate LRU. Indeed, any approach which periodically clears the use bits and then differentiates between which pages haveuse 
Hit Rate 

OPERATING SYSTEMS ARPACI-DUSSEAU 
bits of 1 versus 0 to decide which to replace would be .ne. The clock algorithm of Corbato’s was just one early approach which met with some success, and had the nice property of not repeatedly scanning through all of memory looking for an unused page. 
The behavior of a clock algorithm variant is shown in Figure 21.3. This variant randomly scans pages when doing a replacement; when it encounters a page with a reference bit set to 1, it clears thebit (i.e., sets it to 0); when it .nds a page with the reference bit set to 0,it chooses it as its victim. As you can see, although it doesn’t doquite as well as perfect LRU, it does better than approaches that don’t con­sider history at all. 

21.9 Considering Dirty Pages 
One small modi.cation to the clock algorithm (also originally sug­gested by Corbato [C69]) that is commonly made is the additional consideration of whether a page has been modi.ed or not while in memory. The reason for this: if a page has been modi.ed and is thus dirty,it must be written back to disk to evict it, which isexpensive. If it has not been modi.ed (and is thus clean), the eviction is free; the physical frame can simply be reused for other purposes without additional I/O. Thus, some VM systems prefer to evict clean pages over dirty pages. 
To support this behavior, the hardware should include a modi.ed bit (a.k.a. dirty bit). This bit is set any time a page is written, and thus can be incorporated into the page-replacement algorithm. The clock algorithm, for example, could be changed to scan for pages that are both unused and clean to evict .rst; failing to .nd those, then for unused pages that are dirty; etc. 

21.10 Other VM Policies 
Page replacement is not the only policy the VM subsystem em­ploys (though it may be the most important). For example, the OS also has to decide when to bring a page into memory. This policy, sometimes called the page selection policy [D70], presents the OS with some different options. 
For most pages, the OS simply uses demand paging,which means the OS brings the page into memory when it is accessed, “on de-
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
mand” as it were. Of course, the OS could guess that a page is about to be used, and thus bring it in ahead of time; this behavior is known as prefetching and should only be done when there is reasonable chance of success. For example, some systems will assume thatif a code page P is brought into memory, that code page P +1 will likely soon be accessed and thus should be brought into memory too. 
Another policy determines how the OS writes pages out to disk. Of course, they could simply be written out one at a time; however, many systems instead collect a number of pending writes together in memory and write them to disk in one (more ef.cient) write. This behavior is usually called clustering or simply grouping of writes, and is effective because of the nature of disk drives, which perform asingle, large write more ef.ciently than many small writes (as we will see). 

21.11 Thrashing 
Before closing, we address one .nal question: what should the OS do when memory is simply oversubscribed, and the memory de­mands of the set of running processes simply exceeds the available physical memory? In this case, the system will constantly be paging, acondition sometimes referred to as thrashing [D70]. 
Some earlier operating systems had a fairly sophisticated set of mechanisms to both detect and cope with thrashing when it took place. For example, given a set of processes, a system could decide not to run a subset of processes, with the hope that the reducedset of processes working sets (the pages that they are using actively) .t in memory and thus can make progress. This approach, generally known as admission control,statesthat it issometimes better to do less work well than to try to do everything at once and make little or no progress in all directions, a situation we often encounter in real life as well. 
More modern systems sometimes take more a draconian approach. 
For example, some versions of Linux run an “out-of-memory killer” 
when memory is oversubscribed; this daemon picks a random pro­
cess and kills it, thus reducing memory in a not-too-subtle manner. 
While successful at reducing memory pressure, this approachhas its 
problem, if, for example, it kills the X server and thus renders any 
applications that use the display unusable. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

21.12 Summary 
We have seen the introduction of a number of page-replacement 
(and other) policies, which are part of the VM subsystem of allmod­
ern operating systems. Modern systems add some tweaks to straight­
forward LRU approximations like clock; for example, scan resis­
tance is an important part of many modern algorithms, such as ARC 
[MM03]. Scan-resistant algorithms are usually LRU-like butalso try 
to avoid the worst-case behavior of LRU, which we saw with the 
looping-sequential workload. Thus, the evolution of page-replacement 
algorithms continues. 
However, in many cases the importance of said algorithms has de­creased, as the discrepancy between memory-access and disk-access times has increased. Because paging to disk is so expensive, the cost of frequent paging is prohibitive. Thus, the best solution toexces­sive paging is often a simple (if intellectually dissatisfying) one: buy more memory. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

References 
[AD03] “Run-Time Adaptation in River” 
Remzi H. Arpaci-Dusseau 
ACM TOCS, 21:1, February 2003 

Asummary of one of the authors’ dissertation work on a system named River. Certainly one place where he learned that comparison against the ideal is animportant technique for system designers. 
[B66] “A Study of Replacement Algorithms for Virtual-Storage Computer” Laszlo A. Belady IBM Systems Journal 5(2): 78-101, 1966 
The paper that introduces the simple way to compute the optimal behavior of a policy (the MIN algorithm). 
[BNS69] “An anomaly in space-time characteristics of certain programs running in a paging machine” 
L. A. Belady and R. A. Nelson and G. S. Shedler 
Communications of the ACM, 12:6, June 1969 

Introduction of the little sequence of memory references known as Belady’s Anomaly. How do Nelson and Shedler feel about this name, we wonder? 
[CD85] “An evaluation of buffer management strategies for relational database systems” Hong-Tai Chou and David J. DeWitt VLDB ’85, Stockholm, Sweden, August 1985 
Afamous database paper on the different buffering strategies you should use under a number of common database access patterns. 
[C69] “A paging experiment with the Multics system” 
F.J. Corbato Included in a Festschrift published in honor of Prof. P.M. Morse MIT Press, Cambridge, MA, 1969 
The original (and hard to .nd!) reference to the clock algorithm, though not the .rst usage of a use bit. Thanks to H. Balakrishnan of MIT for digging up this paper for us. 
[D70] “Virtual Memory” Peter J. Denning Computing Surveys, Vol. 2, No. 3, September 1970 
Denning’s early and famous survey on virtual memory systems. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
[EF78] “Cold-start vs. Warm-start Miss Ratios” Malcolm C. Easton and Ronald Fagin Communications of the ACM, 21:10, October 1978 
Agood discussion of cold-start vs. warm-start misses. 
[HP06] “Computer Architecture: A Quantitative Approach” John Hennessy and David Patterson Morgan-Kaufmann, 2006 
Agreat and marvelous book about computer architecture. Readit! 
[H87] “Aspects of Cache Memory and Instruction Buffer Performance” Mark D. Hill Ph.D. Dissertation, U.C. Berkeley, 1987 
Mark Hill, in his dissertation work, introduced the Three C’s, which later gained wide popularity with its inclusion in H&P [HP06]. The quote from therein: “I have found it useful to partition misses ... into three components intuitively based on the cause of the misses (page 49).” 
[KE+62] “One-level Storage System” 
T. Kilburn, and D.B.G. Edwards and M.J. Lanigan and F.H. Sumner IRE Trans. EC-11:2, 1962 
Although Atlas had a use bit, it only had a very small number of pages, and thus the scanning of the use bits in large memories was not a problem the authors solved. 
[M+70] “Evaluation techniques for storage hierarchies” 
R. L. Mattson, J. Gecsei, D. R. Slutz, I. L. Traiger IBM Systems Journal, Volume 9:2, 1970 
Apaper that is mostly about how to simulate cache hierarchiesef.ciently; certainly a classic in that regard, as well for its excellent discussion of some of the properties of various replacement algorithms. Can you .gure out why the stack property might be useful for simulating a lot of different-sized caches at once? 
[MM03] “ARC: A Self-Tuning, Low Overhead Replacement Cache” Nimrod Megiddo and Dharmendra S. Modha FAST 2003, February 2003, San Jose, California 
An excellent modern paper about replacement algorithms, which includes a new policy, ARC, that is now used in some systems. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

Homework 
This simulator, paging-policy.py,allows you to play around with different page-replacement policies. For example, let’s examine how LRU performs with a series of page references with a cache of size 3: 
01201303121 
To do so, run the simulator as follows: 
prompt> ./paging-policy.py --addresses=0,1,2,0,1,3,0,3,1,2,1--policy=LRU --cachesize=3 -c 
And what you would see is: 
ARG addresses 0,1,2,0,1,3,0,3,1,2,1 
ARG numaddrs 10 
ARG policy LRU
ARG cachesize 3 
ARG maxpage 10
ARG seed 0 

Solving... 
Access: 0 MISS LRU-> [0]<-MRU Replace:-[Hits:0 Misses:1] Access: 1 MISS LRU-> [0, 1]<-MRU Replace:-[Hits:0 Misses:2]Access: 2 MISS LRU->[0, 1, 2]<-MRU Replace:-[Hits:0 Misses:3]Access: 0 HIT LRU->[1, 2, 0]<-MRU Replace:-[Hits:1 Misses:3] Access: 1 HIT LRU->[2, 0, 1]<-MRU Replace:-[Hits:2 Misses:3]Access: 3 MISS LRU->[0, 1, 3]<-MRU Replace:2 [Hits:2 Misses:4]Access: 0 HIT LRU->[1, 3, 0]<-MRU Replace:2 [Hits:3 Misses:4] Access: 3 HIT LRU->[1, 0, 3]<-MRU Replace:2 [Hits:4 Misses:4] Access: 1 HIT LRU->[0, 3, 1]<-MRU Replace:2 [Hits:5 Misses:4]Access: 2 MISS LRU->[3, 1, 2]<-MRU Replace:0 [Hits:5 Misses:5]Access: 1 HIT LRU->[3, 2, 1]<-MRU Replace:0 [Hits:6 Misses:5] 
The complete set of possible arguments for paging-policy is listed on the following page, and includes a number of options for varying the policy, how addresses are speci.ed/generated, and otherimpor­tant parameters such as the size of the cache. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
prompt> ./paging-policy.py --helpUsage: paging-policy.py [options] 
Options:-h, --help show this help message and exit -a ADDRESSES, --addresses=ADDRESSES 
aset ofcomma-separated pages toaccess; -1 means randomly generate-f ADDRESSFILE, --addressfile=ADDRESSFILE afile witha bunchof addresses in it 
-n NUMADDRS, --numaddrs=NUMADDRS if -a (--addresses) is -1, this is thenumber of addrs to generate
-p POLICY, --policy=POLICYreplacement policy: FIFO, LRU, LFU, OPT,UNOPT, RAND, CLOCK -b CLOCKBITS, --clockbits=CLOCKBITS for CLOCK policy, how many clock bits to use -C CACHESIZE, --cachesize=CACHESIZE size of the page cache, in pages 
-m MAXPAGE, --maxpage=MAXPAGEif randomly generating page accesses,this is the max page number 
-s SEED, --seed=SEED random number seed -N, --notrace do not print out a detailed trace -c, --compute compute answers for me 
As usual, -c is used to solve a particular problem, whereas with­out it, the accesses are just listed (and the program does not tell you whether or not a particular access is a hit or miss). 
To generate a random problem, instead of using -a/--addresses to pass in some page references, you can instead pass in -n/--numaddrs as the number of addresses the program should randomly generate, with -s/--seed used to specify a different random seed. For exam­ple: 
prompt> ./paging-policy.py -s 10 -n 3 
.. . 
Assuming a replacement policy of FIFO, and a cache of size 3 pages, 
figure out whether each of the following page references hit or miss 
in the page cache. 

Access: 5 Hit/Miss? State of Memory? 
Access: 4 Hit/Miss? State of Memory? 
Access: 5 Hit/Miss? State of Memory? 

As you can see, in this example, we specify -n 3 which means the program should generate 3 random page references, which it does: 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
5, 7, and 5. The random seed is also speci.ed (10), which is what gets us those particular numbers. After working this out yourself, have the program solve the problem for you by passing in the same arguments but with -c (showing just the relevant part here): 
prompt> ./paging-policy.py -s 10 -n 3 -c 
... 
Solving... 
Access: 5 MISS FirstIn-> [5] <-Lastin Replace:-[Hits:0 Misses:1]Access: 4 MISS FirstIn->[5, 4] <-Lastin Replace:-[Hits:0 Misses:2]Access: 5 HIT FirstIn->[5, 4] <-Lastin Replace:-[Hits:1 Misses:2] 
The default policy is FIFO, though others are available, includ­ing LRU, MRU, OPT (the optimal replacement policy, which peeks into the future to see what is best to replace), UNOPT (which isthe pessimal replacement), RAND (which does random replacement), and CLOCK (which does the clock algorithm). The CLOCK algo­rithm also takes another argument (-b), which states how many bits should be kept per page; the more clock bits there are, the better the algorithm should be at determining which pages to keep in memory. 
Other options include: -C/--cachesize which changes the size of the page cache; -m/--maxpage which is the largest page number that will be used if the simulator is generating references for you; and-f/--addressfile which lets you specify a .le with addresses in them, in case you wish to get traces from a real application or otherwise use a long trace as input. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
Questions 
• 	
Generate random addresses with the following arguments: -s 0-n 10, -s 1 -n 10,and -s2 -n 10.Change the policy from FIFO, to LRU, to OPT. Compute whether each access in said address traces are hits or misses. 

• 	
For a cache of size 5, generate worst-case address reference streams for each of the following policies: FIFO, LRU, and MRU (worst-case reference streams cause the most misses possible. For the worst case reference streams, how much bigger of a cache is needed to improve performance dramatically and ap­proach OPT? 

• 	
Generate a random trace (use python or perl). How would you expect the different policies to perform on such a trace? 

• 	
Now generate a trace with some locality. How can you gener­ate such a trace? How does LRU perform on it? How much better than RAND is LRU? How does CLOCK do? How about CLOCK with different numbers of clock bits? 

• 	
Use a program like valgrind to instrument a real applica­tion and generate a virtual page reference stream. For example, running valgrind --tool=lackey --trace-mem=yes ls will output a nearly-complete reference trace of every instruc­tion and data reference made by the program ls.To make this useful for the simulator above, you’ll have to .rst transform each virtual memory reference into a virtual page-number ref­erence (done by masking off the offset and shifting the result­ing bits downward). How big of a cache is needed for your application trace in order to satisfy a large fraction of requests? Plot a graph of its working set as the size of the cache increases. 


THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
22 




Case Study: The VAX/VMS VirtualMemory System 
Before we end our study of VM systems, let us take a closer look at one particularly clean and well done virtual memory manager, that found in the VAX/VMS operating system [LL82]. In this note, we will discuss the system to illustrate how some of the concepts brought forth in earlier notes come together in a complete memory manager. 
22.1 Background 
The VAX-11 minicomputer architecture was introduced in the late 1970’s by Digital Equipment Corporation (DEC) 1.The architecture was realized in a number of implementations, including the VAX­11/780 and the lesser-powered VAX-11/750. The OS for the system was known as VAX/VMS (or just plain VMS), one of whose primary architects was Dave Cutler, who later led the effort to develop Microsoft’s Windows NT [C93]. VMS had the general problem that it would be run on a broad range of ma­chines, including very inexpensive VAXen (yes, that is the proper plural) to extremely high-end and powerful machines in the same ar­
1
DEC, for a time, was a massive player in the computer industry during the era of the mini-computer, employing well over 100,000 workers. A series of bad decisions and the advent of the PC slowly (but surely) led to their demise. Read Christensen’s “The Innovator’s Dilemma” [C03] for more information on how technology disruption can topple a large company. 
279 chitecture family. Thus, the OS had to have mechanisms and policies that worked (and worked well) across this huge range of systems. 
THE CRUX:HOW TO AVOID THE CURSE OF GENERALITY 
Operating systems often have a problem known as “the curse of generality”, where they are tasked with general support for abroad class of applications and systems. The fundamental result ofthe curse is that the OS is not likely to support any one installation very well. In the case of VMS, the curse was very real, as the VAX-11 architecture was realized in a number of different implementations. Thus, how can an OS be built so as to run effectively on a wide range of systems? 
As an additional issue, VMS is an excellent example of software innovations used to hide some of the inherent .aws of the architec­ture. Although the OS often relies on the hardware to build ef.cient abstractions and illusions, sometimes the hardware designers don’t quite get everything right; in the VAX hardware, we’ll see a few ex­amples of this, and what the VMS operating system does to buildan effective, working system despite these hardware .aws. 

22.2 Memory Management Hardware 
The VAX-11 provided a 32-bit virtual address space per process, divided into 512-byte pages. Thus, a virtual address consisted of a 23-bit VPN and a 9-bit offset. Further, the upper two bits of the VPN were used to differentiate which segment the page resided within; thus, the system was a hybrid of paging and segmentation, as we saw in a previous chapter. 
The lower-half of the address space was known as “process space” 
and is unique to each process. In the .rst half of process space(known 
as P0), the user program is found, as well as a heap which grows 
downward. In the second half of process space (P1), we .nd the 
stack, which grows upwards. The upper-half of the address space is 
known as system space (S), although only half of it is used. Protected 
OS code and data reside here, and the OS is in this way shared across 
processes. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
0 
Page 0: Invalid Trap Tables User Stack User Code  
User Heap  


Kernel Data Kernel Code  
Kernel Heap  

Unused  

User (P0) 
230 
User (P1) 
231 
System (S) 
232 
Figure 22.1: The VAX/VMS Address Space 
One major concern of the VMS designers was the incredibly small size of pages in the VAX hardware (512 bytes). This size, chosen for historical reasons, has the fundamental problem of making simple linear page tables excessively large. Thus, one of the .rst goals of the VMS designers was to make sure that VMS would not overwhelm memory with page tables. 
The system reduced the pressure page tables place on memory in two ways. First, by segmenting the user address space into two, the VAX-11 provides a page table for each of these regions (P0 and P1) per process; thus, no page-table space is needed for the unused por­tion of the address space between the stack and the heap. The base 
ARPACI-DUSSEAU 



THREE EASY PIECES (V0.5) 
and bounds registers are used as you would expect; a base register holds the address of the page table for that segment, and the bounds holds its size (i.e., number of page-table entries). 
Second, the OS reduces memory pressure even further by plac­ing user page tables (for P0 and P1,thus two per process) in kernel virtual memory. Thus, when allocating or growing a page table, the kernel allocates space out of its own virtual memory, in segment S. If memory comes under severe pressure, the kernel can swap pages of these page tables out to disk, thus making physical memory avail­able for other uses. 
Putting page tables in kernel virtual memory means that address translation is even further complicated. For example, to translate a virtual address in P0 or P1,the hardware hasto .rst try to look up the page-table entry for that page in its page table (the P0 or P1 page table for that process); in doing so, however, the hardware may .rst have to consult the system page table (which lives in physicalmem­ory); with that translation complete, the hardware can learnthe ad­dress of the page of the page table, and then .nally learn the address of the desired memory access. All of this, fortunately, is made faster by the VAX’s hardware-managed TLBs, which usually (hopefully) circumvent this laborious lookup. 
ASIDE:WHY NULL POINTER DEREFERENCES SEG FAULT 
You should now have a good understanding of exactly what hap­
pens on a null-pointer dereference. A process generates a virtual 
address of 0, by doing something like this: 
int *p= NULL;// setp = 0 *p=10; //try tostore value 10to virtual address 0 
The hardware tries to look up the VPN (also 0 here) in the TLB, and suffers a TLB miss. The page table is consulted, and the entry for VPN 0 is found to be marked invalid. Thus, we have an invalid ac­cess, which transfers control to the OS, which likely terminates the process (on UNIX systems, processes are sent a signal which allows them to react to such a fault; if uncaught, however, the process is killed). 
OPERATING SYSTEMS ARPACI-DUSSEAU 

22.3 A Real Address Space 
One neat aspect of studying VMS is that we can see how a real address space is constructed. Thus far, we have assumed a simple address space of just user code, user data, and user heap, but as we can see above, a real address space is notably more complex. 
For example, the code segment never begins at page 0. This page, instead, is marked inaccessible, in order to provide some support for detecting null-pointer accesses. Thus, one concern when designing an address space is support for debugging, which the inaccessible zero page provides here in some form. 
Perhaps more importantly, the kernel virtual address space (i.e., its data structures and code) is a part of each user address space. On acontext switch, the OS changes the P0 and P1 registers to point to the appropriate page tables of the soon-to-be-run process; however, it does not change the S base and bound registers, and as a result the “same” kernel structures are mapped into each user address space. 
The kernel is mapped into each address space for a number of reasons. This construction makes life easier for the kernel;when, for example, the OS is handed a pointer from a user program (e.g., on awrite() system call), it is easy to copy data from that pointer to its own structures. The OS is naturally written and compiled, without worry of where the data it is accessing comes from. If in contrast the kernel were located entirely in physical memory, it would be quite hard to do things like swap pages of the page table to disk; if incon­trast the kernel were given its own address space, moving databe­tween user applications and the kernel would again be complicated and painful. With this construction (used in modern systems), the kernel appears almost as a library to applications, albeit a protected one. 
One last point about this address space relates to protection. Clearly, 
the OS does not want user applications reading or writing OS data 
or code. Thus, the hardware must support different protection levels 
for pages to enable this. The VAX did so by specifying, in protection 
bits in the page table, what privilege level the CPU must be at in or­
der to access a particular page. Thus, system data and code areset 
to a higher level of protection than user data and code; an attempted 
access to such information from user code will generate a trapinto 
the OS, and (you guessed it) the likely termination of the offending 
process. 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 

22.4 Page Replacement 
The page table entry (PTE) in VAX contains the following bits:a valid bit, a protection .eld (4 bits), a modify (or dirty) bit,a .eld reserved for OS use (5 bits), and .nally a physical frame number (PFN) to store the location of the page in physical memory. Theas­tute reader might note: no reference bit!Thus, the VMS replacement algorithm must make do without hardware support for determining which pages are active. 
The developers were also concerned about memory hogs,pro­grams that use a lot of memory and make it hard for other programs to run. Most of the policies we have looked at thus far are suscepti­ble to such hogging; for example, LRU is a global policy that doesn’t share memory fairly among processes. 
Segmented FIFO 
To address these two problems, the developers came up with the seg­mented FIFO replacement policy [RL81]. The idea is simple: each process has a maximum number of pages it can keep in memory, known as its resident set size (RSS). Each of these pages is kept on a FIFO list; when a process exceeds its RSS, the “.rst-in” page is evicted. FIFO clearly does not need any support from the hardware, and is thus easy to implement. 
Of course, pure FIFO does not perform particularly well, as we saw earlier. To improve FIFO’s performance, VMS introduced two second-chance lists where pages are placed before getting evicted from memory, speci.cally a global clean-page free list and dirty-page list.When a process P exceeds its RSS, a page is removed from its per-process FIFO; if clean (not modi.ed), it is placed on the end of the clean-page list; if dirty (modi.ed), it is placed on the end of the dirty-page list. 
If another process Q needs a free page, it takes the .rst free page off of the global clean list. However, if the original process P faults on that page before it is reclaimed, P reclaims it from the free (or dirty) list, thus avoiding a costly disk access. The bigger these global second-chance lists are, the closer segmented FIFO performsto LRU [RL81]. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
ASIDE:EMULATING REFERENCE BITS As it turns out, you don’t need a hardware reference bit in order to get some notion of which pages are in use in a system. In fact, in the early 1980’s, Babaoglu and Joy showed that protection bits on the VAX can be used to emulate reference bits [BJ81]. The basic idea: if you want to gain some understanding of which pages are actively being used in a system, mark all of the pages in the page table as inaccessible (but keep around the information as to which pages are really accessible by the process, perhaps in the “reserved OS.eld” portion of the page table entry). When a process accesses a page, it will generate a trap into the OS; the OS will then check if the page really should be accessible, and if so, revert the page to its normal protections (e.g., read-only, or read-write). At the time ofa replace­ment, the OS can check which pages remain marked inaccessible, and thus get an idea of which pages have not been recently used. 
The key to this “emulation” of reference bits is reducing overhead while still obtaining a good idea of page usage. The OS must notbe too aggressive in marking pages inaccessible, or overhead would be too high; the OS also must not be too passive in such marking, orall pages will end up referenced, and the OS will again have no good idea of which page to evict. 

Page Clustering 
Another optimization used in VMS also helps overcome the small page size in VMS. Speci.cally, with such small pages, disk I/Odur­ing swapping could be highly inef.cient, as disks do better with large transfers. To make swapping I/O more ef.cient, VMS adds anumber of optimizations, but mostimportantis clustering.With clustering, VMS groups large batches of pages together from the global dirty list, and writes them to disk in one fell swoop (thus making them clean). Clustering is used in most modern systems, as thefree­dom to place pages anywhere within swap space lets the OS group pages, perform fewer and bigger writes, and thus improve perfor­mance. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 


22.5 Other Neat VM Tricks 
The VMS virtual memory system had two other now standard tricks: demand zeroing and copy-on-write. We now describe these cool optimizations that save space in memory, through the virtue of being lazy. 
One form of laziness in VMS (and most modern systems) is de­mand zeroing of pages. To understand this better, let’s consider the example of adding a page to your address space, say in your heap. In a naive implementation, the OS responds to a request to add a page to your heap by .nding a page in physical memory, zeroing it (required for security; otherwise you’d be able to see what was on the page from when some other process used it!), and then mapping it into your address space (i.e., setting up the page table to refer to that physical page as desired). But the naive implementationcanbe costly, particularly if the page does not get used by the process. 
With demand zeroing, the OS instead does very little work when the page is added to your address space; it puts an entry in the page table that marks the page inaccessible. If the process then reads or writes the page, a trap into the OS takes place. When handling the trap, the OS notices (usually through some bits marked in the “re­served for OS” portion of the page table entry) that this is actually a demand-zero page; at this point, the OS then does the needed work of .nding a physical page, zeroing it, and mapping it into the pro­cess’s address space. If the process never accesses the page,all of this work is avoided, and thus the virtue of demand zeroing. 
Another cool optimization found in VMS (and again, in virtu­ally every modern OS) is copy-on-write (COW for short). The idea, which goes at least back to the TENEX operating system [BB+72], is simple: when the OS needs to copy a page from one address space to another, instead of copying it, it can map it into the targetaddress space and mark it read-only in both address spaces. If both address spaces only read the page, no further action is taken, and thusthe OS has affected a fast copy without actually moving any data. 
If, however, one of the address spaces does indeed try to writeto the page, it will trap into the OS. The OS will then notice that the page is a COW page, and thus (lazily) allocate a new page, .ll it withthe data, and map this new page into the address space of the faulting process. The process then continues and now has its own private copy of the page. 
OPERATING SYSTEMS ARPACI-DUSSEAU DESIGN TIP:BE LAZY 
Being lazy can be a virtue in both life as well as in operating systems. 
Laziness can put off work until later, which is bene.cial within an 
OS for a number of reasons. First, putting off work might reduce 
the latency of the current operation, thus improving responsiveness; 
for example, operating systems often report that writes to a .le suc­
ceeded immediately, and only write them to disk later in the back­
ground. Second, and more importantly, laziness sometimes obviates 
the need to do the work at all; for example, delaying a write until the 
.le is deleted removes the need to do the write at all. Lazinessis also 
good in life: for example, by putting off your OS project, you may 
.nd that the project speci.cation bugs are worked out by your fel­
low classmates; however, the class project is unlikely to getcanceled, 
so being too lazy may be problematic, leading to a late project, bad 
grade, and a sad professor. Don’t make your professor sad! 
COW is useful for a number of reasons. Certainly any sort of shared library can be mapped copy-on-write into the address spaces of many processes, saving valuable memory space. In UNIX sys­tems, COW is even more critical, due to the semantics of fork() and exec().As you might recall, fork() creates an exact copy of the address space of the caller; with a large address space, making such acopy is slow and data intensive. Even worse, mostof the address space is immediately over-written by a subsequent call to exec(), which overlays the calling process’s address space with thatof the soon-to-be-exec’d program. By instead performing a copy-on-write fork(),the OS avoidsmuch of the needless copying andthus re­tains the correct semantics while improving performance. 

22.6 Summary 
You have now seen a top-to-bottom review of an entire virtual memory system. Hopefully, most of the details were easy to follow, as you should have already had a good understanding of most of the basic mechanisms and policies. More detail is available in the excel­lent (and short) paper by Levy and Lipman [LL82]; we encourage you to read it, a great way to see what the source material behind these chapters is like. 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 

References 
[BB+72] “TENEX, A Paged Time Sharing System for the PDP-10” Daniel G. Bobrow, Jerry D. Burch.el, Daniel L. Murphy, Raymond S. Tomlinson Communications of the ACM, Volume 15, March 1972 
An early time-sharing OS where a number of good ideas came from. Copy-on-write was just one of those; inspiration for many other aspects of modern systems, including process management, virtual memory, and .le systems are found herein. 
[BJ81] “Converting a Swap-Based System to do Paging in an Architecture Lacking Page-Reference Bits” Ozalp Babaoglu and William N. Joy SOSP ’81, December 1981, Paci.c Grove, California 
Aclever idea paper on howto exploit existing protection machinery within a machine in order to emulate reference bits. The idea came from the group at Berkeley working on their own version of UNIX,known as the Berkeley Systems Distribution, or BSD. The group was heavily in.uential in the development of UNIX,in virtual memory, .le systems, and networking. 
[C03] “The Innovator’s Dilemma” 
Clayton M. Christenson 
Harper Paperbacks, January 2003 

Afantastic book about the disk-drive industry and how newinnovations disrupt existing ones. Agood read for business majors and computer scientists alike. 
[C93] “Inside Windows NT” 
Helen Custer and David Solomon 
Microsoft Press, 1993 

The book about Windows NT that explains the system top to bottom, in more detail than you might like. But seriously, a pretty good book. 
[LL82] “Virtual Memory Management in the VAX/VMS Operating System” Henry M. Levy, Peter H. Lipman IEEE Computer, Volume 15, Number 3 (March 1982) Read the original source of most of this material; tt is a concise and easy read. Particularly important if you wish to go to graduate school, where all you do is read papers, work, read some more papers, work more, eventually write apaper, and then work some more. But it is fun! 
[RL81] “Segmented FIFO Page Replacement” 
Rollins Turner and Henry Levy 
SIGMETRICS ’81 

Ashort paper that shows for some workloads, segmented FIFO can approach the performance of LRU. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
23 



Summary Dialogue on MemoryVirtualization 
Student: (Gulps) Wow, that was a lot of material. 
Professor: Yes, and? 
Student: Well, how am I supposed to remember it all? You know, for the 

exam? 
Professor: Goodness, I hope that’s not why you are trying to remember it. 
Student: Why should I then? 
Professor: Come on, I thought you knew better. You’re trying to learn 

something here, so that when you go off into the world, you’ll understand how systems actually work. 
Student: Hmm... can you give an example? Professor: Sure! One time back in graduate school, my friends and I were measuring how long memory accesses took, and once in a while the numbers were way higher than we expected; we thought all the data was .tting nicely into the second-level hardware cache, you see, and thus should have been really fast to access. 
Student: (nods) 
Professor: We couldn’t .gure out what was going on. So what do you do 

289 
in such a case? Easy, ask a professor! So we went and asked one ofour pro­fessors, who looked at the graph we had produced, and simply said “TLB”. Aha! Of course, TLB misses! Why didn’t we think of that? Havinga good model of how virtual memory works helps diagnose all sorts of interesting performance problems. 
Student: Ithink Isee. I’m trying to build these mental models of how things work, so that when I’m out there working on my own, I won’t be surprised when a system doesn’t quite behave as expected. I should even be able to anticipate how the system will work just by thinking about it. 
Professor: Exactly. So what have you learned? What’s in your mental model of how virtual memory works? 
Student: Well, I think I now have a pretty good idea of what happens when memory is referenced by a process, which, as you’ve said many times, happens on each instruction fetch as well as explicit loads and stores. 
Professor: Sounds good – tell me more. 
Student: Well, one thing I’ll always remember is that the addresses we see in a user program, written in C for example... 
Professor: What other language is there? 
Student: (continuing) ... Yes, I know you like C. So do I! Anyhow, as I was saying, I now really know that all addresses that we can observe within a program are virtual addresses; that I, as a programmer, am just given this illusion of where data and code are in memory. I used to think itwas cool that I could print the address of a pointer, but now I .nd it frustrating – it’s just a virtual address! I can’t see the real physical address where the data lives. 
Professor: Nope, the OS de.nitely hides that from you. What else? 
Student: Well, I think the TLB is a really key piece, providing the system with a small hardware cache of address translations. Page tables are usually quite large and hence live in big and slow memories. Without that TLB, programs would certainly run a great deal more slowly. Seems like the TLB truly makes virtualizing memory possible. I couldn’t imagine building a system without one! And I shudder at the thought of a program with a working set that exceeds the coverage of the TLB: with all those TLB misses, 
OPERATING SYSTEMS ARPACI-DUSSEAU 
it would be hard to watch. 
Professor: Yes, cover the eyes of the children! Beyond the TLB, what did you learn? 
Student: Ialso now understand that the page table is one of those data structures you need to know about; it’s just a data structure,though, and that means almost any structure could be used. We started withsimple structures, like arrays (a.k.a. linear page tables), and advanced all the way up to multi-level tables (which look like trees), and even crazier things like pageable page tables in kernel virtual memory. All to save a little space in memory! 
Professor: Indeed. 
Student: And here’s one more important thing: I learned that the address translation structures need to be .exible enough to support what program­mers want to do with their address spaces. Structures like themulti-level table are perfect in this sense; they only create table space when the user needs a portion of the address space, and thus there is little waste. Ear­lier attempts, like the simple base and bounds register, justweren’t.exible enough; the structures need to match what users expect and want out of their virtual memory system. 
Professor: That’s a nice perspective. What about all of the stuff we learned about swapping to disk? 
Student: Well, it’s certainly fun to study, and good to know how page re­placement works. Some of the basic policies are kind of obvious (like LRU, for example), but building a real virtual memory system seemsmorein­teresting, like we saw in the VMS case study. But somehow, I found the mechanisms more interesting, and the policies less so. 
Professor: Oh, why is that? 
Student: Well, as you said, in the end the best solution to policy problems is simple: buy more memory. But the mechanisms you need to understand to know how stuff really works. Speaking of which... 
Professor: Yes? 
Student: Well, my machine is running a little slowly these days... and 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
memory certainly doesn’t cost that much... Professor: Oh .ne, .ne! Here’s a few bucks. Go and get yourself some DRAM, cheapskate. Student: Thanks professor! I’ll never swap to disk again – or, if I do, at least I’ll know what’s actually going on!  
OPERATING SYSTEMS  ARPACI-DUSSEAU  



Part II 
Concurrency 

293 
24 


ADialogue on Concurrency 
Professor: And thus we reach the second of our four pillars of operating systems: concurrency. 
Student: But what is concurrency, oh wonderful professor? 
Professor: Well, imagine we have a peach – 
Student: (interrupting) Peaches again! What is it with you and peaches? 
Professor: Ever read T.S. Eliot? The Love Song of J. Alfred Prufrock, “Do Idare to eat a peach”, and all that fun stuff? 
Student: Oh yes! In English class in high school. Great stuff! I really liked the part where – 
Professor: (interrupting) This has nothing to do with that – I just like peaches. Anyhow, imagine there are a lot of peaches on a table,and a lot of people who wish to eat them. Let’s say we did it this way: eacheater .rst identi.es a peach visually, and then tries to grab it and eat it. What is wrong with this approach? 
Student: Hmmm... seems like you might see a peach that somebody else also sees. If they get there .rst, when you reach out, no peach for you! 
Professor: Exactly! So what should we do about it? 
Student: Well, probably develop a better way of going about this. Maybe form a line, and when you get to the front, grab a peach and get onwith it. 
295 
Professor: Good! But what’s wrong with your approach? 
Student: Sheesh, do I have to do all the work? 
Professor: Yes. 
Student: OK, let me think. Well, we used to have many people grabbing for peaches all at once, which is faster. But in my way, we just go one at atime, which is correct, but quite abit slower. The best kind of approach would be fast and correct, probably. 
Professor: You are really starting to impress. In fact, you just told us everything we need to know about concurrency! Well done. 
Student: Idid? Ithought we were justtalking aboutpeaches. Remember, this is usually a part where you make it about computers again. 
Professor: Indeed. My apologies! One must never forget the concrete. Well, as it turns out, there are certain types of programs thatwe call multi-threaded applications; each thread is kind of like an independent agent running around in this program, doing things on the program’sbehalf. But these threads access memory, and for them, each spot of memoryis kind of like one of those peaches. If we don’t coordinate access to memory between threads, the program won’t work as expected. Make sense? 
Student: Kind of. But why do we talk about this in an OS class? Isn’t that just application programming? 
Professor: Good question! A few reasons, actually. First, the OS must support multi-threaded applications with primitives such as locks and con­dition variables,which we’ll talk about soon. Second, the OS itself was the .rst concurrent program – it must access its own memory very care­fully or many strange and terrible things will happen. Really, it can get quite grisly. 
Student: Isee. Sounds interesting. There are more details, I imagine? 
Professor: Indeed there are... 
OPERATING SYSTEMS ARPACI-DUSSEAU 
25 



Concurrency: An Introduction 
Thus far, we have seen the development of the basic abstractions that the OS performs. We have seen how to take a single physical CPU and turn it into multiple virtual CPUs,thusenabling the illu­sion of multiple programs running at the same time. We have also seen how to create the illusion of a large, private virtual memory for each process; this abstraction of the address space enables each program to behave as if it has its own memory when indeed the OS is secretly multiplexing address spaces across physical memory (and sometimes, disk). 
In this note, we introduce a new abstraction for a single running process: that of a thread.Instead of our classic view of a single point of execution within a program (i.e., a single PC where instructions are being fetched from and executed), a multi-threaded program has more than one point of execution (i.e., multiple PCs, each of which is being fetched and executed from). Perhaps another way to think of this is that each thread is very much like a separate process, except for one difference: they share the same address space and thus can access the same data. 
The state of a single thread is thus very similar to that of a pro­cess. It has a program counter (PC) that tracks where the program is fetching instructions from. Each thread has its own private set of registers it uses for computation; thus, if there are two threads that are running on a single processor, when switching from running one (T1) to running the other (T2), a context switch must take place. The context switch between threads is quite similar to the context switch between processes, as the register of T1 must be saved and the regis­
297 
0KB the code segment: where instructions live 
1KB the heap segment: contains malloc’d data 
2KB 
Program Code  
Heap  
(free)  
Stack  

dynamic data structures (it grows downward) 
(it grows upward) the stack segment: 15KB 
contains local variables arguments to routines, return values, etc. 
16KB 
Figure 25.1: A Single-Threaded Address Space 
ter state of T2 restored before running T2. With processes, wesaved state to a process control block (PCB);now, we’ll need one or more thread control blocks (TCBs) to store the state of each thread of a process. There is one major difference, though, in the context switch we perform between threads as compared to processes: the address space remains the same (i.e., there is no need to switch which page table we are using). 
One other major difference between threads and processes con­cerns the stack. In our simple model of the address space of a classic process (which we can now call a single-threaded process), there is asingle stack, perhaps residing atthe bottom of the address space (Figure 25.1). 
0KB 
1KB 
2KB 
15KB 
16KB 
Program Code  
Heap  
(free)  
Stack (2)  
(free)  
Stack (1)  

Figure 25.2: A Multi-Threaded Address Space 
However, in a multi-threaded process, each thread runs indepen­dently and of course may call into various routines to do whatever work it is doing. Thus, instead of a single stack within the address 
OPERATING SYSTEMS ARPACI-DUSSEAU 
space, there will be one for each thread. Let’s say we have a multi-threaded process that has two threads in it. In such a case, ourad­dress space looks different (Figure 25.2). 
In this .gure, you can see two stacks spread throughout the ad­dress space of the process. Thus, any stack-allocated variables, pa­rameters, return values, and other things that we put on the stack will be placed in what is sometimes called thread-local storage, i.e., the stack of the relevant thread. 
You might also notice how this ruins our beautiful address space layout. Before, the stack and heap could grow independently and trouble only arose when you ran out of room in the address space. Here, we no longer have such a nice situation. Fortunately, this is usually OK, as stacks do not generally have to be very large (the exception being in programs that make heavy use of recursion). 
25.1 An Example: Thread Creation 
#include <stdio.h> 
#include <assert.h> 
#include <pthread.h> 

void *mythread(void *arg) {
printf("%s\n", (char *)arg); 
return NULL; 

} 
int 
main(int argc, char *argv[]) {pthread_t p1, p2;int rc; printf("main: begin\n"); rc = pthread_create(&p1, NULL, mythread, "A"); assert(rc ==0); rc = pthread_create(&p2, NULL, mythread, "B"); assert(rc ==0); // join waits for the threads to finish rc = pthread_join(p1, NULL); assert(rc == 0); rc = pthread_join(p2, NULL); assert(rc == 0);printf("main: end\n"); return 0; 
} 
Figure 25.3: Simple Thread Creation Code 
Let’s say we wanted to run a program that created two threads, each of which was doing some independent work, in this case print-
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
ing “A” or “B”. The code might look something like what you see in Figure 25.3. 
The main program creates two threads, each of which will run the function mythread(),though with different arguments (the string A or B). Once a thread is created, it may start running right away (de­pending on the whims of the scheduler); alternately, it may beput in a“ready” butnot“running” state and thus notrun yet. After creating the two threads T1 and T2, the main thread calls pthread join(), which waits for a particular thread to complete. Let us examine the possible execution ordering of this little program. In the diagram, time increases downwards: 
main starts running
main prints "main: begin"
main creates thread T1 
main creates thread T2 
main waits for T1 

T1 runs T1 prints "A"T1 returns 
main waits for T2 T2 runs T2 prints "B"T2 returns 
main prints "main: end" 
Note, however, that this is not the only possible ordering. Infact, there are many, depending on what the scheduler decides to runat a given point. For example, once a thread is created, it may run imme­diately, which would lead to the following execution: 
main starts running
main prints "main: begin"
main creates thread T1 

T1 runs T1 prints "A"T1 returns 
main creates thread T2 T2 runs T2 prints "B"T2 returns 
main waits for T1 (returns immediately because T1 is finished)main waits for T2 (returns immediately because T2 is finished)main prints "main: end" 
OPERATING SYSTEMS ARPACI-DUSSEAU 

We also could even see ”B” printed before ”A”, if, say, the sched­
uler decided to run it .rst even though thread 1 was created .rst. 
This .nal execution ordering is shown here: 
main starts running
main prints "main: begin"
main creates thread T1 
main creates thread T2 

T2 runs T2 prints "B"T2 returns 
main waits for T1 
T1 runs 
T1 prints "A"
T1 returns 

main waits for T2 
(returns immediately because T2 is finished)

main prints "main: end" 
As you might be able to see, one way to think about thread cre­ation is that it is a bit like making a function call; however, instead of .rst executing the function and then returning to the caller,the sys­tem instead creates a new thread of execution for the routine that is being called, and it runs independently of the caller, perhaps before returning from the create, but perhaps much later. 
As you also might be able to tell from this example, threads make 
life complicated: it is already hard to tell what will run when! Unfor­
tunately, it gets worse. Much worse. 

25.2 Why It Gets Worse: Shared Data 
The simple thread example we showed above was useful in show­ing how threads are created and how they can run in different or­ders depending on how the scheduler decides to run them. What it doesn’t show you, though, is how threads interact when they access shared data. 
Let us imagine a simple example where two threads wish to up­
date a global shared variable. The code might look something like 
what is in Figure 25.4. 
Afew notes about the code. First, as Stevens suggests [S92], we wrap the thread creation and join routines to simply exit on failure; for a program as simple as this one, we want to at least notice anerror occurred (if it did), but not do anything very smart about it (e.g., just 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
#include <stdio.h> 
#include <pthread.h>
#include "mythreads.h" 

static volatile int counter = 0; 

//
// mythread()
//
// Simply adds 1 to counter repeatedly, in a loop
// No, this is not how you would add 10,000,000 to 
// a counter, but it shows the problem nicely. 
//
void * 
mythread(void *arg)
{ 

printf("%s: begin\n", (char *)arg); 
int i; 
for (i = 0;i < 1e7; i++) { 

counter = counter + 1; }printf("%s: done\n", (char *)arg); return NULL; 
} 

//
// main()
//
// Just launches two threads (pthread_create)
// and then waits for them (pthread_join)
//
int 
main(int argc, char *argv[])
{ 

pthread_t p1, p2;
printf("main: begin (counter = %d)\n", counter);
Pthread_create(&p1, NULL, mythread, "A");
Pthread_create(&p2, NULL, mythread, "B"); 

// join waits for the threads to finish 
Pthread_join(p1, NULL);
Pthread_join(p2, NULL);
printf("main: done with both (counter = %d)\n", counter); 
return 0; 

} 
Figure 25.4: Sharing Data: Oh Oh 
OPERATING SYSTEMS ARPACI-DUSSEAU 
exit). Thus, Pthread 
create() simply calls pthread 
create() and makes sure the return code is 0; if it isn’t, Pthread 
create() just prints a message and exits. 
Second, you can see that instead of using two separate function bodies for the worker threads, we just use a single piece of code, and pass the thread an argument (in this case, a string) so we can have each thread print a different letter before its messages. 
Finally, and most importantly, we can now look at what each worker is trying to do: add a number to the shared variable counter, and do so 10 million times (1e7) in a loop. Thus, the desired .nal re­sult is: 20,000,000. 
We now compile and run the program, to see how it behaves. If everything works as we might think it would, the program produces the following output: 
prompt> gcc -o main main.c -Wall -lpthread prompt> ./mainmain: begin (counter = 0)
A: begin

B: begin

A: done 

B: done 
main: done with both (counter = 20000000) 
prompt> 

Unfortunately, when we run this code, even on a single processor, we don’t necessarily get the desired result. Sometimes, we might get something more like this: 
prompt> gcc -o main main.c -Wall -lpthread prompt> ./mainmain: begin (counter = 0)
A: begin

B: begin

A: done 

B: done 
main: done with both (counter = 19345221) 

Let’s try it one more time, just to see if we’ve gone crazy. After all, aren’t computers supposed to produce deterministic results? This is what you have been taught! Perhaps your professors have been lying to you? (gasp) 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
prompt> ./main
main: begin (counter = 0)

A: begin

B: begin

A: done 

B: done 
main: done with both (counter = 19221041) 
prompt> 

Not only is each run wrong, but also yields a different result! A big question remains: why does this happen? 

25.3 The Heart of the Problem: Uncontrolled Scheduling 
To understand why this happens, we must understand the code sequence that the compiler generates for the update to counter.In this case, we wish to simply add a number (1) to counter.Thus, the code sequence for doing so might look something like this (an x86 example here): 
mov 0x8049a1c, %eax 
add $0x1, %eax 
mov %eax, 0x8049a1c 

This example assumes that the variable counter is located at ad­dress 0x8049a1c. In this three-instruction sequence, the x86 mov in­struction is used .rst to get the memory value at the address and put it into register eax.Then, the add is performed, adding 1 (0x1) to the contents of the eax register, and .nally, the contents of eax are stored back into memory at the same address. 
Let us imagine one of our two threads (thread 1) enters this region of code, and is thus about to increment counter by one. It loads the value of counter (let’s say it is zero to begin with) into its register eax.Thus, eax=0 for thread 1. Now, something unfortunate hap­pens: a timer interrupt goes off. This causes the OS to save thestate of the currently running thread (its PC, its registers including eax, etc.) to the TCB for this thread. 
Now something worse happens: thread 2 is chosen to run, and it enters this same piece of code. It also executes the .rst instruction, getting the value of counter and putting it into its eax (remember: each thread when running has its own private registers; the registers are virtualized by the context-switch code that saves and restores 
OPERATING SYSTEMS ARPACI-DUSSEAU 
DESIGN TIP:USE YOUR TOOLS You should always learn new tools that help you write, debug, and understand computer systems. Here, we use a neat tool called a dis­assembler. When you run a disassembler on an executable binary, it shows you what assembly instructions make up the program. For example, if we wish to understand how the low-level code to up­date a counter (as in our example above), we might run objdump (on Linux) to see what the assembly code looks like: 
prompt> objdump -d main 
Doing so produces a long listing of all the instructions in thepro­gram, neatly labeled (particularly if you compiled with the -g .ag), which includes symbol information in the program. The objdump program is just one of many tools you should learn how to use; a de­bugger like gdb,memory pro.lers like valgrind or purify,and of course the compiler itself are others that you should spendtime to learn more about; the better you are at using your tools, thebetter systems you’ll be able to build. 
them). The value of counter is still zero at this point, and thus thread 2 has eax=0.Let’s then assume that thread 2 executes the next two instructions, incrementing eax by 1 (thus eax=1), and then saving the contents of eax into counter (address 0x8049a1c). Thus, the global variable counter now has the value 1. 
Finally, another context switch occurs, and thread 1 resumesrun­ning. Recall that it had just executed the .rst mov instruction, and is now about to add 1 to eax.Recall also that eax=0.Thus, the add instruction increments eax by 1 (thus eax=1), and then the mov instruction saves it to memory (thus counter is set to 1 again). 
Put simply, what has happened is this: the code to increment counter has been run twice, but counter,which started at 0, is now only equal to 1. A “correct” version of this program should have resulted in counter equal to 2. 
Here is a pictorial depiction of what happened and when in the example above. Assume, for this depiction, that the above code is loaded at address 100 in memory, like the following sequence (note for those of you used to nice, RISC-like instruction sets: x86has 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
variable-length instructions; the mov instructions here take up 5 bytes 
of memory, whereas the add takes only 3): 100 mov 0x8049a1c, %eax 105 add $0x1, %eax 108 mov %eax, 0x8049a1c 
With these assumptions, what happens is seen in Figure 25.5. As­
sume the counter starts at value 50, and trace through this example 
to make sure you understand what is going on. 
What we have demonstrated here is called a race condition:the results depend on the timing execution of the code. With some bad luck (i.e., context switches that occur at untimely points inthe execu­tion), we get the wrong result. In fact, we may get a different result each time; thus, instead of a nice deterministic computation (which we are used to from computers), we call this result indeterminate, where it is not known what the output will be and it is indeed likely to be different across runs. 
Because multiple threads executing this code can result in a race condition, we call this code a critical section.A critical section is apiece of code that accesses ashared variable (or more generally, ashared resource) and must not be concurrently executed by more than one thread. 
What we really want for this code is what we call mutual exclu-
sion.This property guarantees that if one thread is executing within 
the critical section, the others will be prevented from doingso. 
(values AFTER inst) 
OS THREAD 1 THREAD 2 PC %eax COUNTER (before crit sect) 100 0 50 mov 0x8049a1c, %eax 105 50 50 add $0x1, %eax 108 51 50 
Interrupt(save T1’s state, restore T2) 100 0 50 
mov 0x8049a1c, %eax 105 50 50 add $0x1, %eax 108 51 50 mov %eax, 0x8049a1c 113 51 51 
Interrupt(save T2’s state, restore T1) 108 51 51 
mov %eax, 0x8049a1c 113 51 51 (not 52!) 
Figure 25.5: The Problem: Up Close and Personal 
OPERATING SYSTEMS ARPACI-DUSSEAU 
Virtually all of these terms, by the way, were coined by Edsger Dijkstra, who was a pioneer in the .eld and indeed won the Turing Award because of this and other work; see his paper on “Cooperat­ing Sequential Processes” [D68] for an amazingly clear description of the problem from 1968! We’ll be hearing more about Dijkstra, of course, in this section of the book. 

25.4 The Wish For Atomicity 
One way to solve this problem would be to have more powerful instructions that, in a single step, did exactly whatever we needed done and thus removed the possibility of an untimely interrupt. For example, if we had a super instruction that looked like this: 
memory-add 0x8049a1c, $0x1 
which added a value to a memory location, the hardware would guarantee that this would execute atomically;when the instruction executed, it would perform the update as desired. It could notbe interrupted mid-instruction, because that is precisely theguarantee we receive from the hardware: when an interrupt occurs, either the instruction has not run at all, or it has run to completion; there is no in-between state. Hardware can be a beautiful thing, no? 
Atomically, in this context, means “as a unit”, which sometimes 
we take as “all or none.” What we’d like is to execute the three in­
struction sequence atomically: 
mov 0x8049a1c, %eax 
add $0x1, %eax 
mov %eax, 0x8049a1c 

As we said, if we had a single instruction to do this, we could just issue that instruction and be done. But in the general case, wewon’t have such an instruction. Imagine we were building a concurrent B-tree, and wished to update it; would we really want the hardware to support an “atomic update of B-tree” instruction? Probably not, at least in a sane instruction set. 
Thus, what we will instead do is ask the hardware for a few useful instructions upon which we can build a general set of what we call synchronization primitives.By using these synchronization primi­tives, in combination with some help from the OS, we will be able to 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
KEY CONCURRENCY TERMS: CRITICAL SECTION,RACE CONDITION, INDETERMINATE,MUTUAL EXCLUSION 
These four terms are so central to concurrent code that we thought it worth while to call them out explicitly. See some of Dijkstra’s early work [D65,D68] for more details. 
• 	
A critical section is a piece of code that accesses a shared resource, usu­ally a variable or data structure. 

• 	
A race condition arises if multiple threads of execution enter the criti­cal section at roughly the same time; both attempt to update the shared data structure, leading to a surprising (and perhaps undesirable) out­come. 

• 	
An indeterminate program consists of one or more race conditions; the output of the program varies from run to run, depending on which threads ran when. The outcome is thus not deterministic,something we usually expect from computer systems. 

• 	
To avoid these problems, threads should use some kind of mutual ex­clusion primitives; doing so guarantees that only a single thread ever enters a critical section, thus avoiding races, and resulting in determin­istic program outputs. 


build multi-threaded code that accesses critical sections in a synchro­nized and controlled manner, and thus reliably produces the correct result despite the challenging nature of concurrent execution. 
This is the problem we will be studying in this section of the book. It is a wonderful and hard problem, and should make your mind hurt (at least a little). If it doesn’t, then you don’t understand it. So keep working until your head hurts; you then know you are headed in the right direction. 
THE CRUX: HOW TO PROVIDE SUPPORT FOR SYNCHRONIZATION 
What support do we need from the hardware in order to build useful synchronization primitives? What support do we need from the OS? How can we build these primitives correctly and ef.ciently? How can programs use them to get the desired results? 
OPERATING SYSTEMS ARPACI-DUSSEAU 

25.5 One More Problem: Waiting For Another 
This chapter has set up the problem of concurrency as if only one type of interaction occurs between threads, that of accessing shared variables and the need to support atomicity for critical sections. As it turns out, there is another common interaction that arises, where one thread must wait for another to complete some action before itcon­tinues. This interaction arises, for example, when a processperforms adisk I/O and is putto sleep; when the I/O completes, the process needs to be woken so it can continue. 
Thus, in the coming chapters, we’ll be not only studying how to build support for synchronization primitives to support atomicity but also for mechanisms to support this type of sleeping/waking in­teraction that is common in multi-threaded programs. If thisdoesn’t make sense right now, that is OK! It will soon enough, when you read the chapter on condition variables.If it doesn’t by then, well, then it is less OK, and you should read that chapter again (and again) until it does make sense. 

25.6 Summary: Why in OS Class? 
Before wrapping up, one question that you might have is: why are we studying this in OS class? “History” is the one-word answer. Simply put, the OS was the .rst concurrent program, and thus most of these techniques arose due to the need for them within the OS. Later, as multi-threaded programs became popular, application pro­grammers also had to consider such things. 
For example, imagine the case where there are two processes run­
ning. One calls into the kernel to open a .le, say using the open() 
system call. At about the same time, another process does the same 
thing. Now say that there is a counter within the OS called opencount, 
which is incremented each time open is called. Because an interrupt 
may occur at any time, the update to the shared variable opencount 
is a critical section; thus, OS designers, from the very beginning of the 
introduction of the interrupt, had to worry about how the OS would 
update internal structures; an untimely interrupt causes all of the 
problems described above. Thus, page tables, process lists,and vir­
tually every kernel data structure has to be carefully accessed, with 
the proper synchronization primitives, to work correctly. 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
DESIGN TIP:ATOMIC OPERATIONS 
Atomic operations are one of the most powerful underlying tech­
niques in building computer systems, from the computer architec­
ture, to concurrent code (what we are studying here), to .le systems 
(which we’ll study soon enough), database management systems, 
and even distributed systems [L+93]. 
The idea behind making a series of actions atomic is simply ex­pressed with the phrase “all or nothing”; it should either appear as if all of the actions you wish to group together occurred, or thatnone of them occurred, with no in-between state visible. Sometimes, the grouping of many actions into a single atomic action is calleda trans-action,an idea developed in great detail in the world of databases and transaction processing [GR92]. 
In our theme of exploring concurrency, we’ll be using synchroniza­tion primitives to turn short sequences of instructions intoatomic blocks of execution, but the idea of atomicity is much bigger than that, as we will see. For example, .le systems use techniques such as journaling or copy-on-write in order to atomically transition their on-disk state, critical for operating correctly in the face of system fail­ures. If that doesn’t make sense, don’t worry – it will, in somefuture chapter. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

References 
[D65] “Solution of a problem in concurrent programming control” 
E. W. Dijkstra Communications of the ACM, 8(9):569, September 1965 
Pointed to as the .rst paper of Dijkstra’s where he outlines the mutual exclusion problem and asolution. The solution, however, is not widely used; advanced hardware and OS support is needed, as we will see in the coming chapters. 
[D68] “Cooperating sequential processes” Edsger W. Dijkstra, 1968 Available: http://www.cs.utexas.edu/users/EWD/ewd01xx/EWD123.PDF 
Dijkstra has an amazing number of his old papers, notes, and thoughts recorded (for posterity) on this website at the last place he worked, the University of Texas. Much of his foundational work, however, was done years earlier while he was at the TechnischeHochshule of Eindhoven (THE), including this famous paper on “cooperating sequential processes”, which basically outlines all of the thinking that has to go into writing multi-threaded programs. Dijkstra discovered much of this while working on an operating system named after his school: the “THE” operating system (said “T”, “H”, “E”, and not like the word “the”). 
[GR92] “Transaction Processing: Concepts and Techniques” Jim Gray and Andreas Reuter Morgan Kaufmann, September 1992 
This book is the bible of transaction processing, written by one of the legends of the .eld, Jim Gray. It is, for this reason, also considered Jim Gray’s “brain dump”, in which he wrote down everything he knows about how database management systems work. Sadly, Gray passed away tragically a few years back, and many of us lost a friend and great mentor, including the co­authors of said book, who were lucky enough to interact with Gray during their graduate school years. 
[L+93] “Atomic Transactions” Nancy Lynch, Michael Merritt, William Weihl, Alan Fekete Morgan Kaufmann, August 1993 
Anice text on some of the theory and practice of atomic transactions for distributed systems. Perhaps a bit formal for some, but lots of good material is found herein. 
[S92] “Advanced Programming in the UNIX Environment” W. Richard Stevens Addison-Wesley, 1992 
As we’ve said many times, buy this book, and read it, in little chunks, preferably before going to bed. This way, you will actually fall asleep more quickly; more importantly, you learn a little more about how to become a serious UNIX programmer. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
26 



Interlude: Thread API 
This chapter brie.y covers the main portions of the thread API. Each part will be explained further in the subsequent chapters, aswe show how to use the API. More details can be found in various books and online sources [B97,B+96,K+96]. We should note that the sub­sequent chapters introduce the concepts of locks and condition vari­ables more slowly, with many examples; this chapter is thus better used as a reference. 
26.1 Thread Creation 
The .rst thing you have to be able to do to write a multi-threaded 
program is to create new threads, and thus some kind of thread cre­
ation interface must exist. In POSIX, it is easy: 
#include <pthread.h>
int 
pthread_create(pthread_t * restrict thread, 

const pthread_attr_t * restrict attr, void * (*start_routine)(void*),void * restrict arg); 
Now, this declaration might look a little complex (particularly if you haven’t used function pointers in C), but actually it’s not too bad. There are four arguments: thread, attr, start routine, and arg.The .rst, thread,is just a pointer to a structure of type pthread t;we’ll use this structure to interact with thisthread, and thus we need to pass it to pthread create() in order to initialize it. 
313 



The second argument, attr,isused to specify any attributes this thread might have. Some examples include setting the stack size or perhaps information about the scheduling priority of the thread. An attribute is initialized with a separate call to pthread attr init(); see the manual page for details. However, in most cases, the defaults will be .ne; in this case, we will simply pass the value NULL in. 
The third argument is the most complex, but is really just ask­ing: which function should this thread start running in? In C,we call this a function pointer,and thisone tellsusthe following isex­pected: a function name (start routine), which is passed a sin­gle argument of type void * (as indicated in the parentheses after start routine), and which returns a value of type void *. 
If this routine instead required an integer argument, instead of avoid *,the declaration would look like this: 
int pthread_create(..., // first two args are the same void * (*start_routine)(int),int arg); 
If instead the routine took a void * as an argument, but returned an integer, it would look like this: 
int pthread_create(..., // first two args are the same int (*start_routine)(void *),void * arg); 
Finally, the fourth argument, arg,is exactly the argument to be 
passed to the function where the thread begins execution. Nowyou 
might be asking: why all of these void * declarations? Well, the an­
swer is quite simple: having a void * argument to the start routine 
allows us to pass in any type of argument the function requires; hav­
ing it as a return value allows the thread to return any type of argu­
ment as well. 
Let’s look at an example in Figure 26.1. Here we just create a thread that is passed two arguments, packaged into a single type we de.ne ourselves (myarg t). The thread, once created, can simply cast its argument to the type it expects and thus unpack the argu­ments as desired. 
And there it is! Once you create a thread, you really have an­other live executing entity, complete with its own call stack, running within the same address space as all the currently existing threads in the program. The fun thus begins! 
OPERATING SYSTEMS ARPACI-DUSSEAU 






#include <pthread.h> 
typedef struct __myarg_t { 
int a; 
int b; 

}myarg_t; 
void *mythread(void *arg) { 
myarg_t *m=(myarg_t *)arg;
printf("%d %d\n", m->a, m->b); 
return NULL; 

} 
int 
main(int argc, char *argv[]) { 
pthread_t p;
int rc; 

myarg_t args; 
args.a = 10; 
args.b = 20; 
rc = pthread_create(&p, NULL, mythread, &args); 
... 

} 
Figure 26.1: Creating a Thread 

26.2 Thread Completion 
The example above shows how to create a thread. However, what happens if you want to wait for a thread to complete? You need to do something special in order to wait for completion; in particular, you must call the routine pthread 
join(). 
int pthread_join(pthread_t thread, void **value_ptr); 
This routine takes only two arguments. The .rst is of type pthread 
t, 
and is used to specify which thread to wait for. This value is exactly 
what you passed into the thread library during creation; if you held 
onto it, you can now use it to wait for the thread to stop running. 
The second argument is a pointer to the return value you expect to get back. Because the routine can return anything, it is de.ned to return a pointer to void; because the pthread 
join() routine changes the value of the passed in argument, you need to pass in a pointer to that value, not just the value itself. 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
Let’s look at another example (Figure 26.2). In the code, a sin­gle thread is again created, and passed a couple of arguments via the myarg 
t structure. To return values, the myret 
t type is used. Once the thread is .nished running, the main thread, which has beenwait­
ing inside of pthread 
join()1,then returns, andwe can access the values returned from the thread, namely whatever is in myret 
t. 
Afew things to note about this example. First, often times we don’t have to do all of this painful packing and unpacking of argu­ments. For example, if we just create a thread with no arguments, we can pass NULL in as an argument when the thread is created. Simi­larly, we can pass NULL into pthread 
join() if we don’t care about the return value. 
Second, if we are just passing in a single value (e.g., an int),we don’t have to package it up as an argument. Figure 26.3 shows an example. In this case, life is a bit simpler, as we don’t have topackage arguments and return values inside of structures. 
Third, we should note that one has to be extremely careful with how values are returned from a thread. In particular, never return a pointer which refers to something allocated on the thread’s call stack. If you do, what do you think will happen? (think about it!) Hereis an example of a dangerous piece of code, modi.ed from the example in Figure 26.2. 
void *mythread(void *arg) { myarg_t *m= (myarg_t *)arg;printf("%d %d\n", m->a, m->b);myret_t r; // ALLOCATED ON STACK -> BAD! 
r.x = 1; 
r.y = 2; 
return (void *)&r; 
} 

In this case, the variable r is allocated on the stack of mythread. However, when it returns, the value is automatically deallocated (that’s why the stack is so easy to use, after all!), and thus, passing back a pointer to a now deallocated variable will lead to all sorts of bad results. Certainly, when you print out the values you think you 
1Note we use wrapper functions here; speci.cally, we call Malloc(), Pthread 
join(), 
and Pthread create(), which just call their similarly-named lower-caseversions and 
make sure the routines did not return anything unexpected. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
#include <stdio.h> #include <pthread.h>#include <assert.h> #include <stdlib.h> 
typedef struct __myarg_t { int a; int b; 
}myarg_t; 
typedef struct __myret_t { int x; int y;
}myret_t; 
void *mythread(void *arg) { myarg_t *m=(myarg_t *)arg;printf("%d %d\n", m->a, m->b); myret_t *r=Malloc(sizeof(myret_t)); r->x = 1; r->y = 2; return (void *)r; 
} 
int 
main(int argc, char *argv[]) { int rc; pthread_t p; myret_t *m; 
myarg_t args; 
args.a = 10; 
args.b = 20; 
Pthread_create(&p, NULL, mythread, &args);
Pthread_join(p, (void **)&m);
printf("returned %d %d\n", m->x, m->y); 
return 0; 

} 
Figure 26.2: Waiting for Thread Completion 
returned, you’ll probably (but not necessarily!) be surprised at what you see. Try it and .nd out for yourself2! Finally, you might notice that the use of pthread 
create() to 
2
Fortunately the compiler gcc will likely complain when you write code like this, yet another reason to pay attention to compiler warnings. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
void *mythread(void *arg) { 
int m = (int) arg;
printf("%d\n", m); 
return (void *)(arg +1); 

} 
int main(int argc, char *argv[]) { pthread_t p;int rc, m; Pthread_create(&p, NULL, mythread, (void *)100); Pthread_join(p, (void **)&m);printf("returned %d\n", m); return 0; 
} 
Figure 26.3: Simpler Argument Passing to a Thread 
create a thread, followed by an immediate call to pthread 
join(), is a pretty strange way to create a thread. In fact, there is an eas­ier way to accomplish this exact task; it’s called a procedure call. Clearly, we’ll usually be creating more than just one thread and wait­ing for it to complete, otherwise there is not much purpose to using threads at all. 
We should note that not all code that is multi-threaded uses the join routine. For example, a multi-threaded web server mightcreate anumber of worker threads, and then use the main thread to ac­cept requests and pass them to the workers, inde.nitely. Suchlong­lived programs thus may not need join. However, a parallel program which creates threads to execute a particular task (in parallel) will likely use join to make sure all such work completes before either exiting or moving onto the next stage of a computation. 

26.3 Locks 
Beyond thread creation and join, probably the next most useful set of functions provided by the POSIX threads library are those for providing mutual exclusion to a critical section via locks.The most basic pair of routines to use for this purpose is provided by this pair of routines: 
int pthread_mutex_lock(pthread_mutex_t *mutex);int pthread_mutex_unlock(pthread_mutex_t *mutex); 
OPERATING SYSTEMS ARPACI-DUSSEAU 
The routines should be easy to understand and use. When you have a region of code you realize is a critical section,and thus needs to be protected by locks in order to operate as desired. You canprob­ably imagine what the code looks like:  
pthread_mutex_t lock;pthread_mutex_lock(&lock); x= x+1; //orwhatever yourcritical section is pthread_mutex_unlock(&lock);  
The intent of the code is as follows: if no other thread holds the lock when pthread mutex lock() is called, the thread should ac­quire the lock and enter the critical section. If another thread does indeed hold the lock, the thread trying to grab the lock will not re­turn from the call until it has acquired the lock (implying that the thread holding the lock has indeed released it via the unlock call). Of course, many threads may be stuck waiting inside the lock acquisi­tion function at a given time; only the thread with the lock acquired, however, should call unlock. Unfortunately, this code is broken, in two important ways. The .rst problem is a lack of proper initialization. All locks must be prop­erly initialized in order to guarantee that they have the correct values to begin with and thus work as desired when lock and unlock are called.  
With POSIX threads, there are two ways to initialize locks. One way to do this is to use PTHREAD MUTEX INITIALIZER,as follows:  
pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;  
Doing so sets the lock to the default values and thus makes the lock usable. The dynamic way to do it (i.e., at run time) is to make a call to pthread mutex init(),as follows:  
int rc = pthread_mutex_init(&lock, NULL);assert(rc == 0); // always check success!  
The .rst argument to this routine is the address of the lock itself, whereas the second is an optional set of attributes. Read moreabout the attributes yourself; passing NULL in simply uses the defaults. Ei­ther way works, but we usually use the dynamic (latter) method. The second problem with the code above is that it fails to checker­rors code when calling lock and unlock. Just like virtually any library  
THREE  
ARPACI-DUSSEAU  EASY PIECES  
(V0.5)  

// Use this to keep your code clean but check for failures // Only use if exiting program is OK upon failurevoid Pthread_mutex_lock(pthread_mutex_t *mutex) { 
int rc = pthread_mutex_lock(mutex);
assert(rc == 0);
} 

Figure 26.4: An Example Wrapper routine you call in a UNIX system, these routines can also fail! If your code doesn’t properly check error codes, the failure will happen silently, which in this case could allow multiple threads into a criti­cal section. Minimally, use wrappers, which assert that the routine succeeded (e.g., as in Figure 26.4); more sophisticated (non-toy) pro­grams, which can’t simply exit when something goes wrong, should check for failure and do something appropriate when the lock or un­lock does not succeed. The lock and unlock routines are not the only routines that pthreads has to interact with locks. In particular, here are a couple more rou­tines which may be of interest: 
int pthread_mutex_trylock(pthread_mutex_t *mutex);int pthread_mutex_timedlock(pthread_mutex_t *mutex, struct timespec *restrict abs_timeout); 
The trylock version returns failure if the lock is already held. The timedlock version of acquiring a lock returns after a timeout or after acquiring the lock, whichever happens .rst. Thus, the timed lock with a timeout of zero degenerates to the trylock case. Both of these versions should generally be avoided; however, there are a few cases where avoiding getting stuck (perhaps inde.nitely) ina lock acquisition routine can be useful, as we’ll see in future chapters (e.g., when we study deadlock). 

26.4 Condition Variables 
The other major component of any threads library, and certainly the case with POSIX threads, is the presence of a condition variable. Condition variables are useful when some kind of signaling must take place between threads, if one thread is waiting for another to do something before it can continue. Two primary routines are used by programs wishing to interact in this way: 
int pthread_cond_wait(pthread_cond_t *cond, pthread_mutex_t *mutex); 
OPERATING SYSTEMS ARPACI-DUSSEAU 
int pthread_cond_signal(pthread_cond_t *cond); 
To use a condition variable, one has to in addition have a lock that is associated with this condition. When calling either of theabove routines, this lock should be held. 
The .rst routine, pthread 
cond 
wait(),putsthe calling thread to sleep, and thus waits for some other thread to signal it, usually when something in the program has changed that the now-sleeping thread might care about. For example, a typical usage looks like this: 
pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER; pthread_cond_t init = PTHREAD_COND_INITIALIZER; 
Pthread_mutex_lock(&lock);while (initialized == 0)
Pthread_cond_wait(&init, &lock);Pthread_mutex_unlock(&lock); 
In this code, a thread checks to see if the variable initialized has yet been set to something other than zero. If not, the thread sim­ply calls the wait routine in order to sleep until some other thread wakes it. 
The code to wake a thread, which would run in some other thread, looks like this: 
Pthread_mutex_lock(&lock);initialized = 1; Pthread_cond_signal(&init);Pthread_mutex_unlock(&lock); 
Afew things to note about this code sequence. First, when signal­ing (as well as when modifying the global variable initialized), we always make sure to have the lock held. This ensures that we don’t accidentally introduce a race condition into our code. 
Second, you might notice that the wait call takes a lock as its sec­ond parameter, whereas the signal call only takes a condition. The reason for this difference is that the wait call, in addition to putting the calling thread to sleep, releases the lock when putting said caller to sleep. Imagine if it did not: how could the other thread acquire the lock and signal it to wake up? However, before returning after being woken, the pthread 
cond 
wait() re-acquires the lock, thus ensuring that any time the waiting thread is running between the 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
lock acquire at the beginning of the wait sequence, and the lock re­lease at the end, it holds the lock. 
One last oddity: the waiting thread re-checks the condition in a while loop, instead of a simple if statement. We’ll discuss this issue in detail when we study condition variables in a future chapter, but in general, using a while loop is the simple and safe thing to do. Al­though it rechecks the condition (perhaps adding a little overhead), there are some pthread implementations that could spuriously wake up a waiting thread; in such a case, without rechecking, the waiting thread will continue thinking that the condition has changedeven though it has not. It is safer thus to view waking up as a hint that something might have changed, rather than an absolute fact. 
Note that sometimes it is tempting to use a simple .ag to signal between two threads, instead of a condition variable and associated lock. For example, we could rewrite the waiting code above to look more like this in the waiting code: 
while (initialized == 0)
;// spin 

The associated signaling code would look like this: 
initialized = 1; 
Don’t ever do this, for the following reasons. First, it performs poorly in many cases (spinning for a long time just wastes CPU cy­cles). Second, it is error prone. As recent research shows [X+10], it is surprisingly easy to make mistakes when using .ags (as above) to synchronize between threads; roughly half the uses of these ad hoc synchronizations were buggy! Don’t be lazy; use condition variables even when you think you can get away without doing so. 

26.5 Compiling and Running 
All of the code examples in this chapter are relatively easy toget up and running. To compile them, you must include the header pthread.h in your code. On the link line, you must also explic­itly link with the pthreads library, by adding the -lpthread .ag and argument. 
For example, to compile a simple multi-threaded program, allyou have to do is the following: 
OPERATING SYSTEMS ARPACI-DUSSEAU 
prompt> gcc -o main main.c -Wall -lpthread 
As long as main.c includes the pthreads header, you have now 
successfully compiled a concurrent program. Whether it works or 
not, as usual, is a different matter entirely. 

26.6 Summary 
We have introduced the basics of the pthread library, including thread creation, building mutual exclusion via locks, and signaling and waiting via condition variables. You don’t need much elseto write robust and ef.cient multi-threaded code, except patience and agreat deal of care! 
We now end the chapter with a set of tips that might be useful to you when you write multi-threaded code (see the side bar forde­tails). There are other aspects of the API that are interesting; see man -k pthread on a Linux system to see over one hundred APIs that make up the entire interface. However, the basics discussed herein should enable you to build sophisticated (and hopefully, correct and performant) multi-threaded programs. The hard part with threads is not the APIs, but rather the tricky logic of how you build concurrent programs. Read on to learn more. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
DESIGN TIP:USING THE THREAD APIS 
There are a number of small but important things to remember when you 
use the POSIX thread library (or really, any thread library) to build a multi-
threaded program. They are: 
• 	
Keep it simple. Above all else, any code to lock or signal between threads should be as simple as possible. Tricky thread interactions lead to bugs. 

• 	
Minimize thread interactions. Try to keep the number of ways in which threads interact to a minimum. Each interaction shouldbe carefully thought out and constructed with tried and true approaches (many of which we will learn about in the coming chapters). 

• 	
Initialize locks and condition variables. Failure to do so will lead to code that sometimes works and sometimes fails in very strangeways. 

• 	
Check your return codes. Ofcourse,inanyCand UNIX programming you do, you should be checking each and every return code, and it’s true here as well. Failure to do so will lead to bizarre and hardto understand behavior, making you likely to (a) scream, (b) pull some of your hair out, or (c) both. 

• 	
Be careful with how you pass arguments to, and return values from, threads. In particular, any time you are passing a reference to a vari­able allocated on the stack, you are probably doing somethingwrong. 

• 	
Each thread has its own stack. As related to the point above, please re­member that each thread has its own stack. Thus, if you have a locally-allocated variable inside of some function a thread is executing, it is essentially private to that thread; no other thread can (easily) access it. To share data between threads, the values must be in the heap or oth­erwise some locale that is globally accessible. 

• 	
Always use condition variables to signal between threads. While it is often tempting to use a simple .ag, don’t do it. 

• 	
Use the manual pages. On Linux, in particular, the pthread man pages are highly informative and discuss much of the nuances presented here, often in even more detail. Read them carefully! 


OPERATING SYSTEMS ARPACI-DUSSEAU 

References 
[B97] “Programming with POSIX Threads” David R. Butenhof Addison-Wesley, May 1997 
Another one of these books on threads. 
[B+96] “PThreads Programming: 
APOSIX Standard for Better Multiprocessing” 
Dick Buttlar, Jacqueline Farrell, Bradford Nichols 
O’Reilly, September 1996 

Areasonable book from the excellent, practical publishing house O’Reilly. Our bookshelves cer­tainly contain a great deal of books from this company, including some excellent offerings on Perl, Python, and Javascript (particularly Crockford’s “Javascript: The Good Parts”.) 
[K+96] “Programming With Threads” Steve Kleiman, Devang Shah, Bart Smaalders Prentice Hall, January 1996 
Probably one of the better books in this space. Get it at your local library. 
[X+10] “Ad Hoc Synchronization Considered Harmful” Weiwei Xiong, Soyeon Park, Jiaqi Zhang, Yuanyuan Zhou, Zhiqiang Ma OSDI 2010, Vancouver, Canada 
This paper shows how seemingly simple synchronization code can lead to a surprising number of bugs. Use condition variables and do the signaling correctly! 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
27 



Locks 
From the introduction to concurrency, we saw one of the fundamen­tal problems in concurrent programming: we would like to execute a series of instructions atomically, but due to the presence ofinterrupts on a single processor (or multiple threads executing on multiple pro­cessors concurrently), we couldn’t. In this chapter, we thusattack this problem directly, with the introduction of something referred to as a lock.Programmers annotate source code with locks, putting them around critical sections, and thus ensure that any such critical section executes as if it were a single atomic instruction. 
27.1 Locks: The Basic Idea 
As an example, assume our critical section looks like this, the canonical update of a shared variable: 
balance = balance + 1; 
Of course, other critical sections are possible, such as adding an element to a linked list or other more complex updates to shared structures, but we’ll just keep to this simple example for now. To use alock, we add some code around the critical section like this: 
lock_t mutex; // some globally-allocated lock ’mutex’ 
... 
lock(&mutex);
balance = balance + 1; 
unlock(&mutex); 
327 
Alock is just a variable, and thus to use one, you must declare a lock variable of some kind (such as mutex above). This lock vari­able (or just “lock” for short) holds the state of the lock at any in­stant in time. It is either available (or unlocked or free)and thus no thread holds the lock, or acquired (or locked or held), and thus exactly one thread holds the lock and presumably is in a critical sec­tion. We could store other information in the data type as well, such as which thread holds the lock, or a queue for ordering lock acquisi­tion, but information like that is hidden from the user of the lock. 
The semantics of the lock() and unlock() routines are sim­ple. Calling the routine lock() tries to acquire the lock; if no other thread holds the lock (i.e., it is free), the thread will acquire the lock and enter the critical section; this thread is sometimes saidto be the owner of the lock. If another thread then calls lock() on that same lock variable (mutex in this example), it will not return while the lock is held by another thread; in this way, other threads are pre­vented from entering the critical section while the .rst thread that holds the lock is in there. 
Once the owner of the lock calls unlock(),the lock is now avail­able (free) again. If no other threads are waiting for the lock(i.e., no other thread has called lock() and is stuck therein), the state of the lock is simply changed to free. If there are waiting threads (stuck in lock()), one of them will (eventually) notice (or be informed of) this change of the lock’s state, acquire the lock, and enter the critical section. 
Locks provide some minimal amount of control over scheduling to programmers. In general, we view threads as entities created by the programmer but scheduled by the OS, in any fashion that the OS chooses. Locks yield some of that control back to the program­mer; by putting a lock around a section of code, the programmercan guarantee that no more than a single thread can ever be active within that code. Thus locks help transform the chaos that is traditional OS scheduling into a more controlled activity. 

27.2 Pthread Locks 
The name that the POSIX library uses for a lock is a mutex,asit is 
used to provide mutual exclusion between threads, i.e., if one thread 
is in the critical section, it excludes the others from entering until it 
OPERATING SYSTEMS ARPACI-DUSSEAU 
has completed the section. Thus, when you see the following POSIX threads code, you should understand that it is doing the same thing as above (note again that we use our own wrappers that check for errors upon lock and unlock): 
pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER; 
Pthread_mutex_lock(&lock); // wrapper for pthread_mutex_lock() 
balance = balance + 1; 
Pthread_mutex_unlock(&lock); 

You might also notice here that the POSIX version passes a vari­able to lock and unlock, as we may be using different locks to protect different variables. Doing so can increase concurrency: instead of one big lock that is used any time any critical section is accessed (a coarse-grained locking strategy), one will often protect different data and data structures with different locks, thus allowing morethreads to be in locked code at once (a more .ne-grained approach). 

27.3 Building A Lock 
By now, you should have some understanding of how a lock works, 
from the perspective of a programmer. But how should we build a 
lock? What hardware support is needed? What OS support? It is this 
set of questions we address in the rest of this chapter. 
The Crux: HOW TO BUILD A LOCK 
How can we build an ef.cient lock? Ef.cient locks provided mu­tual exclusion at low cost, and also might attain a few other proper­ties we discuss below. What hardware support is needed? What OS support? 
To build a working lock, we will need some help from our old friend, the hardware, as well as our good pal, the OS. Over the years, anumber of different hardware primitives have been added to the instruction sets of various computer architectures; while we won’t study how these instructions are implemented (that, after all, is the topic of a computer architecture class), we will study how to use them in order to build a mutual exclusion primitive like a lock. We 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
will also study how the OS gets involved to complete the picture and enable us to build a sophisticated locking library. 

27.4 Evaluating Locks 
Before building any locks, we should .rst understand what our goals are, and thus we ask how to evaluate the ef.cacy of a partic­ular lock implementation. To evaluate whether a lock works (and works well), we should .rst establish some basic criteria. The .rst is whether the lock does its basic task, which is to provide mutual ex­clusion.Basically, does the lock work, preventing multiple threads from entering a critical section? 
The second is fairness.Does each thread contending for the lock get a fair shot at acquiring it once it is free? Another way to look at this is by examining the more extreme case: does any thread con­tending for the lock starve while doing so, thus never obtaining it? 
The .nal criterion is performance,speci.cally the time overheads added by using the lock. There are a few different cases that are worth considering here. One is the case of no contention; whena single thread is running and grabs and releases the lock, whatis the overhead of doing so? Another is the case where multiple threads are contending for the lock on a single CPU; in this case, are thereper­formance concerns? Finally, how does the lock perform when there are multiple CPUs involved, and threads on each contending for the lock? By comparing these different scenarios, we can better under­stand the performance impact of using various locking techniques, as described below. 

27.5 Controlling Interrupts 
One of the earliest solutions used to provide mutual exclusion 
was to disable interrupts for critical sections; this solution was in­
vented for single-processor systems. The code would look like this: 
void lock() {
DisableInterrupts();}
void unlock() {
EnableInterrupts();
} 

OPERATING SYSTEMS ARPACI-DUSSEAU 
Assume we are running on such a single-processor system. By turning off interrupts (using some kind of special hardware instruc­tion) before entering a critical section, we ensure that the code inside the critical section will not be interrupted, and thus will execute as if it were atomic. When we are .nished, we re-enable interrupts (again, via a hardware instruction) and thus the program proceeds as usual. 
The main positive of this approach is its simplicity. You certainly don’t have to scratch your head too hard to .gure out why this works. Without interruption, a thread can be sure that the code it executes will execute and that no other thread will interfere with it. 
The negatives, unfortunately, are many. First, this approach re­quires us to allow any calling thread to perform a privileged opera­tion (turning interrupts on and off), and thus trust that this facility is not abused. As you already know, any time we are required to trust an arbitrary program, we are probably in trouble. Here, the trouble manifests in numerous ways: a greedy program could call lock() at the beginning of its execution and thus monopolize the processor; worse, an errant or malicious program could call lock() and go into an endless loop. In this latter case, the OS will never regain control of the system, and the only way to address the problem is to restart the system. Thus, using interrupt disabling as a general-purpose syn­chronization solution requires too much trust in applications. 
Second, the approach does not work on multiprocessors. If multi­ple threads are running on different CPUs, and each try to enter the same critical section, it does not matter whether interruptsare dis­abled; threads will be able to run on other processors, and thus could enter the critical section. As multiprocessors are now commonplace, our general solution will have to do better than this. 
Third, and probably least important, this approach can be inef.­cient. Compared to normal instruction execution, code that masks or unmasks interrupts tends to be executed slowly by modern CPUs. 
For these reasons, turning off interrupts is only used in limited contexts as a mutual-exclusion primitive. For example, in some cases an operating system itself will sometimes use interrupt masking to guarantee atomicity when accessing its own data structures,or at least to prevent certain messy interrupt handling situations from aris­ing. This usage makes sense, as the trust issue disappears inside the OS, which always trusts itself to perform privileged operations any­how. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
ASIDE:DEKKER AND PETERSON’S ALGORITHMS 
In the 1960’s, Dijkstra posed the concurrency problem to his friends, and one of them, a mathematician named Theodorus Jozef Dekker, came up with a solution [D68]. Unlike the solutions wedis­cuss here, which use special hardware instructions and even OS sup­port, Dekker’s approach uses just loads and stores (assumingthey are atomic with respect to each other). 
Dekker’s approach was later re.ned by Peterson [P81] (and thus “Peterson’s algorithm”), shown here. Once again, just loadsand stores are used, and the idea is to ensure that two threads never enter acritical section atthe same time. Here is Peterson’s algorithm (for two threads); see if you can understand it. 
int flag[2];
int turn; 

void init() {flag[0] = flag[1] = 0; // 1->thread wants to grab lock turn = 0; // whose turn? (thread 0 or 1?)
}
void lock() {flag[self] = 1; // self: thread ID of caller turn = 1 -self; // make it other thread’s turn while ((flag[1-self] == 1) && (turn == 1 -self))
;//spin-wait }
void unlock() {
flag[self] = 0; // simply undo your intent } 
For some reason, developing locks that work without special hardware support became all the rage for a while, giving theory-types a lot of problems to work on. Of course, this all became quite useless when people realized it is much easier to assume a little hard­ware support (and indeed that support had been around from the very earliest days of multiprocessing). Further, algorithms like the ones above don’t work on modern hardware (due to relaxed memory consistency models), thus making them even less useful than they were before. Yet more research relegated to the dustbin of history... 
OPERATING SYSTEMS ARPACI-DUSSEAU 
typedef struct __lock_t { int flag; } lock_t; 
void init(lock_t *mutex) { 
// 0 -> lock is available, 1 -> held 
mutex->flag = 0; 

} 
void lock(lock_t *mutex) {
while (mutex->flag == 1) // TEST the flag 
;// spin-wait(donothing)
mutex->flag = 1; // now SET it! 
} 

void unlock(lock_t *mutex) { 
mutex->flag = 0; 

} 
Figure 27.1: First Attempt: A Simple Flag 

27.6 Test And Set (Atomic Exchange) 
Because disabling interrupts does not work on multiple proces­
sors, system designers started to invent hardware support for lock­
ing. The earliest multiprocessor systems, such as the Burroughs B5000 
in the early 1960’s [M82], had such support; today all systemspro­
vide this type of support, even for single CPU systems. 
The simplest bit of hardware support to understand is what is known as a test-and-set instruction,also known as atomic exchange. To understand how test-and-set works, let’s .rst try to builda simple lock without it. In this failed attempt, we use a simple .ag variable to denote whether the lock is held or not. 
In this .rst attempt (Figure 27.1), the idea is quite simple: use a simple variable to indicate whether some thread has possession of a lock. The .rst thread that enters the critical section will call lock(), which tests whether the .ag is equal to 1 (in this case, it is not), and then sets the .ag to 1 to indicate that the thread now holds the lock. When .nished with the critical section, the thread calls unlock() and clears the .ag, thus indicating that the lock is no longer held. 
If another thread happens to call lock() while that .rst thread is in the critical section, it will simply spin-wait in the while loop for that thread to call unlock() and clear the .ag. Once that .rst thread does so, the waiting thread will fall out of the while loop, setthe .ag to 1 for itself, and proceed into the critical section. 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
Unfortunately, this piece of code has two problems: one of correct­ness, and another of performance. The correctness problem issimple to see once you get used to thinking about concurrent programming. Imagine the code interleaving as seen below (assume we start in the state flag=0). 
Thread 0 Thread 1 
call lock()
while (flag == 1) // flag=0->continue
[INTERRUPT, SWITCH TO THREAD 1] 

call lock()
while (flag == 1) // flag=0 ... 
flag = 1; // set flag to1 
[INTERRUPT, SWITCH TO THREAD 0] 

flag = 1; // set flag to 1 (too!) 
As you can see from this interleaving, with timely (untimely?) in­terrupts, we can easily produce a case where both threads set their .ags to 1 and both threads are thus able to enter the critical section. This is bad! We have obviously failed to provide the most basicre­quirement: providing mutual exclusion. 
The performance problem, which we will address more later on, is the fact that the way a thread waits to acquire a lock that is already held: it endlessly checks the value of .ag, a technique known as spin-waiting.Spin-waiting wastes time waiting for another thread to release a lock. The waste is exceptionally high on a uniprocessor, where the thread that the waiter is waiting for cannot even run(at least, until a context switch occurs)! Thus, as we move forward and develop more sophisticated solutions, we should also consider ways to avoid this kind of waste. 
DESIGN TIP:THINKING ABOUT CONCURRENCY 
What we also get from this example is a sense of the approach we need to take when trying to understand concurrent execution.What you are really trying to do is to pretend you are a malicious sched­uler,one that interruptsthreads at the most inopportune of timesin order to foil their feeble attempts at building synchronization prim­itives. Although the exact sequence of interrupts may be improbable, it is possible,andthat is all we need to show to demonstrate that a particular approach does not work. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

27.7 Building A Working Spin Lock 
While the idea behind the example above is a good one, it is not possible to implement without some support from the hardware. Fortunately, some systems provide an instruction to supportthe cre­ation of simple locks based on this concept. This more powerful in­struction has different names – on SPARC, it is the load/storeun­signed byte instruction (ldstub), whereas on x86, it is the atomic ex­change instruction (xchg)– but basically does the same thing across platforms, and is usually generally referred to as test-and-set.We de.ne what the test-and-set instruction does with the following C code snippet: 
int TestAndSet(int *ptr, int new) { int old = *ptr; // fetch old value at ptr *ptr = new; // store ’new’ into ptr return old; // return the old value 
} 
What the test-and-set instruction does is as follows. It returns the 
old value pointed to by the ptr,and simultaneously updates said 
value to new.The key, of course, is that this sequence of operations 
is performed atomically1.The reason it is called “test and set” is that it enables you to “test” the old value (which is what is returned) while simultaneously “setting” the memory location to a new value; as it turns out, this slightly more powerful instruction is enough to build a simple spin lock,as we now examine in Figure 27.2. 
Let’s make sure we understand why this works. Imagine .rst the case where a thread calls lock() and no other thread currently holds the lock; thus, flag should be 0. When the thread then calls TestAndSet(flag, 1),the routine will return the old value of flag,which is0; thus, the calling thread,which is testing the value of .ag, will not get caught spinning in the while loop and will acquire the lock. The thread will also atomically set the value to 1, thus indi­cating that the lock is now held. When the thread is .nished with its critical section, it calls unlock() to set the .ag back to zero. 
The second case we can imagine arises when one thread already has the lock held (i.e., flag is 1). In this case, this thread will call lock() and then call TestAndSet(flag, 1) as well. This time, however, TestAndSet() will return the old value at .ag, which is 1 (because the lock is held), while simultaneously setting it to 1 again. 
1
How does the hardware do this? Take a hardware class and .nd out! 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
typedef struct __lock_t { 
int flag;
}lock_t; 

void init(lock_t *lock) {// 0 indicates that lock is available, 1 that it is heldlock->flag = 0; 
} 
void lock(lock_t *lock) {while (TestAndSet(&lock->flag, 1) == 1);//spin-wait (donothing) } 
void unlock(lock_t *lock) { 
lock->flag = 0; 
} 

Figure 27.2: A Simple Spin Lock using Test-and-set As long as the lock is held by another thread, TestAndSet() will repeatedly return 1, and thus this thread will spin and spin until the lock is .nally released. When the .ag is .nally set to 0 by some other thread, this thread will call TestAndSet() again, which will now return 0 while atomically setting the value to 1 and thus acquire the lock and enter the critical section. By making both the test (of the old value of the lock) and set (of the new value) a single atomic operation, we can ensure that only one thread acquires the lock. Thus we can build a successful mutual exclusion primitive! You may also now understand why this type of lock is usually re­ferred to as a spin lock.It is the simplest type of lock to build, and simply spins, using CPU cycles, until the lock becomes available. To work correctly on a single processor, it requires a preemptive sched­uler (i.e., one that will interrupt a thread via a timer, in order torun a different thread, from time to time). Without preemption, spin locks don’t make much sense on a single CPU, as a thread spinning on a CPU will never relinquish it. 

27.8 Evaluating Spin Locks 
Given our basic spin lock, we can now evaluate how effective itis along our previously described axes. The most important aspect of alock is correctness: does itprovide mutual exclusion? The answer here is obviously yes: the spin lock only allows a single thread to 
OPERATING SYSTEMS ARPACI-DUSSEAU 
enter the critical section at a time. Thus, we have a correct lock. 
The next axis is fairness. How fair is a spin lock to a waiting thread? Can you guarantee that a waiting thread will ever enter the critical section? The answer here, unfortunately, is bad news: spin locks don’t provide any fairness guarantees. Indeed, a thread spin­ning may spin forever, under contention. Thus, spin locks arenot fair and may indeed lead to starvation. 
The .nal axis is performance. What are the costs of using a spin lock? To analyze this more carefully, we suggest thinking about a few different cases. In the .rst, imagine threads competing for the lock on a single processor; in the second, consider the threads as spread out across many processors. 
For spin locks, in the single CPU case, performance overheadscan be quite painful; imagine the case where the thread holding the lock is pre-empted within a critical section. The scheduler mightthen run every other thread (imagine there are N - 1 others), each of which tries to acquire the lock. In this case, each of those threads will spin for the duration of a time slice before giving up the CPU, whichis quite a waste of CPU cycles. 
However, on multiple CPUs, spin locks work reasonably well (if the number of threads roughly equals the number of CPUs). The thinking goes as follows: imagine Thread A on CPU 1 and Thread Bon CPU 2, both contending for a lock. If ThreadA (CPU 1) grabs the lock, and then Thread B tries to, B will spin (on CPU 2). How­ever, presumably the critical section is short, and thus soonthe lock becomes available, and is acquired by Thread B. Spinning to wait for alock held on another processor doesn’twaste many cycles in this case, and thus can be quite effective. 

27.9 Compare-And-Swap 
Another hardware primitive that some systems provide is known as the compare-and-swap instruction (as it is called on SPARC, for example), or compare-and-exchange (as it called on x86). The C pseudocode for this single instruction is found in Figure 27.3. 
The basic idea is for compare-and-swap to test whether the value at the address speci.ed by ptr is equal to expected;if so,update the memory location pointed to by ptr with the new value. If not, do nothing. In either case, return the actual value at that memory 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
int CompareAndSwap(int *ptr, int expected, int new) { int actual = *ptr;if (actual == expected) 
*ptr = new; 
return actual; 
} 

Figure 27.3: Compare-and-swap 
location, thus allowing the code calling compare-and-swap to know whether it succeeded or not. 
With the compare-and-swap instruction, we can build a lock in amanner quite similar to that with test-and-set. For example, we could just replace the lock() routine above with the following: 
void lock(lock_t *lock) {while (CompareAndSwap(&lock->flag, 0, 1) == 1);//spin } 
The rest of the code is the same as the test-and-set example above. This code works quite similarly; it simply checks if the .ag is0 and if so, atomically swaps in a 1 thus acquiring the lock. Threads that try to acquire the lock while it is held will get stuck spinning until the lock is .nally released. 
If you want to see how to really make a C-callable x86-version of compare-and-swap, this code sequence might be useful (stolen from here [S05]): 
char CompareAndSwap(int *ptr, int old, int new) { unsigned char ret; 
// Note that sete sets a ’byte’ not the word 
__asm__ 	__volatile__ (" lock\n" " cmpxchgl %2,%1\n" " sete %0\n" :"=q" (ret), "=m" (*ptr):"r" (new), "m"(*ptr), "a" (old) :"memory"); 
return ret; 
} 

Finally, as you may have sensed, compare-and-swap is a more powerful instruction than test-and-set. We will make some use of 
OPERATING SYSTEMS ARPACI-DUSSEAU 
int LoadLinked(int *ptr) { 
return *ptr;

} 
int StoreConditional(int *ptr, int value) { 
if (no one has updated *ptr since the LoadLinked to this address) { *ptr = value; return 1; // success! 
}else { 
return 0; // failed to update
}
} 

Figure 27.4: Load-linked and Store-conditional 
this power in the future when we brie.y delve into wait-free syn­
chronization [H91]. However, if we just build a simple spin lock 
with it, its behavior is identical to the spin lock we analyzedabove. 

27.10 Load-Linked and Store-Conditional 
Some platforms provide a pair of instructions that work in concert to help build critical sections. On the MIPS architecture [H93], for example, the load-linked and store-conditional instructions can be used in tandem to build locks and other concurrent structures. The C pseudocode for these instructions is as found in Figure 27.4.Alpha, PowerPC, and ARM provide similar instructions [W09]. 
The load-linked operates much like a typical load instruction, and simply fetches a value from memory and places it in a register.The key difference comes with the store-conditional, which onlysucceeds (and updates the value stored at the address just load-linkedfrom) if no intermittent store to the address has taken place. In thecaseof success, the store-conditional returns 1 and updates the value at ptr to value;if it fails,the value at ptr is not updated and 0 is returned. 
As a challenge to yourself, try thinking about how to build a lock using load-linked and store-conditional. Then, when you are.n­ished, look at the code below which provides one simple solution. Do it! The solution is in Figure 27.5. 
The lock() code is the only interesting piece. First, a thread spins waiting for the .ag to be set to 0 (and thus indicate the lock is not held). Once so, the thread tries to acquire the lock via the store-conditional; if it succeeds, the thread has atomically changed the .ag’s value to 1 and thus can proceed into the critical section. 
Note how failure of the store-conditional might arise. One thread 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
void lock(lock_t *lock) { while (1) {while (LoadLinked(&lock->flag) == 1);// spinuntil it’szeroif (StoreConditional(&lock->flag, 1) == 1)return; // if set-it-to-1 was a success: all done // otherwise: try it all over again}} 
void unlock(lock_t *lock) { 
lock->flag = 0; 

} 
Figure 27.5: Using LL/SC To Build A Lock 
calls lock() and executes the load-linked, returning 0 as the lock is not held. Before it can attempt the store-conditional, it is inter­rupted and another thread enters the lock code, also executing the load-linked instruction, and also getting a 0 and continuing. At this point, two threads have each executed the load-linked and each are about to attempt the store-conditional. The key feature of these in­structions is that only one of these threads will succeed in updating the .ag to 1 and thus acquire the lock; the second thread to attempt the store-conditional will fail (because the other thread updated the value of .ag between its load-linked and store-conditional)and thus have to try to acquire the lock again. 
In class a few years ago, undergraduate student David Capel sug­gested a more concise form of the above, for those of you who enjoy short-circuiting boolean conditionals. See if you can .gureout why it is equivalent. It certainly is shorter! 
void lock(lock_t *lock) {while (LoadLinked(&lock->flag)||!StoreConditional(&lock->flag, 1));// spin } 

27.11 Fetch-And-Add 
One .nal hardware primitive is the fetch-and-add instruction, which atomically increments a value while returning the old value at a particular address. The C pseudocode for the fetch-and-add in­struction looks like this: 
OPERATING SYSTEMS ARPACI-DUSSEAU 
int FetchAndAdd(int *ptr) { 
int old = *ptr; 
*ptr = old + 1; 
return old; } 
In this example, we’ll use fetch-and-add to build a more interest­ing ticket lock,as introduced by Mellor-Crummey and Scott [MS91]. The lock and unlock code looks like what you see in Figure 27.6. 
Instead of a single value, this solution uses a ticket and turnvari­able in combination to build a lock. The basic operation is pretty simple: when a thread wishes to acquire a lock, it .rst does an atomic fetch-and-add on the ticket value; that value is now considered this thread’s “turn” (myturn). The globally shared lock->turn is then used to determine which thread’s turn it is; when (myturn == turn) for a given thread, it is that thread’s turn to enter the critical section. Unlock is accomplished simply by incrementing the turn such that the next waiting thread (if there is one) can now enter the critical section. 
Note one important difference with this solution versus our pre­vious attempts: it ensures progress for all threads. Once a thread is assigned its ticket value, it will be scheduled at some point in the fu­ture (once those in front of it have passed through the critical section and released the lock). In our previous attempts, no such guarantee existed; a thread spinning on test-and-set (for example) could spin forever even as other threads acquire and release the lock. 
CODING TIP:LESS CODE IS BETTER CODE Programmers tend to brag about how much code they wrote to do something. Doing so is fundamentally broken. What one should brag about, rather, is how little code one wrote to accomplish a given task. Short, concise code is always preferred; it is likely easier to un­derstand and has fewer bugs. As Hugh Lauer said, when discussing the construction of the Pilot operating system: “If the same people had twice as much time, they could produce as good of a system in half the code.” [L81] We’ll call this Lauer’s Law,and it is well worth remembering. So next time you’re bragging about how much code you wrote to .nish the assignment, think again, or betteryet, go back, rewrite, and make the code as clear and concise as possible. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
typedef struct __lock_t {
int ticket; 
int turn; 

}lock_t; 
void lock_init(lock_t *lock) { 
lock->ticket = 0; 
lock->turn = 0; 

} 
void lock(lock_t *lock) { int myturn = FetchAndAdd(&lock->ticket);while (lock->turn != myturn)
;//spin 
} 

void unlock(lock_t *lock) {
FetchAndAdd(&lock->turn);

} 
Figure 27.6: Ticket Locks 

27.12 Summary: So Much Spinning 
Our simple hardware-based locks are simple (only a few lines of code) and they work (you could even prove that if you’d like to,by writing some code), which are two excellent properties of anysys­tem or code. However, in some cases, these solutions can be quite inef.cient. Imagine you are running two threads on a single proces­sor. Now imagine that one thread (thread 0) is in a critical section and thus has a lock held, and unfortunately gets interrupted.The second thread (thread 1) now tries to acquire the lock, but .nds that it is held. Thus, it begins to spin. And spin. Then it spins somemore. And .nally, a timer interrupt goes off, thread 0 is run again, which re­leases the lock, and .nally (the next time it runs, say), thread 1 won’t have to spin so much and will be able to acquire the lock. Thus, any time a thread gets caught spinning in a situation like this, itwastes an entire time slice doing nothing but checking a value that isn’t go­ing to change! The problem gets worse with N threads contending for a lock; N - 1 time slices may be wasted in a similar manner, sim­ply spinning and waiting for a single thread to release the lock. And thus, our next problem: 
OPERATING SYSTEMS ARPACI-DUSSEAU 

THE CRUX:HOW TO AVOID SPINNING How can we develop a lock that doesn’t needlessly waste time spinning on the CPU? 
Hardware support alone cannot solve the problem. We’ll need OS support too! Let’s now .gure out just how that might work. 

27.13 A Simple Approach: Just Yield, Baby 
Hardware support got us pretty far: working locks, and even (as with the case of the ticket lock) fairness in lock acquisition. However, we still have a problem: what to do when a context switch occursin acritical section, and threads startto spin endlessly, waiting for the interrupt (lock-holding) thread to be run again? 
Our .rst try is a simple and friendly approach: when you are going to spin, instead give up the CPU to another thread. Or, as Al Davis might say, “just yield, baby!” [D91]. Figure 27.7 presents the approach. 
void init() {
flag = 0; 

} 
void lock() {
while (TestAndSet(&flag, 1) == 1)
yield(); // give up the CPU 
} 

void unlock() {
flag = 0; 

} 
Figure 27.7: Lock with test-and-set and yield 
In this approach, we assume an operating system primitive yield() 
which a thread can call when it wants to give up the CPU and let 
another thread run. Because a thread can be in one of three states 
(running, ready, or blocked), you can think of this as an OS system 
call that moves the caller from the running state to the ready state, 
and thus promotes another thread to running. 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
Think about the example with two threads on one CPU; in this case, our yield-based approach works quite well. If a thread happens to call lock() and .nd a lock held, it will simply yield the CPU, and thus the other thread will run and .nish its critical section.In this simple case, the yielding approach works well. 
Let us now consider the case where there are many threads (say 100) contending for a lock repeatedly. In this case, if one thread ac­quires the lock and is preempted before releasing it, the other 99 will each call lock(),.ndthe lock held,and yield the CPU.Assuming some kind of round-robin scheduler, each of the 99 will execute this run-and-yield pattern before the thread holding the lock gets to run again. While better than our spinning approach (which would waste 99 time slices spinning), this approach is still costly; the cost of a con­text switch can be substantial, and there is thus plenty of waste. 
Worse, we have not tackled the starvation problem at all. A thread may get caught in an endless yield loop while other threads repeat­edly enter and exit the critical section. We clearly will needan ap­proach that addresses this problem directly. 

27.14 Using Queues: Sleeping Instead Of Spinning 
The real problem with our previous approaches is that they leave too much to chance. The scheduler determines which thread runs next; if the scheduler makes a bad choice, a thread runs that must either spin waiting for the lock (our .rst approach), or yieldthe CPU immediately (our second approach). Either way, there is potential for waste and no prevention of starvation. 
Thus, we must explicitly exert some control over who gets to ac­quire the lock next after the current holder releases it. To dothis, we will need a little more OS support, as well as a queue to keep track of which threads are waiting to enter the lock. 
For simplicity, we will use the support provided by Solaris, in terms of two calls: park() to put a calling thread to sleep, and unpark(threadID) to wake a particular thread as designated by threadID.These two routines can be used in tandem to build a lock that puts a caller to sleep if it tries to acquire a held lock andwakes it when the lock is free. Let’s look at the code in Figure 27.8 to under­stand one possible use of such primitives. 
We do a couple of interesting things in this example. First, we 
OPERATING SYSTEMS ARPACI-DUSSEAU 
typedef struct __lock_t { int flag;int guard; queue_t *q;
}lock_t; 
void lock_init(lock_t *m) { m->flag = 0; m->guard = 0; queue_init(m->q);
} 
void lock(lock_t *m) {while (TestAndSet(&m->guard, 1) == 1);//acquireguardlock byspinning
if (m->flag == 0) { 
m->flag = 1; // lock is acquired 
m->guard = 0; 

}else {
queue_add(m->q, gettid());
m->guard = 0; 
park();

}} 
void unlock(lock_t *m) {while (TestAndSet(&m->guard, 1) == 1);//acquireguardlock byspinningif (queue_empty(m->q))m->flag = 0; // let go of lock; no one wants it else unpark(queue_remove(m->q)); // hold lock (for next thread!) m->guard = 0; } 
Figure 27.8: Lock with Queues, test-and-set, yield, and wakeup 
combine the old test-and-set idea with an explicit queue of lock wait­ers to make a more ef.cient lock. Second, we use a queue to help control who gets the lock next and thus avoid starvation. 
You might notice how the guard is used, basically as a spin-lock around the .ag and queue manipulations the lock is using. This approach thus doesn’t avoid spin-waiting entirely; a threadmight be interrupted while acquiring or releasing the lock, and thus cause other threads to spin-wait for this one to run again. However,the time spent spinning is quite limited (just a few instructionsinside 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
the lock and unlock code, instead of the user-de.ned criticalsection), and thus this approach may be a reasonable one. 
Second, you might notice that in lock(),when a threadcan not acquire the lock (it is already held), we are careful to add ourselves to a queue (by calling the gettid() call to get the thread ID of the current thread), set guard to 0, and yield the CPU. A question for the reader: What would happen if the release of the guard lock came after the park(),and not before? Hint: something bad. 
You might also notice the interesting fact that the .ag does not get set back to 0 when when another thread gets woken up. Why is this? Well, it is not an error, but rather a necessity! When a thread is woken up, it will be as if it is returning from park();however, it does not hold the guard at that point in the code and thus cannot even tryto set the .ag to 1. Thus, we just pass the lock directly from the thread releasing the lock to the next thread acquiring it; .ag is not set to 0 in-between. 
Finally, you might notice the perceived race condition in theso­lution, just before the call to park().With just the wrong timing, athread will be about to park, assuming that itshould sleep until the lock is no longer held. A switch at that time to another thread (say, a thread holding the lock) could lead to trouble, for example, if that thread then released the lock. The subsequent park by the .rst thread would then sleep forever (potentially). This problem is sometimes called the wakeup/waiting race;to avoid it, we need to do some extra work. 
Solaris solves this problem by adding a third system call: setpark(). 
By calling this routine, a thread can indicate it is about to park. If it 
then happens to be interrupted and another thread calls unpark be­
fore park is actually called, the subsequent park returns immediately 
instead of sleeping. The code modi.cation, inside of lock(),isquite 
small: 
queue_add(m->q, gettid());setpark(); // new code m->guard = 0; 
Adifferent solution could pass the guard into the kernel. In that 
case, the kernel could take precautions to atomically release the lock 
and dequeue the running thread. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
void mutex_lock (int *mutex) { unsigned int v;/* Bit 31 was clear, we got the mutex (this is the fastpath) if (atomic_bit_test_set (mutex, 31) == 0) return; atomic_increment (mutex);while (1) {if (atomic_bit_test_set (mutex, 31) == 0) { atomic_decrement (mutex); return; }/* We have to wait now. First make sure the futex value we are monitoring is truly negative (i.e. locked). */ v= *mutex; if (v >= 0)continue; futex_wait (mutex, v); }}  */  
void mutex_unlock (int *mutex) { /* Adding 0x80000000 to the counter results in 0 if and only if there are not other interested threads */if (atomic_add_zero (mutex, 0x80000000)) return;  
/* There are other threads waiting for this mutex, wake one of them up. */futex_wake (mutex);  
Figure 27.9: Linux-based Futex Locks  
27.15  Different OS, Different Support  
We have thus far seen one type of support that an OS can provide in order to build a more ef.cient lock in a thread library. Other OS’s provide similar support; the details vary. For example, Linux provides something called a futex which is similar to the Solaris interface but provides a bit more in-kernel func­tionality. Speci.cally, each futex has associated with it a speci.c physical memory location; associated with each such memory lo­cation is an in-kernel queue. Callers can use futex calls (described below) to sleep and wake as need be. Speci.cally, two calls are available. The call to futex wait(address,expected) puts the calling thread to sleep, assuming the value at  
ARPACI-DUSSEAU  THREE EASY PIECES (V0.5)  

address is equal to expected.If it is not equal, the call returns immediately. The call to the routine futex wake(address) wakes one thread that is waiting on the queue. The usage of these in Linux is as found in 27.9. 
This code snippet from lowlevellock.h in the nptl library (part of the gnu libc library) [L09] is pretty interesting. Basically, it uses a single integer to track both whether the lock is held or not (the high bit of the integer) and the number of waiters on the lock (all the other bits). Thus, if the lock is negative, it is held (because the high bit is set and that bit determines the sign of the integer). The code is also interesting because it shows how to optimize for the common case where there is no contention: with only one thread acquiring and releasing a lock, very little work is done (the atomic bit test-and-set to lock and an atomic add to release the lock). See if you can puzzle through the rest of this “real-world” lock to see how it works. 

27.16 Two-Phase Locks 
One .nal note: the Linux approach has the .avor of an old ap­proach that has been used on and off for years, going at least asfar back to Dahm Locks in the early 1960’s [M82], and is now referred to as a two-phase lock.A two-phase lock realizes that spinning can be useful, particularly if the lock is about to be released. So inthe .rst phase, the lock spins for a while, hoping that it can acquire the lock. 
However, if the lock is not acquired during the .rst spin phase, asecond phase is entered, where the caller is putto sleep, andonly woken up when the lock becomes free later. The Linux lock aboveis aform of such alock, but itonly spins once; a generalization of this could spin in a loop for a .xed amount of time before using the futex support in the kernel to sleep. 
Two-phase locks are yet another instance of a hybrid approach, where combining two good ideas may indeed yield a better one. Of course, whether it does depends strongly on many things, including the hardware environment, number of threads, and other workload details. As always, making a single general-purpose lock, good for all possible use cases, is quite a challenge. 
OPERATING SYSTEMS ARPACI-DUSSEAU 


27.17 Summary 
The above approach shows how real locks are built these days: some hardware support (in the form of a more powerful instruction) plus some operating system support (e.g., in the form of park() and unpark() primitives on Solaris, or futex on Linux). Of course, the details differ, and the exact code to perform such locking is usually highly tuned. Check out the Solaris or Linux open source code bases if you want to see more details; they are a fascinating read [L09, S09]. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

References 
[D91] “Just Win, Baby: Al Davis and His Raiders” Glenn Dickey, Harcourt 1991 
There is even an undoubtedly bad book about Al Davis and his famous “just win” quote. Or, I guess, the book is more about Al Davis and the Raiders, and maybe not just the quote. Read the book to .nd out? 
[D68] “Cooperating sequential processes” Edsger W. Dijkstra, 1968 Available: http://www.cs.utexas.edu/users/EWD/ewd01xx/EWD123.PDF 
One of the early seminal papers in the area. Discusses how Dijkstra posed the original concur­rency problem, and Dekker’s solution. 
[H93] “MIPS R4000 Microprocessor User’s Manual”. 
Joe Heinrich, Prentice-Hall, June 1993 
Available: http://cag.csail.mit.edu/raw/ 
documents/R4400 Uman book Ed2.pdf 

[H91] “Wait-free Synchronization” 
Maurice Herlihy 
ACM Transactions on Programming Languages and Systems (TOPLAS) 
Volume 13, Issue 1, January 1991 

Alandmark paper introducing a different approach to building concurrent data structures. How­ever, because of the complexity involved, many of these ideashave been slow to gain acceptance in deployed systems. 
[L81] “Observations on the Development of an Operating System” Hugh Lauer SOSP ’81 
Amust-read retrospective about the development of the PilotOS, an early PC operating system. Fun and full of insights. 
[L09] “glibc 2.9 (include Linux pthreads implementation)” Available: http://ftp.gnu.org/gnu/glibc/ 
In particular, take a look at the nptl subdirectory where you will .nd most of the pthread support in Linux today. 
[M82] “The Architecture of the Burroughs B5000 
20 Years Later and Still Ahead of the Times?” 
Alastair J.W. Mayer, 1982 
www.ajwm.net/amayer/papers/B5000.html 

From the paper: “One particularly useful instruction is the RDLK (read-lock). It is an indivisible operation which reads from and writes into a memory location.” RDLK is thus an early test-and­set primitive, if not the earliest. Some credit here goes to anengineer named Dave Dahm, who apparently invented a number of these things for the Burroughs systems, including a form of spin locks (called “Buzz Locks” as well as a two-phase lock eponymously called “Dahm Locks.”) 
OPERATING SYSTEMS ARPACI-DUSSEAU 



[MS91] “Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors” John M. Mellor-Crummey and M. L. Scott ACM TOCS, February 1991 
An excellent survey on different locking algorithms. However, no OS support is used, just fancy hardware instructions. 
[P81] “Myths About the Mutual Exclusion Problem” 
G.L. Peterson Information Processing Letters. 12(3) 1981, 115–116 
Peterson’s algorithm introduced here. 
[S05] “Guide to porting from Solaris to Linux on x86” 
Ajay Sood, April 29, 2005 
Available: http://www.ibm.com/developerworks/linux/library/l-solar/ 

[S09] “OpenSolaris Thread Library” 
Available: http://src.opensolaris.org/source/xref/onnv/onnv-gate/ 
usr/src/lib/libc/port/threads/synch.c 

This is also pretty interesting to look at, though who knows what will happen to it now that Oracle owns Sun. Thanks to Mike Swift for the pointer to the code. 
[W09] “Load-Link, Store-Conditional” 
Wikipedia entry on said topic, as of October 22, 2009 

http://en.wikipedia.org/wiki/Load-Link/Store-Conditional 
Can you believe I referenced wikipedia? Pretty shabby. But, Ifound the information there .rst, and it felt funny not to cite it. Further, they even listed the instructions for the different ar­chitectures: ldl l/stl c and ldq l/stq c (Alpha), lwarx/stwcx (PowerPC), ll/sc (MIPS), and ldrex/strex (ARM version 6 and above). 
[WG00] “The SPARC Architecture Manual: Version 9” David L. Weaver and Tom Germond, September 2000 SPARC International, San Jose, California Available: http://www.sparc.org/standards/SPARCV9.pdf 

Also see: http://developers.sun.com/solaris/articles/atomic sparc/ for some more details on Sparc atomic operations. 




THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
28 



Using Locks (INCOMPLETE) 
Before moving beyond locks, we’ll .rst describe how to use locks in some common data structures. Hopefully this will help you, dear reader, understand how such locks are used in practice. 
28.1 Concurrent Counter 
One of the simplest data structures is a counter. We de.ne it as follows: 
typedef struct __counter_t { 
int value; 

}counter_t; 
void init(counter_t *c) { 
c->value = 0; 

} 
void increment(counter_t *c) { 
c->value++; 

} 
void decrement(counter_t *c) { 
c->value--; 

} 
int get(counter_t *c) { 
return c->value; 

} 
353 
This simple counter is relatively easy to make thread safe. now do so.  We  
typedef struct __counter_t { int value; pthread_lock_t lock;}counter_t;  
void init(counter_t *c) { c->value = 0; Pthread_mutex_init(&lock, NULL); }  
void increment(counter_t *c) {Pthread_mutex_lock(&lock);c->value++; Pthread_mutex_unlock(&lock);}  
void decrement(counter_t *c) {Pthread_mutex_lock(&lock);c->value--; Pthread_mutex_unlock(&lock);}  
int get(counter_t *c) {Pthread_mutex_lock(&lock);int rc = c->value; Pthread_mutex_unlock(&lock); return rc; }  
28.2  Concurrent Linked List  
#include <stdio.h> #include <stdlib.h>  
typedef struct __node_t { int key; struct __node_t *next; }node_t;  
typedef struct __list_t { node_t *head; pthread_mutex_t lock; }list_t;  
OPERATING SYSTEMS  ARPACI-DUSSEAU  

void List_Init(list_t *L) { 
L->head = NULL; 

} 
void List_Insert(list_t *L, int key) { 
node_t *new = malloc(sizeof(node_t));
if (new == NULL) { perror("malloc"); return; } 
new->key = key; 
new->next = L->head; 
L->head = new; 

} 
design tip: be careful placing locks around exit(), functionreturn 
statements, and other control .ow changes. (reference bug .nding 
papers) 

28.3 Concurrent Hash Table 
DESIGN TIP:LOCK GRANULARITY 
One of the key aspects in designing multi-threaded data struc­tures is how you decide to protect them under concurrent access. One big lock only allows one thread to access the data structure at atime, and thus limits concurrency (the coarse-grained approach). Having more locks allows more threads to access the data structure, but is usually more complex (the .ne-grained approach). A typical example is a hash table, where you can have one lock for the en­tire table or perhaps one lock per hash bucket. Thus, when thinking about multi-thread safe data structures, the .rst thing you should think about is whether you need concurrent access for the sakeof performance. If you do, you’ll need to think about more sophisti­cated, .ne-grained locking approaches. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
29 



Condition Variables 
Thus far we have developed the notion of a lock and seen how one can be properly built with the right combination of hardware and OS support. Unfortunately, locks are not the only primitivesthat are needed to build concurrent programs. 
In particular, there are many cases where a thread wishes to check whether a condition is true before continuing its execution. For ex­ample, a parent thread might wish to check whether a child thread has completed before continuing (this is often called a join()); how should such a wait be implemented? Let’s look at Figure 29.1. 
1 void * 
2 child(void *arg) { 
3 printf("child\n"); 
4 // XXX how to indicate we are done? 
5 return NULL; 
6 } 
7 
8 int 
9 main(int argc, char *argv[]) { 

10 printf("parent: begin\n"); 
11 pthread_t c; 
12 Pthread_create(&c, NULL, child, NULL); // create child 
13 // XXX how to wait for child? 
14 printf("parent: end\n"); 
15 return 0; 
16 } 
Figure 29.1: A Parent Waiting For Its Child 
357 
What we would like to see here is the following output: 
parent: begin
child 
parent: end 

We could try using a shared variable, as you see in Figure 29.2. This solution will generally work, but it is hugely inef.cient as the parent spins and wastes CPU time. What we would like here instead is some way to put the parent to sleep until the condition we are waiting for (e.g., the child is done executing) comes true. 
THE CRUX:HOW TO WAIT FOR A CONDITION 
In multi-threaded programs, it is often useful for a thread towait for some condition to become true before proceeding. The simple ap­proach, of just spinning until the condition becomes true, isgrossly inef.cient and wastes CPU cycles, and in some cases, can be incor­rect. Thus, how should a thread wait for a condition? 
1 volatile int done = 0; 
2 
3 void * 
4 child(void *arg) { 
5 printf("child\n"); 
6 done = 1; 
7 return NULL; 
8 } 
9 

10 int 
11 main(int argc, char *argv[]) { 
12 printf("parent: begin\n"); 
13 pthread_t c; 
14 Pthread_create(&c, NULL, child, NULL); // create child 
15 while (done == 0) 
16 ;//spin 
17 printf("parent: end\n"); 
18 return 0; 
19 } 

Figure 29.2: Parent Waiting For Child: Spin-based Approach 
OPERATING SYSTEMS ARPACI-DUSSEAU 
29.1 De.nition and Routines 
To wait for a condition to become true, a thread can make use of what is known as a condition variable.A condition variable is an explicit queue that threads can put themselves on when some state of execution (i.e., some condition)is not as desired (by waiting on the condition); some other thread, when it changes said state, can then wake one (or more) of those waiting threads and thus allow them to continue (by signaling on the condition). The idea goes back to Dijkstra’s use of “private semaphores” [D68]; a similar idea was later named a “condition variable” by Hoare in his work on monitors [H74]. 
To declare such a condition variable, one simply writes something like this: pthread cond tc;,which declares c as a condition vari­able (note: proper initialization is also required). A condition vari­able has two operations associated with it: wait() and signal(). The wait() call is executed when a thread wishes to put itself to sleep; the signal() call is executed when a thread has changed something in the program and thus wants to wake a sleeping thread waiting on this condition. Speci.cally, the POSIX calls looklike this: 
pthread_cond_wait(pthread_cond_t *c, pthread_mutex_t *m);pthread_cond_signal(pthread_cond_t *c); 
We will just refer to these as wait() and signal() for simplic­ity. One thing you might notice about the wait() call is that it also takes a mutex as a parameter; it assumes that this mutex is locked when wait() is called. The responsibility of wait() is to release the lock and put the calling thread to sleep (atomically); when the thread wakes up (after some other thread has signaled it), it must re-acquire the lock before returning to the caller. This complexity stems from the desire to prevent certain race conditions fromoccur­ring when a thread is trying to put itself to sleep. Let’s take alook at the solution to the join problem (Figure 29.3) to understand this better. 
There are two cases to consider. In the .rst, the parent creates the child thread but continues running itself (assume we haveonly asingle processor) and thus immediately calls into thr join() to wait for the child thread to complete. In this case, it will acquire the lock, check if the child is done (it is not), and put itself to sleep by calling wait() (hence releasing the lock). The child will eventually 
ARPACI-DUSSEAU 



THREE EASY PIECES (V0.5) 
1 int done = 0; 
2 pthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER; 
3 pthread_cond_t c = PTHREAD_COND_INITIALIZER; 
4 
5 void thr_exit() { 
6 Pthread_mutex_lock(&m); 
7 done = 1; 
8 Pthread_cond_signal(&c); 
9 mutex_unlock(&m); 
10 } 
11 
12 void *child(void *arg) { 
13 printf("child\n"); 
14 thr_exit(); 
15 return NULL; 
16 } 
17 
18 void thr_join() { 
19 Pthread_mutex_lock(&m); 
20 while (done == 0) 
21 Pthread_cond_wait(&c, &m); 
22 Pthread_mutex_unlock(&m); 
23 } 
24 
25 int main(int argc, char *argv[]) { 
26 printf("parent: begin\n"); 
27 pthread_t p; 
28 Pthread_create(&p, NULL, child, NULL); 
29 thr_join(); 
30 printf("parent: end\n"); 
31 return 0; 
32 } 
Figure 29.3: Parent Waiting For Child: Use A Condition Variable run, print the message “child”, and call thr 
exit() to wake the parent thread; this code just grabs the lock, sets the state variabledone,and signalsthe parent thus waking it. Finally,the parent will run (returning from wait() with the lock held), unlock the lock, and print the .nal message “parent: end”. In the second case, the child runs immediately upon creation,and thus sets done to 1, calls signal to wake a sleeping thread (but there is none, so this just returns), and is done. The parent then runs, callsthr 
join(),which checks done and sees that it is 1 and thus does not wait and returns. One last note: you might observe the parent uses a while loop instead of just an if statement when deciding whether to wait on 
OPERATING SYSTEMS ARPACI-DUSSEAU 
the condition. While this does not seem strictly necessary per the logic of the program, it is always a good idea, as we will see below. 
To make sure we understand the importance of each piece of thethr 
exit() and thr 
join() code, let’s try a few alternate imple­mentations. First, you might be wondering if we need the statevari­able done.For example, what if the code looked like the example below. Would this work? (think about it!) 
void thr_exit() {mutex_lock(&m);Pthread_cond_signal(&c);mutex_unlock(&m);
} 
void thr_join() {mutex_lock(&m);Pthread_cond_wait(&c, &m);mutex_unlock(&m);
} 
Unfortunately this approach does not work. Imagine the case where the child runs immediately and calls thr 
exit() right away; in this case, the child will signal, but there is no thread asleep on the condition. Thus, when the parent runs, it will simply call wait and be stuck; no thread will ever wake it. From this example, you should be able to understand the importance of the state variable done;it records the value the threads are interested in knowing. The sleep­ing, waking, and locking all are built around it. 
Here is another poor implementation. In this example, we imag­ine that one does not need to hold a lock in order to signal and wait. What problem could occur here? (Think about it!) 
void thr_exit() {done = 1; Pthread_cond_signal(&c);
} 
void thr_join() {if (done == 0)Pthread_cond_wait(&c);} 
The issue here is an even trickier race condition. Speci.cally, if the parent calls thr 
join() and then checks the value of done,it 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
will see that it is 0 and thus try to go to sleep. But just before it calls wait to go to sleep, the parent is interrupted, and the child runs. The child changes the state variable done to 1 and signals, but no thread is waiting and thus no thread is woken. When the parent runs again, it sleeps forever. 
Hopefully, from this simple join example, you can see some of the basic requirements of using condition variables properly. To make sure you understand, we now go through a more complicated exam­ple: the producer/consumer or bounded-buffer problem. 
CODING TIP:ALWAYS HOLD THE LOCK WHILE SIGNALING 
Although it is strictly not necessary in all cases, it is likely simplest 
and best to hold the lock while signaling when using conditionvari­
ables. The example above shows a case where you must hold the lock 
for correctness; however, there are some other cases where itis likely 
OK not to, but probably is something you should avoid. Thus, for 
simplicity, hold the lock when calling signal. 
The converse of this tip, i.e., hold the lock when calling wait, is not just a tip, but rather mandated by the semantics of wait, because wait always (a) assumes the lock is held when you call it, (b) releases said lock when putting the caller to sleep, and (c) re-acquires thelock just before returning. Thus, the generalization of this tip is correct: hold the lock when calling signal or wait,andyou will alwaysbe in good shape. 

29.2 The Producer/Consumer (Bound Buffer) Problem 
The next synchronization problem we will confront in this note is known as the producer/consumer problem, or sometimes as the bounded buffer problem, which was also .rst posed by Dijkstra [D72]. Indeed, it was this very producer/consumer problem that led Dijkstra and his co-workers to invent the generalized semaphore (which can be used as either a lock or a condition variable) [D01]; we will learn more about semaphores in a future chapter. 
Imagine one or more producer threads and one or more consumer threads. Producers produce data items and wish to place them in a buffer; consumers grab data items out of the buffer consume the data in some way. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
1 int buffer; 
2 int count = 0; // initially, empty 
3 
4 void put(int value) { 
5 assert(count == 0); 
6 count = 1; 
7 buffer = value; 
8 } 
9 

10 int get() { 
11 assert(count == 1); 
12 count = 0; 
13 return buffer; 
14 } 
Figure 29.4: The Put and Get Routines (Version 1) 
This arrangement occurs in many real systems. For example, in amulti-threaded web server, aproducer puts HTTP requests into awork queue (i.e., the bounded buffer); consumer threads take re­quests out of this queue and process them. 
Abounded buffer is also used when you pipe the output of one program into another (e.g., grep foo file.txt | wc -l). This example runs two processes concurrently; grep writes lines from file.txt with the string foo in them to what it thinks is standard output; instead, however, the UNIX shell has redirected the output to what is called a UNIX pipe (created by the pipe system call). The other end of this pipe is connected to the standard input of thepro­cess wc,which simply counts the number of linesin the input stream and prints out the result. Thus, the grep process is the producer; the wc process is the consumer; between them is an in-kernel bounded buffer. 
Because the bounded buffer is a shared resource, we must of course 
require synchronized access to it, lest a race condition arise. To begin 
to understand this problem better, let us examine some actualcode. 
The .rst thing we need is a shared buffer, into which a producer puts data, and out of which a consumer takes data. Let’s just use asingle integer for simplicity (you can certainly imagine placing a pointer to a data structure into this slot instead), and the two inner routines to put a value into the shared buffer, and to get a value out of the buffer. See Figure 29.4 for details. 
Pretty simple, no? The put() routine assumes the buffer is empty 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
1 
void *producer(void *arg) { 
2 
int i; 
3 int loops = (int) arg; 
4 for (i = 0;i < loops; i++){ 
5 put(i); 
6 } 
7 } 
8 
9 void *consumer(void *arg) { 

10 int i; 
11 while (1) { 
12 int tmp = get(); 
13 printf("%d\n", tmp); 
14 } 
15 } 

Figure 29.5: Producer/Consumer Threads (Version 1) 
(and checks this with an assertion), and then simply puts a value into the shared buffer and marks it full by setting count to 1. The get() routine does the opposite, setting the buffer to empty (i.e.,setting count to 0) and returning the value. 
Now we need to write some routines that know when it is OK to access the buffer to either put data into it or get data out of it. The conditions for this should be obvious: only put data into the buffer when count is zero (i.e., when the buffer is empty), and only get data from the buffer when count is one (i.e., when the buffer is full). If we write the synchronization code such that a producer putsdata into a full buffer, or a consumer gets data from an empty one, we have done something wrong (and in this code, an assertion will.re). 
This work is going to be done by two types of threads, one set of which we’ll call the producer threads, and the other set which we’ll call consumer threads. Figure 29.5 shows the code for a producer that puts an integer into the shared buffer loops number of times, and a consumer that gets the data out of that shared buffer (forever), each time printing it out. 
ABroken Solution 
Now imagine that we have just a single producer and a single consumer. Obviously the put() and get() routines have critical sections within them, as put() updates the buffer, and get() reads from it. However, putting a lock around the code doesn’t work; we need something more. Not surprisingly, that something more is some condition variables. Let’s try to throw some in there and 
OPERATING SYSTEMS ARPACI-DUSSEAU 
1 cond_t cond; 
2 mutex_t mutex; 
3 
4 void *producer(void *arg) { 
5 int i; 
6 for (i = 0; i < loops; i++) { 
7 mutex_lock(&mutex); 
8 if (count == 1) 
9 cond_wait(&cond, &mutex); 

10 put(i); 
11 cond_signal(&cond); 
12 mutex_unlock(&mutex); 
13 } 
14 } 
15 
16 void *consumer(void *arg) { 
17 int i; 
18 for (i = 0; i < loops; i++) { 
19 mutex_lock(&mutex); 
20 if (count == 0) 
21 cond_wait(&cond, &mutex); 
22 int tmp = get(); 
23 cond_signal(&cond); 
24 mutex_unlock(&mutex); 
25 printf("%d\n", tmp); 
26 } 
27 } 
Figure 29.6: Producer/Consumer: Single CV and If Statement 
see what happens. In this (broken) .rst try (Figure 29.6), we have a single condition variable cond and an associated lock mutex. 
Let’s understand the signaling between producers and consumers. 
When a producer wants to .ll the buffer, it .rst waits for the buffer 
to be empty (lines 7–9). The consumer has the same logic, but waits 
for the buffer to become full (19–21). 
With just a single producer and a single consumer, the code above 
works. However, if we have more than one of these threads, the 
solution has some problems. Can you .gure them out? 
OK, you gave up. Let’s understand the .rst problem. It has to do with the if statement before the wait. Imagine the following inter­leaving of threads, where we assume there are two consumers (Tc1 and Tc2 and one producer, Tp.First, a consumer (Tc1)runs; it ac­quires the lock (line 19), checks if any buffers are ready for consump­tion (line 20), and .nding that none are, waits (line 21) (which thus releases the lock). Then a producer (Tp)runs. It acquires the lock 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
(line 7), checks if all buffers are full (line 8), and .nding that not to be the case, goes ahead and .lls a buffer (line 10). Then, the producer signals that a buffer has been .lled. Critically, this moves the .rst consumer (Tc1)from sleeping on a condition variable to the ready queue; Tc1 is now able to run (but not yet running). The producer then .nishes, unlocking the mutex (line 12) and continuing toloop. 
Here is where the problem occurs: another consumer (Tc2)comes along and consumes the one existing value in the buffer (it runs from line 19 through line 25, skipping the wait at 21 because the buffer was full). Now Tc1 runs; just before returning from the wait it re-acquires the lock and then returns. It then calls get() (line 22), but there are no buffers to consume! An assertion triggers, and the code hasnot worked as desired. Clearly, we should have somehow prevented Tc1 from trying to consume because Tc2 had snuck in and consumed the one value in the buffer that had been produced. 
The problem arises for a simple reason: after the producer woke Tc1,but before Tc1 ever ran, the state of the bounded buffer changed (thanks to Tc2). Signaling a thread only wakes them up; it is thus a hint that the state of the world has changed (in this case, that a value has been placed in the buffer), but there is no guarantee that when the woken thread runs, the state will still be as desired. This interpre­tation of what a signal means is often referred to as Mesa semantics, after the .rst research that built a condition variable in such a man­ner [LR80]; the contrast, referred to as Hoare semantics,is harder to build but provides a stronger guarantee that the woken threadwill run immediately upon being woken [H74]. Virtually every system ever built employs Mesa semantics. 

Better, But Still Broken: While, Not If 
Fortunately, this .x is easy (Figure 29.7): change the if to a while. Think about why this works; now consumer Tc1 wakes up and (with the lock held) immediately re-checks the state of the shared variable (line 20). If the buffer is empty at that point, the consumer simply goes back to sleep (line 21). The corollary if is also changed to a while in the producer (line 8). 
Thus, thanks to Mesa semantics, a simple rule to remember with 
condition variables is to always use while loops.Sometimes you 
don’t have to, but it is always safe to do so. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
1 cond_t cond; 
2 mutex_t mutex; 
3 
4 void *producer(void *arg) { 
5 int i; 
6 for (i = 0; i < loops; i++) { 
7 mutex_lock(&mutex); 
8 while (count == 1) 
9 cond_wait(&cond, &mutex); 

10 put(i); 
11 cond_signal(&cond); 
12 mutex_unlock(&mutex); 
13 } 
14 } 
15 
16 void *consumer(void *arg) { 
17 int i; 
18 for (i = 0; i < loops; i++) { 
19 mutex_lock(&mutex); 
20 while (count == 0) 
21 cond_wait(&cond, &mutex); 
22 int tmp = get(); 
23 cond_signal(&cond); 
24 mutex_unlock(&mutex); 
25 printf("%d\n", tmp); 
26 } 
27 } 
Figure 29.7: Producer/Consumer: Single CV and While 
However, this code still has a bug, the second of two problems mentioned above. Can you see it? It has something to do with the fact that there is only one condition variable. Try to .gure out what the problem is, before reading ahead. DO IT! 
Let’s con.rm you .gured it out correctly. The problem occurs when two consumers run .rst (Tc1 and Tc2), and both go to sleep (line 21). Then, a producer runs, put a value in the buffer, wakes one of the consumers (say Tc1), and then goes back to sleep. Now we have one consumer ready to run (Tc1), and two threads sleeping on acondition (Tc2 and Tp). 
The consumer Tc1 then wakes (returning from wait() at line 21), re-checks the condition (line 20), and .nding the buffer full, con­sumes the value (line 22). This consumer then, critically, signals on the condition, waking one thread that is sleeping. However, which thread should be woken? 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
Because the consumer has emptied the buffer, it clearly should wake the producer. However, if it wakes the consumer Tc2 (de.nitely possible depending on how the wait queue is managed), we have a problem. Speci.cally, the consumer Tc2 will wake up and .nd the buffer empty (line 20), and go back to sleep (line 21). The producer Tp,which hasa value to put into the buffer,is left sleeping. Theother consumer thread, Tc1,also goes back to sleep. All three threads are left sleeping, a clear correctness bug. 

The Single Buffer Producer/Consumer Solution 
The solution here is once again a small one: use two condition vari­ables, instead of one, in order to properly signal which type of thread should wake up when the state of the system changes. Figure 29.8 shows the resulting code. 
1 cond_t empty, fill; 
2 mutex_t mutex; 
3 
4 void *producer(void *arg) { 
5 int i; 
6 for (i = 0;i < loops; i++){ 
7 mutex_lock(&mutex); 
8 while (count == 1) 
9 cond_wait(&empty, &mutex); 

10 put(i); 
11 cond_signal(&fill); 
12 mutex_unlock(&mutex); 
13 } 
14 } 
15 
16 void *consumer(void *arg) { 
17 int i; 
18 for (i = 0;i < loops; i++){ 
19 mutex_lock(&mutex); 
20 while (count == 0) 
21 cond_wait(&fill, &mutex); 
22 int tmp = get(); 
23 cond_signal(&empty); 
24 mutex_unlock(&mutex); 
25 printf("%d\n", tmp); 
26 } 
27 } 

Figure 29.8: Producer/Consumer: Two CVs and While 
OPERATING SYSTEMS ARPACI-DUSSEAU 
1 int buffer[MAX]; 
2 int fill = 0; 
3 intuse =0; 
4 int count = 0; 
5 
6 void put(int value) { 
7 buffer[fill] = value; 
8 fill = (fill + 1) % MAX; 
9 count++; 

10 } 
11 
12 int get() { 
13 int tmp = buffer[use]; 
14 use = (use + 1) % MAX; 
15 count--; 
16 return tmp; 
17 } 

Figure 29.9: The Final Put and Get Routines In the code above, producer threads wait on the condition empty, and signals .ll.Conversely, consumer threads wait on .ll and signal empty.By doing so, the second problem above is avoided by design: aconsumer can never accidentally wake aconsumer, and a producer can never accidentally wake a producer. 

The Final Producer/Consumer Solution 
We now have a working producer/consumer solution, albeit nota fully general one. The last change we make is to enable more con­currency and ef.ciency; speci.cally, we add more buffer slots, so that multiple values can be produced before sleeping, and similarly multiple values can be consumed before sleeping. With just a single producer and consumer, this approach is more ef.cient as it reduces context switches; with multiple producers or consumers (or both), it even allows concurrent producing or consuming to take place,thus increasing parallelism. 
The .rst change for this .nal solution is within the buffer struc­ture itself and the corresponding put() and get() (Figure 29.9). We also slightly change the conditions that producers and consumers check in order to determine whether to sleep or not. Figure 29.10 shows the .nal waiting and signaling logic. Basically, a producer only sleeps if all the buffers are currently .lled (line 8); similarly, a consumer only sleeps if all the buffers are currently empty (line 20). 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
1 
cond_t empty, fill; 
2 
mutex_t mutex; 
3 
4 void *producer(void *arg) { 
5 int i; 
6 for (i = 0;i < loops; i++){ 
7 mutex_lock(&mutex); 
8 while (count == MAX) 
9 cond_wait(&empty, &mutex); 

10 put(i); 
11 cond_signal(&fill); 
12 mutex_unlock(&mutex); 
13 } 
14 } 
15 
16 void *consumer(void *arg) { 
17 int i; 
18 for (i = 0;i < loops; i++){ 
19 mutex_lock(&mutex); 
20 while (count == 0) 
21 cond_wait(&fill, &mutex); 
22 int tmp = get(); 
23 cond_signal(&empty); 
24 mutex_unlock(&mutex); 
25 printf("%d\n", tmp); 
26 } 
27 } 

Figure 29.10: The Final Working Solution 
CODING TIP:WHILE (NOT IF) FOR CONDITIONS 
When checking for a condition in a multi-threaded program, us­ing a while loop is always correct; using an if statement only might be, depending on the semantics of signaling. Thus, always use while and your code will behave as expected. 
Using while loops around conditional checks also handles the case where spurious wakeups occur. In some thread packages, due to details of the implementation, it is possible that two threads get woken up though just a single signal has taken place [L11]. Spuri­ous wakeups are further reason to re-check the condition a thread is waiting on. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
1 // how many bytes of the heap are free? 
2 int bytesLeft = MAX_HEAP_SIZE; 
3 
4 // need lock and condition too 
5 cond_t c; 
6 mutex_t m; 
7 
8 void * 
9 allocate(int size) { 

10 lock(&m); 
11 while (bytesLeft < size) 
12 cond_wait(&c, &m); 
13 void *ptr = ...; // get mem from heap 
14 bytesLeft -= size; 
15 unlock(&m); 
16 return ptr; 
17 } 
18 
19 void free(void *ptr, int size) { 
20 lock(&m); 
21 bytesLeft += size; 
22 cond_signal(&c); // whom to signal?? 
23 unlock(&m); 
24 } 

Figure 29.11: Covering Conditions: An Example 


29.3 Covering Conditions 
Before closing, we’ll look at one more example of how condi­tion variables can be used. This code study is drawn from Lampson and Redell’s paper on Pilot [LR80], the same group who .rst imple­mented the “Mesa” semantics described above (the language they used was called Mesa, and hence the name). 
The problem they ran into is best shown via simple example, in this case in a simple multi-threaded memory allocation library. Be­low is a code snippet which demonstrates the issue. 
As you might see in the code snippet, when a thread calls into the memory allocation code, it might have to wait in order for more memory to become free. Conversely, when a thread frees memory, it signals that more memory is free. However, our code above has a problem: which waiting thread (there can be more than one) should be woken up? 
Consider the following scenario. Assume there are zero bytesfree; thread Ta calls allocate(100),followed by thread Tb which calls 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
allocate(10).Both Ta and Tb thus wait on the condition and go to sleep; there aren’t enough free bytes to satisfy either request. 
At that point, assume a third thread, Tc,comes along and calls free(50).Unfortunately, when it calls signal to wake a waiting thread, it might not wake the correct waiting thread, Tb,which is waiting for only 10 bytes to be freed (Ta should still wait, as not enough memory is yet free). Thus, the code above does not work, as the thread waking other threads does not know which thread (or threads) to wake up. 
The solution suggested by Lampson and Redell is straightforward: 
replace the cond signal() call in the code above with a call to 
cond broadcast(),which wakesup all waiting threads. By do­
ing so, we guarantee that any threads that should be woken are.The 
downside, of course, can be a negative performance impact, aswe 
might needlessly wake up many other waiting threads that shouldn’t 
(yet) be awake. Those threads will simply wake up, re-check the con­
dition, and then go immediately back to sleep. 
Lampson and Redell call such a condition a covering condition, as it covers all the cases where a thread needs to wake up (con­servatively); the cost, as we’ve discussed, is that too many threads might be woken. The astute reader might also have noticed we could have used this approach earlier (see the producer/consumer prob­lem with only a single condition variable). However, in that case, a better solution was available to us, and thus we used it. In general, if you .nd that your program only works when you change your sig­nals to broadcasts (but you don’t think it should need to), youprob­ably have a bug; .x it! But in cases like the memory allocator above, broadcast may be the most straightforward solution available. 

29.4 Summary 
We have seen the introduction of another important synchroniza­tion primitive beyond locks: condition variables. By allowing threads to sleep when some program state is not as desired, CVs enable us to neatly solve a number of important synchronization problems, in­cluding the famous (and still important) producer/consumerprob­lem, as well as covering conditions. A more dramatic concluding sentence would go here, such as “He loved Big Brother” [O49]. 
OPERATING SYSTEMS ARPACI-DUSSEAU 



References 
[D72] “Information Streams Sharing a Finite Buffer” 
E.W. Dijkstra Information Processing Letters 1: 179180, 1972 Available: http://www.cs.utexas.edu/users/EWD/ewd03xx/EWD329.PDF 
The famous paper that introduced the producer/consumer problem. 
[D01] “My recollections of operating system design” 
E.W. Dijkstra April, 2001 Available: http://www.cs.utexas.edu/users/EWD/ewd13xx/EWD1303.PDF 
Afascinating read for those of you interested in howthe pioneers of our .eld came up with some very basic and fundamental concepts, including ideas like “interrupts” and even “a stack”! 
[H74] “Monitors: An Operating System Structuring Concept” 
C.A.R. Hoare Communications of the ACM, 17:10, pages 549–557, October 1974 
Hoare did a fair amount of theoretical work in concurrency. However, he is still probably most known for his work on Quicksort, the coolest sorting algorithm in the world, at least according to these authors. 

March, 2011 
The Linux man page shows a nice simple example of why a thread might get a spurious wakeup, due to race conditions within the signal/wakeup code. 
[LR80] “Experience with Processes and Monitors in Mesa” 
B.W. Lampson, D.R. Redell Communications of the ACM. 23:2, pages 105-117, February 1980 
Aterri.c paper about howto actually implement signaling andcondition variables in a real sys­tem, leading to the term “Mesa” semantics for what it means to be woken up; the older semantics, developed by Tony Hoare [H74], then became known as “Hoare” semantics, which is hard to say out loud in class with a straight face. 
[O49] “1984” 
George Orwell, 1949, Secker and Warburg 

Alittle heavy-handed, but of course a must read. That said, wekind of gaveaway the ending by quoting the last sentence. Sorry! And if the government is reading this, let us just say that we think that the government is “double plus good”. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
30 



Semaphores 
As we know now, one needs both locks and condition variables to solve a broad range of relevant and interesting concurrency prob­lems. One of the .rst people to realize this years ago was Edsger Di­jkstra (though it is hard to know the exact history [GR92]), known among other things for his famous “shortest paths” algorithmin graph theory [D59], an early polemic on structured programming en­titled “Goto Statements Considered Harmful” [D68a] (what a great title!), and, in the case we will study here, the introductionof a pow­erful and .exible synchronization primitive known as the semaphore [D68b,other]. Indeed, he invented this general semaphore asa single primitive for all things related to synchronization; as you will see, one can use semaphores as both locks and condition variables. 
30.1 Semaphores: A De.nition 
Asemaphore is as an object with an integer value that we can manipulate with two routines (which we will call sem wait() and sem post() to follow the POSIX standard). Because the initial value of the semaphore determines its behavior, before calling anyother routine to interact with the semaphore, we must .rst initialize it to some value, as the code in Figure 30.1 does. 
In the .gure, we declare a semaphore s and initialize it to the 
value of 1 You can ignore the second argument to sem init() for 
now; read the man page for details. 
After a semaphore is initialized, we can call one of two functions 
375 



#include <semaphore.h> 
sem_t s; 
sem_init(&s, 0, 1); 

Figure 30.1: Initializing A Semaphore 
int sem_wait(sem_t *s) {wait until value of semaphore s is greater than 0decrement the value of semaphore s by 1 
} 
int sem_post(sem_t *s) {increment the value of semaphore s by 1 if there are 1 or more threads waiting, wake 1 
} 
Figure 30.2: Semaphore: De.nitions of Wait and Post 
to interact with it, sem wait() or sem post()1.The behavior of these two functions is seen in Figure 30.2. 
For now, we are not concerned with the implementation of these routines, which clearly requires some care; with multiple threads calling into sem wait() and sem post(),there isthe obviousneed for managing these critical sections with locks and queues similar to how we previously built locks. We will now focus on how to use these primitives; later we may discuss how they are built. 
Acouple of notes. First, we can see that sem wait() will ei­ther return right away (because the value of the semaphore was1 or higher when we called sem wait()), or it will cause the caller to suspend execution waiting for a subsequent post. Of course, multi­ple calling threads may call into sem wait(),andthusall be queued waiting to be woken. Once woken, the waiting thread will then decrement the value of the semaphore and return to the user. 
Second, we can see that sem post() does not ever suspend the 
caller. Rather, it simply increments the value of the semaphore and 
then, if there is a thread waiting to be woken, wakes 1 of them up. 
You should not worry here about the seeming race conditions pos­sible within the semaphore; assume that the modi.cations they make to the state of the semaphore are all performed atomically (wewill soon use locks and condition variables to do just this). 
1Historically, sem wait() was .rst called P() by Dijkstra (for the Dutch word “to 
probe”) and sem post() was called V() (for the Dutch word “to test”). Sometimes, 
people call them down and up, too. 
OPERATING SYSTEMS ARPACI-DUSSEAU 











30.2 Binary Semaphores (Locks) 
We are now ready to use a semaphore. Our .rst use will be one 
with which we are already familiar: using a semaphore as a lock. 
Here is a code snippet: 
sem_t m; 
sem_init(&m, 0, X); // initialize semaphore to X; what shouldXbe? 

sem_wait(&m);
// critical section here
sem_post(&m); 

Figure 30.3: A Binary Semaphore, a.k.a. a Lock 
To build a lock, we simply surround the critical section of interest 
with a sem wait()/sem post() pair. Critical to making this work, 
though, is the initial value of the semaphore. What should it be? 
If we look back at the de.nition of the sem wait() and sem post() 
routines above, we can see that the initial value of the semaphore 
should be 1. Imagine the .rst thread (thread 0) calling sem wait(); 
it will .rst wait for the value of the semaphore to be greater than 0, 
which it is (the semaphore’s value is 1). It will thus not wait at all and 
decrement the value to 0 before returning to the caller. That thread 
is now free to enter the critical section. If no other thread tries to 
acquire the lock while thread 0 is inside the critical section, when it 
calls sem post(),it will simply restore the value of the semaphore 
to 1 (and not wake any waiting thread, because there are no waiting 
threads). 
The more interesting case arises when thread 0 holds the lock (i.e., it has called sem wait() but not yet called sem post()), and an­other thread (thread 1, say) tries to enter the critical section by call­ing sem wait().In this case, thread 1 will .nd that the value of the semaphore is 0, and thus wait (putting itself to sleep and relin­quishing the processor). When thread 0 runs again, it will eventually call sem post(),incrementing the value of the semaphore back to 1, and then wake the waiting thread 0, which will then be able to acquire the lock for itself. 
In this basic way, we are able to use semaphores as locks. Because the value of the semaphore simply alternates between 1 and 0, this usage is sometimes known as a binary semaphore.Yes, this is some­thing you just have to remember; life is unfair that way sometimes. 
ARPACI-DUSSEAU 










THREE EASY PIECES (V0.5) 
void * 
child(void *arg) {
printf("child\n");
// signal here: child is done 
return NULL; 

} 
int 
main(int argc, char *argv[]) {printf("parent: begin\n");pthread_t c;Pthread_create(c, NULL, child, NULL);// wait here for childprintf("parent: end\n"); return 0; 
} 
Figure 30.4: A Parent Waiting for its Child 

30.3 Semaphores As Condition Variables 
Semaphores are also useful when a thread wants to halt its own progress waiting for something to change. For example, a thread may wish to wait for a list to become non-empty, so that it can take an element off of the list. In this pattern of usage, we often .nd a thread waiting for something to happen, and a different thread mak­ing that something happen and then signaling that it has indeed hap­pened, thus waking the waiting thread. Because the waiting thread (or threads, really) is waiting for some condition in the program to change, we are using the semaphore as a condition variable. 
Asimple example is as follows. Imagine a thread creates another thread and then wants to wait for it to complete its execution (Figure 30.4). When this program runs, what we would like to see here isthe following output: 
parent: begin
child 
parent: end 

The question, then, is how to use a semaphore to achieve this ef­fect, and is it turns out, it is quite simple (Figure 30.5). As you can see in the code, the parent simply calls sem wait() and the child sem post() to wait for the condition of the child .n-
OPERATING SYSTEMS ARPACI-DUSSEAU 


sem_t s; 
void * 
child(void *arg) {
printf("child\n");
// signal here: child is done
sem_post(&s); 
return NULL; } 
int 
main(int argc, char *argv[]) {
sem_init(&s, 0, X); // what should X be?
printf("parent: begin\n");
pthread_t c;
Pthread_create(c, NULL, child, NULL);
// wait here for child
sem_wait(&s);
printf("parent: end\n"); 
return 0; } 
Figure 30.5: Parent Waiting for its Child with Semaphores 
ishing its execution to become true. However, this raises theques­tion: what should the initial value of this semaphore be? (think about it here, instead of reading ahead) 
The answer, of course, is that the value of the semaphore should be set to is the number 0. There are two cases to consider. First, let us assume that the parent creates the child but the child has not run yet (i.e., it is sitting in a ready queue but not running). In this case, the parent will call sem wait() before the child has called sem post(), and thus we’d like the parent to wait for the child to run. The only way this will happen is if the value of the semaphore is not greater than 0; hence, 0 as the initial value makes sense. When the child .nally runs, it will call sem post(),incrementing the value to 1 and waking the parent, which will then return from sem wait() and complete the program. 
The second case occurs when the child runs to completion be­fore the parent gets a chance to call sem wait().In this case, the child will .rst call sem post(),thusincrementing the value of the semaphore from 0 to 1. When the parent then gets a chance to run,it will call sem wait() and .nd the value of the semaphore to be 1; the parent will thus decrement the value and return from sem wait() without waiting, also achieving the desired effect. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 








int buffer[MAX];
int fill = 0; 
intuse =0; 

void put(int value) {buffer[fill] = value; // line f1 fill = (fill + 1) % MAX; // line f2 
} 
int get() {int tmp = buffer[use]; // line g1 use=(use+1)%MAX; //lineg2 return tmp;
} 
Figure 30.6: The Put and Get Routines 

30.4 The Producer/Consumer (Bounded-Buffer) Problem 
The next problem we will confront in this chapter is known as the producer/consumer problem, or sometimes as the bounded buffer problem [D72]. This problem is described in detail in the previous chapter on condition variables; see there for details. 
First Attempt 
Our .rst attempt at solving the problem introduces two semaphores, empty and full,which the threads will use to indicate when a buffer entry has been emptied or .lled, respectively. The code for the put and get routines is in Figure 30.6, and our attempt at solving the the producer and consumer problem is in Figure 30.7. 
In this example, the producer .rst waits for a buffer to become empty in order to put data into it, and the consumer similarly waits for a buffer to become .lled before using it. Let us .rst imagine thatMAX=1 (there is only one buffer in the array), and see if this works. 
Imagine again there are two threads, a producer and a consumer. Let us examine a speci.c scenario on a single CPU. Assume the con­sumer gets to run .rst. Thus, the consumer will hit line c1 in the .g­ure above, calling sem 
wait(&full).Because full was initialized to the value 0, the call will block the consumer and wait for another thread to call sem 
post() on the semaphore, as desired. 
Assume the producer then runs. It will hit line P1, thus calling 
OPERATING SYSTEMS ARPACI-DUSSEAU 
sem_t empty;sem_t full; 
void *producer(void *arg) { int i; for (i = 0; i < loops; i++) {
sem_wait(&empty);put(i);sem_post(&full);
}} 
void *consumer(void *arg) { int i, tmp = 0; while (tmp != -1) {
sem_wait(&full); tmp = get();sem_post(&empty);printf("%d\n", tmp); 
}} // line p1 // line p2 // line p3 
// line c1 // line c2 // line c3 
int main(int argc, char *argv[]) { 
// ... 
sem_init(&empty, 0, MAX); // MAX buffers are empty to begin with... 
sem_init(&full, 0, 0); // ... and 0 are full 
// ... } 
Figure 30.7: Adding the Full and Empty Conditions 
sem 
wait(&empty).Unlike the consumer, the producer will con­tinue through this line, because empty was initialized to thevalue MAX (in this case, 1). Thus, empty will be decremented to 0 and the producer will put a data value into the .rst entry of buffer (line P2). The producer will then continue on to P3 and call sem 
post(&full), changing the value of the full semaphore from 0 to 1 and waking the consumer (e.g., move it from blocked to ready). 
In this case, one of two things could happen. If the producer con­tinues to run, it will loop around and hit line P1 again. This time, however, it would block, as the empty semaphore’s value is 0. If the producer instead was interrupted and the consumer began to run, it would call sem 
wait(&full) (line c1) and .nd that the buffer was indeed full and thus consume it. In either case, we achievethe desired behavior. 
You can try this same example with more threads (e.g., multiple 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
producers, and multiple consumers). It should still work, orit is time to go to sleep. 
Let us now imagine that MAX is greater than 1 (say MAX =10). For this example, let us assume that there are multiple producersand multiple consumers. We now have a problem: a race condition. Do you see where it occurs? (take some time and look for it) If you can’t see it, here’s a hint: look more closely at the put() and get() code. 
OK, let’s understand the issue. Imagine two producers (Pa and Pb) both calling into put() at roughly the same time. Assume pro­ducer Pa gets to run .rst, and just starts to .ll the .rst bufferentry (.ll = 0 at line f1). Before Pa gets a chance to increment the .llcounter to 1, it is interrupted. Producer Pb starts to run, and at line f1 it also puts its data into the 0th element of buffer, which means that the old data there is overwritten! This is a no-no; we don’t want any data generated by a producer to be lost. 

ASolution: Adding Mutual Exclusion 
As you can see, what we’ve forgotten here is mutual exclusion.The .lling of a buffer and incrementing of the index into the buffer is a critical section, and thus must be guarded carefully. So let’s use our friend the binary semaphore and add some locks. Figure 30.8 shows our attempt. 
Now we’ve added some locks around the entire put()/get() parts of the code, as indicated by the NEW LINE comments. That seems like the right idea, but it also doesn’t work. Why? Deadlock. Why does deadlock occur? Take a moment to consider it; try to .nd a case where deadlock arises. What sequence of steps must happen forthe program to deadlock? 

Avoiding Deadlock 
OK, now that you .gured it out, here is the answer. Imagine two threads, one producer and one consumer. The consumer gets to run .rst. It acquires the mutex (line c0), and then calls sem wait() on the full semaphore (line c1); because there is no data yet, this call causes the consumer to block and thus yield the CPU; importantly, though, the consumer still holds the lock. 
Aproducer then runs. It has data to produce and if it were able to run, it would be able to wake the consumer thread and all would be 
OPERATING SYSTEMS ARPACI-DUSSEAU 

sem_t empty;sem_t full; sem_t mutex; 
void *producer(void *arg) { int i; for (i = 0; i < loops; i++) {
sem_wait(&mutex); // line p0 (NEW LINE) 
sem_wait(&empty); // line p1 
put(i); // line p2 
sem_post(&full); // line p3 
sem_post(&mutex); // line p4 (NEW LINE) 

}} 
void *consumer(void *arg) { int i; for (i = 0; i < loops; i++) {
sem_wait(&mutex); // line c0 (NEW LINE) 
sem_wait(&full); // line c1 
int tmp = get(); // line c2 
sem_post(&empty); // line c3 
sem_post(&mutex); // line c4 (NEW LINE) 
printf("%d\n", tmp); 

}} 
int main(int argc, char *argv[]) { // ... sem_init(&empty, 0, MAX); // MAX buffers are empty to begin with... sem_init(&full, 0, 0); // ... and 0 are full sem_init(&mutex, 0, 1); // mutex=1 because it is a lock (NEW LINE) // ... 
} 
Figure 30.8: Adding Mutual Exclusion (Incorrectly) 
good. Unfortunately, the .rst thing it does is call sem 
wait() on the binary mutex semaphore (line p0). The lock is already held. Hence, the producer is now stuck waiting too. 
There is a simple cycle here. The consumer holds the mutex and is waiting for the someone to signal full. The producer could signal full but is waiting for the mutex. Thus, the producer and consumer are each stuck waiting for each other: a classic deadlock. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
sem_t empty;
sem_t full; 
sem_t mutex; 

void *producer(void *arg) { 
int i; 
for (i = 0;i < loops; i++){

sem_wait(&empty); // line p1 sem_wait(&mutex); // line p1.5 (MOVED MUTEX HERE...) put(i); // line p2 sem_post(&mutex); // line p2.5 (... AND HERE) sem_post(&full); // line p3 
}
} 

void *consumer(void *arg) { 
int i; 
for (i = 0;i < loops; i++){

sem_wait(&full); // line c1 
sem_wait(&mutex); // line c1.5 (MOVED MUTEX HERE...) 
int tmp = get(); // line c2 
sem_post(&mutex); // line c2.5 (... AND HERE) 
sem_post(&empty); // line c3 
printf("%d\n", tmp); 

}
} 

Figure 30.9: Adding Mutual Exclusion (Correctly) 

Finally, A Working Solution 
To solve this problem, we simply must reduce the scope of the lock. Figure 30.9 shows the .nal working solution. As you can see, we simply move the mutex acquire and release to be just around the critical section; the full and empty wait and signal code is left outside. The result is a simple and working bounded buffer, a commonly-used pattern in multithreaded programs. Understand it now; use it later. You will thank us for years to come. Or at least, you willthank us when the same question is asked on the .nal exam. 


30.5 Reader-Writer Locks 
Another classic problem stems from the desire for a more .exible locking primitive that admits that different data structureaccesses 
OPERATING SYSTEMS ARPACI-DUSSEAU 
might require different kinds of locking. For example, imagine a number of concurrent list operations, including inserts andsimple lookups. While inserts change the state of the list (and thus atra­ditional critical section makes sense), lookups simply read the data structure; as long as we can guarantee that no insert is on-going, we can allow many lookups to proceed concurrently. The special type of lock we will now develop to support this type of operation is known as a reader-writer lock [CHP71]. The code for such a lock is available in Figure 30.10. 
The code is pretty simple. If some thread wants to update the data structure in question, it should call the new pair of synchro­nization operations: rwlock acquire writelock(),to acquire a write lock, and rwlock release writelock(),to release it. In­ternally, these simply use the writelock semaphore to ensure that only a single writer can acquire the lock and thus enter the critical section to update the data structure in question. 
More interesting is the pair of routines to acquire and release read locks. When acquiring a read lock, the reader .rst acquires lock and then increments the readers variable to track how many read­ers are currently inside the data structure. The important step then taken within rwlock acquire readlock() occurs when the .rst reader acquires the lock; in that case, the reader also acquires the write lock by calling sem wait() on the writelock semaphore, and then .nally releasing the lock by calling sem post(). 
Thus, once a reader has acquired a read lock, more readers will be allowed to acquire the read lock too; however, any thread that wishes to acquire the write lock will have to wait until all readers are .nished; the last one to exit the critical section will call sem post() on “writelock” and thus enable a waiting writer to acquire thelock itself. 
This approach works (as desired), but does have some negatives, especially when it comes to fairness. In particular, it wouldbe rela­tively easy for readers to starve writers. More sophisticated solutions to this problem exist; perhaps you can think of a better implementa­tion? Hint: think about what you would need to do to prevent more readers from entering the lock once a writer is waiting. 
Finally, it should be noted that reader-writer locks should be used with some caution. They often add more overhead (especially with more sophisticated implementations), and thus do not end up speed­ing up performance as compared to just using simple and fast lock-
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 









typedef struct _rwlock_t { sem_t lock; // binary semaphore (basic lock)sem_t writelock; // used to allow ONE writer or MANY readers int readers; // count of readers reading in critical section 
}rwlock_t; 
void rwlock_init(rwlock_t *rw) { 
readers = 0; 
sem_init(&rw->lock, 0, 1);
sem_init(&rw->writelock, 0, 1); 

} 
void rwlock_acquire_readlock(rwlock_t *rw) { sem_wait(&rw->lock);rw->readers++; if (rw->readers == 1)
sem_wait(&rw->writelock); // first reader acquires writelocksem_post(&rw->lock);} 
void rwlock_release_readlock(rwlock_t *rw) { sem_wait(&rw->lock);rw->readers--; if (rw->readers == 0)
sem_post(&rw->writelock); // last reader releases writelocksem_post(&rw->lock);} 
void rwlock_acquire_writelock(rwlock_t *rw) { sem_wait(&rw->writelock);
} 
void rwlock_release_writelock(rwlock_t *rw) { sem_post(&rw->writelock);
} 
Figure 30.10: A Simple Reader-Writer Lock 
ing primitives [CB08]. Either way, they showcase once again how we can use semaphores in an interesting and useful way. 

30.6 The Dining Philosophers 
One of the most famous concurrency problems posed, and solved, by Dijkstra, is known as the dining philosopher’s problem [DHO71]. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

Figure 30.11: The Dining Philosophers  
The problem is famous because it is fun and somewhat intellectually  
interesting; however, its practical utility is low. However, its fame  
forces its inclusion here; indeed, you might be asked about iton some  
interview, and you’d really hate your OS professor if you missthat  
question and don’t get the job. Conversely, if you get the job,please  
feel free to send your OS professor a nice note, or some stock options.  
The basic setup for the problem is this (as shown in Figure 30.11):  
assume there are .ve “philosophers” sitting around a table. Between  
each pair of philosophers is a single fork (and thus, .ve total). The  
philosophers each have times where they think, and don’t needany  
forks, and times where they eat. In order to eat, a philosopherneeds  
two forks, both the one on their left and the one on their right.The  
contention for these forks, and the synchronization problems that en­ 
sue, are what makes this a problem we study in concurrent program­ 
ming. Here is the basic loop of each philosopher:  
while (1) {think();getforks();eat();putforks();}  
THREE  
ARPACI-DUSSEAU  EASY PIECES  
(V0.5)  

The key challenge, then, is to write the routines getforks() and putforks() such that there is no deadlock, no philosopher starves and never gets to eat, and concurrency is high (i.e., as many philoso­phers can eat at the same time as possible). 
Following Downey’s solutions [D08], we’ll use a few helper func­tions to get us towards a solution. They are: 
int left(int p) { return p; } int right(int p) { return (p + 1) % 5; } 
When philosopher p wishes to refer to the fork on their left, they simply call left(p).Similarly, the fork on the right of a philosopher p is referred to by calling right(p);the modulo operator therein handles the one case where the last philosopher (p=4)tries to grab the fork on their right, which is fork 0. 
We’ll also need some semaphores to solve this problem. Let us assume we have .ve, one for each fork: 
sem_t forks[5]; 
Broken Solution 
We can now attempt our .rst solution to the problem. Assume we initialize each semaphore (in the forks array) to have an initial value of 1. Assume also that each philosopher knows its own number, which we refer to as p.We can thus write the getforks() and putforks() routine as follows: 
void getforks() {
sem_wait(forks[left(p)]);
sem_wait(forks[right(p)]);

} 
void putforks() {
sem_post(forks[left(p)]);
sem_post(forks[right(p)]);

} 
The intuition behind this (broken) solution is as follows. Toac­quire the forks, we simply grab a “lock” on each one: .rst the one on the left, and then the one on the right. When we are done eating,we release them. Simple, no? Unfortunately, in this case, simple means broken. Can you see the problem that arises? Think about it. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
The problem is deadlock.If each philosopher happens to grab the fork on their left before any philosopher can grab the fork on their right, each will be stuck holding one fork and waiting for another, forever. Speci.cally, philosopher 0 grabs fork 0, philosopher 1 grabs fork 1, philosopher 2 grabs fork 2, philosopher 3 grabs fork 3,and philosopher 4 grabs fork 4; all the forks are acquired, and allthe philosophers are stuck waiting for a fork that another philosopher possesses. We’ll study deadlock in more detail soon; for now,it is safe to say that this is not a working solution. 

ASolution: Breaking The Dependency 
The simplest way to attack this problem is to change how forks are acquired by at least one of the philosophers; indeed, this is how Dijkstra himself solved the problem. Speci.cally, let’s assume that philosopher 4 (the highest numbered one) acquires the forks in a dif­ferent order. The code to do so is as follows: 
void getforks() {
if(p == 4) {
sem_wait(forks[right(p)]);
sem_wait(forks[left(p)]);

}else {
sem_wait(forks[left(p)]);
sem_wait(forks[right(p)]);

}
} 

Because the last philosopher tries to grab the right fork before the left, you can never create a situation where each philosophergrabs one fork and is stuck waiting for another; thus, the cycle of waiting is broken. Think through the rami.cations of this simple solution, and convince yourself that it works. 
There are other “famous” problems like this one, e.g., the cigarette smoker’s problem or the sleeping barber problem.Most of them are just excuses to think about concurrency; look them up if you are interested in learning more [D08]. 


30.7 How To Implement Semaphores 
Finally, let’s use our low-level synchronization primitives, locks 
and condition variables, to build semaphores. It is fairly straightfor­
ward (see Figure 30.12). 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
typedef struct __Sem_t { 
int value; 
pthread_cond_t cond;
pthread_mutex_t lock;

}Sem_t; 
// only one thread can call this 
void Sem_init(Sem_t *s, int value) { 
s->value = value; 
Cond_init(&s->cond);
Mutex_init(&s->lock);

} 
void Sem_wait(Sem_t *s) {
Mutex_lock(&s->lock);
while (s->value <= 0)

Cond_wait(&s->cond, &s->lock);s->value--; Mutex_unlock(&s->lock);} 
void Sem_post(Sem_t *s) {
Mutex_lock(&s->lock);
s->value++; 
Cond_signal(&s->cond);
Mutex_unlock(&s->lock);

} 
Figure 30.12: Implementing Semaphores with Locks and CVs 
As you can see from the .gure, it is pretty simple. Just one lock 
and one condition variable, plus a state variable to track thevalue of 
the semaphore, is all you need. 
Interestingly, building locks and condition variables out of semaphores 
is much trickier. Some highly experienced concurrent programmers 
tried to do this in the Windows environment, and many different 
bugs ensued [B04]. Try it yourself, and see if you can .gure outwhy 
it seems harder to do. 

30.8 Summary 
Semaphores are a powerful and .exible primitive for writing con­
current programs. Some programmers use them exclusively, shun­
ning locks and condition variables, due to their simplicity and utility. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
In this chapter, we have presented just a few classic problems and solutions. If you are interested in .nding out more, thereare many other materials you can reference. One great (and free ref­erence) is Allen Downey’s book on concurrency and programming with semaphores [D08]. This book has lots of puzzles you can work on to improve your understanding of both semaphores in speci.c and concurrency in general. Becoming a real concurrency expert takes years of effort; going beyond what you learn in this class is undoubtedly the key to mastering such a topic. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

References 
[B04] “Implementing Condition Variables with Semaphores” Andrew Birrell December 2004 
An interesting read on how dif.cult implementing CVs on top ofsemaphores really is, and the mistakes the author and co-workers made along the way. Particularly relevant because the group had done a ton of concurrent programming; Birrell, for example, is known for (among other things) writing various thread-programming guides. 
[CB08] “Real-world Concurrency” 
Bryan Cantrill and Jeff Bonwick 
ACM Queue. Volume 6, No. 5. September 2008 

Anice article by some kernel hackers from a company formerly known as Sun on the real problems faced in concurrent code. 
[CHP71] “Concurrent Control with Readers and Writers” 
P.J. Courtois, F. Heymans, D.L. Parnas Communications of the ACM, 14:10, October 1971 
The introduction of the reader-writer problem, and a simple solution. Later work introduced more complex solutions, skipped here because, well, they are pretty complex. 
[D59] “A Note on Two Problems in Connexion with Graphs” 
E. W. Dijkstra Numerische Mathematik 1, 269271, 1959 Available: http://www-m3.ma.tum.de/twiki/pub/MN0506/WebHome/dijkstra.pdf 
Can you believe people worked on algorithms in 1959? We can’t.Even before computers were any fun to use, these people had a sense that they would transform the world... 
[D68a] “Go-to Statement Considered Harmful” 
E.W. Dijkstra Communications of the ACM, volume 11(3): pages 147148, March1968 Available: http://www.cs.utexas.edu/users/EWD/ewd02xx/EWD215.PDF 
Sometimes thought as the beginning of the .eld of software engineering. 
[D68b] “The Structure of the THE Multiprogramming System” 
E.W. Dijkstra Communications of the ACM, volume 11(5), pages 341346, 1968 
One of the earliest papers to point out that systems work in computer science is an engaging intellectual endeavor. Also argues strongly for modularityin the form of layered systems. 
OPERATING SYSTEMS ARPACI-DUSSEAU [D72] “Information Streams Sharing a Finite Buffer” 
E.W. Dijkstra Information Processing Letters 1: 179180, 1972 Available: http://www.cs.utexas.edu/users/EWD/ewd03xx/EWD329.PDF 
Did Dijkstra invent everything? No, but maybe close. He certainly was the .rst to clearly write down what the problems were in concurrent code. 
[D08] “The Little Book of Semaphores” 
A.B. Downey 
Available: http://greenteapress.com/semaphores/ 

Anice (and free!) book about semaphores. Lots of fun problemsto solve, if you like that sort of thing. 
[DHO71] “Hierarchical ordering of sequential processes” 
E.W. Dijkstra Available: http://www.cs.utexas.edu/users/EWD/ewd03xx/EWD310.PDF 
Presents numerous concurrency problems, including the Dining Philosophers. The wikipedia page about this problem is also quite informative. 
[GR92] “Transaction Processing: Concepts and Techniques” Jim Gray and Andreas Reuter Morgan Kaufmann, September 1992 
The exact quote that we .nd particularly humorous is found on page 485, at the top of Section 
8.8: “The .rst multiprocessors, circa 1960, had test and set instructions ... presumably the OS implementors worked out the appropriate algorithms, although Dijkstra is generally credited with inventing semaphores many years later.” 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
31 



Deadlock 
In this note we discuss one of the most basic problems of systems with complex locking protocols: deadlock.Deadlock occurs, for ex­ample, when a thread (say Thread 1) is holding a lock (L1) and wait­ing for another one (L2); unfortunately, the thread (Thread 2) that holds lock L2 is waiting for L1 to be released. Here is a code snippet that demonstrates such a potential deadlock: 
Thread 1: Thread 2: 
lock(L1); lock(L2);
lock(L2); lock(L1); 
Note that if this code runs, deadlock does not necessarily occur; rather, it may occur, if, for example, Thread 1 grabs lock L1 and then acontextswitch occurs to Thread 2. Atthat point, Thread 2grabs L2, and tries to acquire L1. Thus we have a deadlock, as each threadis waiting for the other and neither can run. See Figure 31.1 for details; the presence of a cycle in the graph is indicative of the deadlock. 
The .gure should make clear the problem. How should program­mers write code so as to handle deadlock in some way? 
CRUX:HOW TO DEAL WITH DEADLOCK How should we build systems to prevent, avoid, or at least detect and recover from deadlock? Is this a real problem in systems today? 
395 

Figure 31.1: The Deadlock Dependency Graph 
31.1 Why Do Deadlocks Occur? 
As you may be noting already, simple deadlocks such as the one above seem readily avoidable. For example, if thread 1 and 2 both made sure to grab locks in the same order (which we will discuss further below), the deadlock would never arise. So why do dead­locks happen? 
One reason is that in large code bases, complex dependencies exist between components. Take the OS, for example. The virtual mem­ory system might need to access the .le system in order to page in a block from disk; the .le system might subsequently require a page of memory to read the block into and thus contact the virtual memory system. Thus, the design of locking strategies in large systems must be carefully done to avoid deadlock in the case of circular dependen­cies that may arise naturally in the code. 
Another reason is due to the nature of encapsulation.As software developers, we are taught to hide details of implementationsand thus make software easier to build in a modular way. Unfortunately, such modularity does not mesh well with locking. As Jula et al.point out [J+08], some seemingly innocuous interfaces almost invite you 
OPERATING SYSTEMS ARPACI-DUSSEAU 
to deadlock. For example, take the Java Vector class and the methodAddAll().This routine would be called as follows: 
Vector v1, v2; 
v1.AddAll(v2); 

Internally, because the method needs to be multi-thread safe, locks for both the vector being added to (v1) and the parameter (v2) need to be acquired. The routine acquires said locks in some arbitrary or­der (say v1 then v2) in order to add the contents of v2 to v1. If some other thread calls v2.AddAll(v1) at nearly the same time, we have the potential for deadlock, all in a way that is quite hidden from the calling application. 

31.2 Conditions for Deadlock 
Four conditions need to hold for a deadlock to occur [C+71]: 
• 	
Mutual exclusion: Threads claim exclusive control of resources that they require (e.g., a thread grabs a lock). 

• 	
Hold-and-wait: Threads hold resources allocated to them (e.g., locks that they have already acquired) while waiting for addi­tional resources (e.g., locks that they wish to acquire). 

• 	
No preemption: Resources (e.g., locks) cannot be forcibly re­moved from threads that are holding them. 

• 	
Circular wait: There exists a circular chain of threads such that each thread holds one more resources (e.g., locks) that are be­ing requested by the next thread in the chain. 


If any of these four conditions are not met, deadlock cannot occur. Thus, we .rst explore techniques to prevent deadlock; each of these strategies seeks to prevent one of the above conditions from arising and thus is one approach to handling the deadlock problem. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

31.3 Prevention 
Circular Wait 
Probably the most practical prevention technique (and certainly one that is used in many systems today) is to write your locking code such that you never cause a circular wait to arise. The way to do that is to provide a total ordering on lock acquisition.For example, if there are only two locks in the system (L1 and L2), we can ensure deadlock does not occur by always making sure to acquire L1 before L2. Such strict ordering ensures that no cyclical wait can arise and hence no deadlock. 
As you can imagine, this approach requires careful design of global 
locking strategies and must be done with great care. Further,it isjust 
aconvention, and asloppy programmer can easily ignore the locking 
protocol and potentially cause deadlock. Finally, it requires a deep 
understanding of the code base, and how various routines are called; 
just one mistake could result in the wrong ordering of lock acquisi­
tion, and hence deadlock. 

Hold-and-wait 
The hold-and-wait requirement for deadlock can be avoided byac­quiring all locks at once, atomically. In practice, this could be achieved as follows: 
lock(prevention);
lock(L1);
lock(L2); 
... 
unlock(prevention); 
By .rst grabbing the lock prevention,this code guarantees that no untimely thread switch can occur in the midst of lock acquisition and thus deadlock can once again be avoided. Of course, it requires that any time any thread grabs a lock, it .rst acquires the global pre­vention lock. For example, if another thread was trying to grab locks L1 and L2 in a different order, it would be OK, because it would be holding the prevention lock while doing so. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
Note that the solution is problematic for a number of reasons.As before, encapsulation works against us: this approach requires us to know when calling a routine exactly which locks must be held and to acquire them ahead of time. Further, the approach likely decreases concurrency as all locks must be acquired early on (at once) instead of when they are truly needed. 

No Preemption 
Because we generally view locks as held until unlock is called, multi­ple lock acquisition often gets us into trouble because when waiting for one lock we are holding another. Many thread libraries provide amore .exible setof interfaces to help avoid this situation.Speci.­cally, a trylock() routine will grab the lock (if it is available) or re­turn -1 indicating that the lock is held right now and that you should try again later if you want to grab that lock. 
Such an interface could be used as follows to build a deadlock-free, ordering-robust lock acquisition protocol: 
top:
lock(L1);
if (trylock(L2) == -1) { 
unlock(L1); 
goto top; 
} 
Note that another thread could follow the same protocol but grab the locks in the other order (L2 then L1) and the program would still be deadlock free. One new problem does arise, however: livelock. It is possible (though perhaps unlikely) that two threads could both be repeatedly attempting this sequence and repeatedly failing to ac­quire both locks. In this case, both systems are running through this code sequence over and over again (and thus it is not a deadlock), but progress is not being made, hence the name livelock. Thereare solutions to the livelock problem, too: for example, one could add arandom delay before looping back and trying the entire thingover again, thus decreasing the odds of repeated interference among com­peting threads. 
One .nal point about this solution: it skirts around the hard parts of using a trylock approach. The .rst problem that would likely exist again arises due to encapsulation: if one of these locks is buried in 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
some routine that is getting called, the jump back to the beginning becomes more complex to implement. If the code had acquired some resources (other than L1) along the way, it must make sure to care­fully release them as well; for example, if after acquiring L1, the code had allocated some memory, it would have to release that memory upon failure to acquire L2, before jumping back to the top to try the entire sequence again. However, in limited circumstances (e.g., the Java vector method above), this type of approach could work well. 

Mutual Exclusion 
The .nal prevention technique would be to avoid the need for mu­tual exclusion at all. In general, we know this is dif.cult, because the code we wish to run does indeed have critical sections. So whatcan we do? 
Herlihy had the idea that one could design various data struc­tures to be wait-free [H91]. The idea here is simple: using powerful hardware instructions, you can build data structures in a manner that does not require explicit locking. 
As a simple example, let us assume we have a compare-and-swap 
instruction, which as you may recall is an atomic instructionpro­
vided by the hardware that does the following: 
int CompareAndSwap(int *address, int expected, int new) { 
if (*address == expected) { 
*address = new; 
return 1; // success 

} 
return 0; // failure 

} 
Imagine we now wanted to atomically increment a value by a cer­tain amount. We could do it as follows: 
void AtomicIncrement(int *value, int amount) { do {int old = *value; }while (CompareAndSwap(value, old, old+ amount) ==0); } 
Instead of acquiring a lock, doing the update, and then releasing 
it, we have instead built an approach that repeatedly tries toupdate 
the value to the new amount and uses the compare-and-swap to do 
OPERATING SYSTEMS ARPACI-DUSSEAU 
so. In this manner, no lock is acquired, and no deadlock can arise (though livelock is still a possibility). 
Let us consider a slightly more complex example: list insertion. Here is code that inserts at the head of a list: 
void insert(int value) {
node_t *n= malloc(sizeof(node_t));
assert(n != NULL);
n->value = value; 
n->next = head; 
head = n; } 
This code performs a simple insertion, but if called by multiple threads at the “same time”, has a race condition (see if you can.gure out why). Of course, we could solve this by surrounding this code with a lock acquire and release: 
void insert(int value) {
node_t *n= malloc(sizeof(node_t));
assert(n != NULL);
n->value = value; 
lock(listlock); // begin critical section 
n->next = head; 
head = n; 
unlock(listlock); // end of critical section } 
In this solution, we are using locks in the traditional manner1.In­stead, let us try to perform this insertion in a wait-free manner sim­ply using the compare-and-swap instruction. Here is one possible approach: 
void insert(int value) {
node_t *n= malloc(sizeof(node_t));
assert(n != NULL);
n->value = value; 
do { 
n->next = head; }while(CompareAndSwap(&head, n->next,n)); } 
1
The astute reader might be asking why we grabbed the lock so late, instead of right when entering the insert() routine; can you, astute reader, .gure out why that is OK? 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
The code here updates the next pointer to point to the current head, and then tries to swap the newly-created node into position as the new head of the list. However, this will fail if some other thread successfully swapped in a new head in the meanwhile, causing this thread to retry again with the new head. 
Of course, building a useful list requires more than just a list in­sert, and not surprisingly building a list that you can insertinto, delete from, and perform lookups on in a wait-free manner is non­trivial. Read more of the rich literature on wait-free synchronization if you .nd this interesting. 


31.4 Avoidance via Scheduling 
Instead of deadlock prevention, in some scenarios deadlock avoid­
ance is preferable. Avoidance requires some global knowledge of 
which locks various threads might grab during their execution, and 
subsequently schedules said threads in a way as to guarantee no 
deadlock can occur. 
For example, assume we have two processors and four threads which must be scheduled upon them. Assume further we know that Thread 1 (T1) grabs locks L1 and L2 (in some order, at some point during its execution), T2 grabs L1 and L2 as well, T3 grabs justL2, and T4 grabs no locks at all. In tabular form: 
T1 T2 T3 T4 L1 yes yes no no L2 yes yes yes no 
Asmart scheduler could thus compute that as long as T1 and T2 
are not run at the same time, no deadlock could ever arise. Hereis 
one such schedule: 

T4
CPU 1 

Note that it is OK for (T3 and T1) or (T3 and T2) to overlap. Even 
though T3 grabs lock L2, it can never cause a deadlock by running 
concurrently with other threads because it only grabs one lock. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
Let’s look at one more example. In this one, there is more con­tention for the same resources (again, locks L1 and L2), as indicated by the following contention table: 
T1 T2 T3 T4 L1 yes yes yes no L2 yes yes yes no 
In particular, threads T1, T2, and T3 all need to grab both locks L1 and L2 at some point during their execution. Here is a possible schedule that guarantees that no deadlock could ever occur: 
CPU 1 
CPU 2 

T4  
T1  T2  T3  

As you can see, static scheduling leads to a conservative approach where T1, T2, and T3 are all run on the same processor, and thus the total time to complete the jobs is lengthened considerably. Though it may have been possible to run these tasks concurrently, thefear of deadlock prevents us from doing so, and the cost is performance. 
One famous exampleof an approachlikethis isDijkstra’sBanker’s 
Algorithm [D64], and many similar approaches have been described 
in the literature. Unfortunately, they are only useful in very limited 
environments, for example, in an embedded system where one has 
full knowledge of the entire set of tasks that must be run and the 
locks that they need. Further, such approaches can limit concurrency, 
as we saw in the second example above. Thus, avoidance of dead­
lock via scheduling is not a widely-used general-purpose solution. 

31.5 Detect and Recover 
One .nal general strategy is to allow deadlocks to occasionally occur, and then take some action once such a deadlock has been de­tected. For example, if an OS froze once a year, you would just reboot it and get happily (or grumpily) on with your work. If deadlocks are rare, such a non-solution is indeed quite pragmatic. 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
DESIGN TIP:TOM WEST’S LAW 
Tom West, famous as the subject of the classic computer-industry book “Soul of a New Machine” [K81], says famously: “Not every­thing worth doing is worth doing well”, which is a terri.c engineer­ing maxim. If a bad thing happens rarely, certainly one shouldnot spend a great deal of effort to prevent it, particularly if thecost of the bad thing occurring is small. 
Many database systems employ deadlock detection and recov­ery techniques. A deadlock detector runs periodically, building a resource graph and checking it for cycles. In the event of a cycle (deadlock), the system needs to be restarted. If more intricate repair of data structures is .rst required, a human being may be involved to ease the process. 

31.6 Summary 
We have brie.y discussed deadlock: why it occurs, and what can be done about it. The problem is as old as concurrency itself, and many hundreds of papers have been written about the topic. The best solution in practice is to be careful, develop a lock acquisition total order, and thus prevent deadlock from occurring in the .rst place. Wait-free approaches also have promise, as some wait-free data structures are now .nding their way into commonly-used li­braries and critical systems, including Linux. However, their lack of generality and the complexity to develop a new wait-free datastruc­ture will likely limit the overall utility of this approach. Perhaps the best solution is to develop new concurrent programming models: in systems such as MapReduce (from Google) [GD02], programmers can describe certain types of parallel computations withoutany locks whatsoever. Locks are problematic by their very nature; thus, per­haps we should seek to avoid using them unless we truly have to. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

References 
[C+71] “System Deadlocks” 
E.G. Coffman, M.J. Elphick, A. Shoshani ACM Computing Surveys, 3:2, June 1971 
The classic paper outlining the conditions for deadlock and how you might go about dealing with it. There are certainly some earlier papers on this topic; seethe references within this paper for details. 
[D64] “Een algorithme ter voorkoming van de dodelijke omarming” Circulated privately, around 1964 Available: http://www.cs.utexas.edu/users/EWD/ewd01xx/EWD108.PDF 
Indeed, not only did Dijkstra come up with a number of solutions to the deadlock problem, he was the .rst to note its existence, at least in written form. However, he called it the “deadly embrace”, which (thankfully) did not catch on. 
[GD02] “MapReduce: Simpli.ed Data Processing on Large Clusters” Sanjay Ghemawhat and Jeff Dean OSDI 2004 
The MapReduce paper ushered in the era of large-scale data processing, and proposes a framework for performing such computations on clusters of generally unreliable machines. 
[H91] “Wait-free Synchronization” Maurice Herlihy ACM TOPLAS, 13(1), pages 124-149, January 1991 
Herlihy’s work pioneers the ideas behind wait-free approaches to writing concurrent programs. These approaches tend to be complex and hard, often more dif.cult than using locks correctly, probably limiting their success in the real world. 
[J+08] “Deadlock Immunity: Enabling Systems To Defend Against Deadlocks” Horatiu Jula, Daniel Tralamazza, Cristian Zam.r, George Candea OSDI ’08, San Diego, CA, December 2008 
An excellent recent paper on deadlocks and how to avoid getting caught in the same ones over and over again in a particular system. 
[K81] “Soul of a New Machine” Tracy Kidder, 1980 
Amust-read for any systems builder or engineer, detailing the early days of how a team inside Data General (DG), led by Tom West, worked to produce a “new machine.” Kidder’s other book are also excellent, in particular, “Mountains beyond Mountains”. Or maybe you don’t agree with me, comma? 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
32 



Real-World Concurrency Bugs(INCOMPLETE) 
Researchers have spent a great deal of time and effort lookinginto concurrency bugs over the past years. Application What it does Non-Deadlock Deadlock 
MySQL Apache Mozilla OpenOf.ce Total  Database Server Web Server Web Browser Of.ce Suite  14 13 41 6 74  9 4 16 2 31  
Table 32.1: xxx  
Atomicity violation::  
Thread 1:: if (thd->proc_info) { ... fputs(thd->proc_info, ...); ... }  
Thread 2:: thd->proc_info = NULL;  
Order violation::  
Thread 1:: void init() { ... mThread = PR_CreateThread(mMain, ...); ...  
407  

}  
Thread 2::  
void mMain(...) {  
...  
mState = mThread->State;  
...  
}  
OPERATING  
SYSTEMS  ARPACI-DUSSEAU  

References 
“Learning from Mistakes – A Comprehensive Study on Real World Concurrency Bug Characteristics” Shan Lu, Soyeon Park, Eunsoo Seo, Yuanyuan Zhou ASPLOS ’08, March 2008, Seattle, Washington 
The .rst in-depth study of concurrency bugs in real software,and the basis for this chapter. Look at YY Zhou’s or Shan Lu’s web pages for many more interesting papers on bugs. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
33 



Summary Dialogue on Concurrency 
Professor: So, does your head hurt now? 
Student: (taking two Motrin tablets) Well, some. It’s hard to think about all the ways threads can interleave. 
Professor: Indeed it is. I am always amazed at how so few line of code, when concurrent execution is involved, can become nearly impossible to understand. 
Student: Me too! It’s kind of embarrassing, as a Computer Scientist, not to be able to make sense of .ve lines of code. 
Professor: Oh, don’t feel too badly. If you look through the .rst papers on concurrent algorithms, they are sometimes wrong! And the authors often professors! 
Student: (gasps) Professors can be ... umm... wrong? 
Professor: Yes, it is true. Though don’t tell anybody – it’s one of our trade secrets. 
Student: Iam sworn to secrecy. But if concurrentcode is so hard to think about, and so hard to get right, how are we supposed to write correct con­current code? 
Professor: Well that is the real question, isn’t it? I think it starts with afew simple things. First, keep it simple! Avoid complex interactions between threads, and use well-known and tried-and-true waysto manage 
411 
thread interactions. 
Student: Like simple locking, and maybe a producer-consumer queue? 
Professor: Exactly! Those are common paradigms, and you should be able to produce the working solutions given what you’ve learned. Second, only use concurrency when absolutely needed; avoid it if at all possible. There is nothing worse than premature optimization of a program. 
Student: Isee – why add threads if you don’t need them? 
Professor: Exactly. Third, if you really need parallelism, seek it in other simpli.ed forms. For example, the Map-Reduce method for writing parallel data analysis code is an excellent example of achieving parallelism without having to handle any of the horri.c complexities of locks, condition vari­ables, and the other nasty things we’ve talked about. 
Student: Map-Reduce, huh? Sounds interesting – I’ll have to read more about it on my own. 
Professor: Good! You should. In the end, you’ll have to do a lot of that, as what we learn together can only serve as the barest introduction to the wealth of knowledge that is out there. Read, read, and read some more! And then try things out, write some code, and then write some more too. As Gladwell talks about in his book “Outliers”, you need to putroughly 10,000 hours into something in order to become a real expert. You can’t do that all inside of class time! 
Student: Wow, I’m not sure if that is depressing, or uplifting. But I’ll assume the latter, and get to work! Time to write some more concurrent code... 
OPERATING SYSTEMS ARPACI-DUSSEAU 

Part III 
Persistence 

413 
34 


ADialogue on Persistence 
Professor: And thus we reach the third of our four pillars of operating 
systems: persistence. 

Student: But what is persistence, oh .ne and good professor? 
Professor: Actually, you probably know what it means in the traditional 
sense, right? As the dictionary would say: “a .rm or obstinatecontinuance 
in a course of action in spite of dif.culty or opposition.” 

Student: It’s kind of like taking your class: some obstinance required. 

Professor: Ha! Yes. But persistence here means something else. Let me 
explain. Imagine you are outside, in a .eld, and you pick a – 
Student: (interrupting) I know! A peach! From a peach tree! 
Professor: Iwas going to say apple, from an apple tree. Oh well; we’ll do 

it your way, I guess. 
Student: (stares blankly) 
Professor: Anyhow, you pick a peach; in fact, you pick many many peaches, 

but you want to make them last for a long time. Winter is hard andcruel in 

Wisconsin, after all. What do you do? 
Student: Well, I think there are some different things you can do. You can 
pickle it! Or bake a pie. Or make a jam of some kind. Lots of fun! 

415 
Professor: Fun? Well, maybe. Certainly, you have to do a lot more work to  
make the peach persist.And so it is with information as well; making in­ 
formation persist, despite computer crashes, disk failures, or power outages  
is a tough and interesting challenge.  
Student: Nice segue; you’re getting quite good at that.  
Professor: Thanks!  A professor can always use a few kind words, you  
know.  
Student: I’ll try to remember that. I guess it’s time to stop talking peaches,  
and start talking computers?  
Professor: Yes, it is that time...  
OPERATING  
SYSTEMS  ARPACI-DUSSEAU  

35 


I/O Devices 
Before delving into the main content of this part of the notes (File Systems), we .rst introduce the concept of an input/output (I/O) de­vice and show how the operating system might interact with such an entity. I/O is quite critical to computer systems, of course;imagine a program without any input (it produces the same result each time); now imagine a program with no output (what was the purpose of it running?). Clearly, for computer systems to be interesting, both input and output are required. 
35.1 System Architecture 
To begin our discussion, let’s look at the structure of a typical sys­tem (Figure 35.1). The picture shows a single CPU attached to the main memory of the system via some kind of memory bus or in­terconnect. Some devices are connected to the system via a general I/O bus,which in many modern systems wouldbe PCI (or one if its many derivatives); graphics and some other higher-performance I/O devices might be found here. Finally, even lower down are one or more of what we call a peripheral bus,such as SCSI, SATA,or USB. These connect the slowest devices to the system, including disks, mice,andother similar components. 
One question you might ask is: why do we need a hierarchical structure like this? Put simply: physics, and cost. The faster a bus is, the shorter it must be; thus, a high-performance memory bus does not have much room to plug devices and such into it. In addition, engineering a bus for high performance is quite costly. Thus,system 
417 

Memory Bus (proprietary) 
General I/O Bus (e.g., PCI) 
Peripheral I/O Bus (e.g., SCSI, SATA, USB) 
Figure 35.1: Prototypical System Architecture. 
designers have adopted this hierarchical approach, where compo­nents that demands high performance (such as the graphics card) are nearer the CPU. Lower performance components are further away. The bene.t of placing disks and other slow devices on a peripheral bus are many; in particular, you can place many devices on sucha slow bus (PCI, for example, might limit you to only a few devices). 

35.2 A Canonical Device 
Let us now look at a canonical device (not a real one), and use this device to drive our understanding of some of the machinery required to make device interaction ef.cient. From Figure 35.2, we cansee that a device has two important components. The .rst is the hard­ware interface it presents to the rest of the system. Just like a piece of software, hardware must also present some kind of interface that allows the system software to control its operation. Thus, all devices have some speci.ed interface and protocol for typical interaction. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

Interface 
Internals 
Figure 35.2: A Canonical Device. 
The second part of any device is its internal structure.This part of the device is implementation speci.c and is responsible for im­plementing the abstraction the device presents to the system. Very simple devices will have one or a few hardware chips to implement their functionality; more complex devices will include a simple CPU, some general purpose memory, and other device-speci.c chipsto get their job done. For example, modern RAID controllers might consist of hundreds of thousands of lines of .rmware (i.e., software within ahardware device) to implement various RAID functionality (we’ll learn more about RAID later). 

35.3 The Canonical Protocol 
In the picture above, the (simpli.ed) device interface is comprised of three registers: a status register, which can be read to see the cur­rent status of the device; a command register, to tell the device to perform a certain task; and a data register to pass data to the device, or get data from the device. By reading and writing these registers, the operating system can control device behavior. 
Let us now describe a typical interaction that the OS might have 
with the device in order to get the device to do something on its 
behalf. The protocol is as follows: 
While (STATUS == BUSY)
;// waituntil device is notbusy 
Write data to DATA register 
Write command to COMMAND register 
(Doing so starts the device and executes the command) 
While (STATUS == BUSY);// waituntil device is donewith your request 
The protocol has four steps. In the .rst, the OS waits until thede-
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
vice is ready to receive a command by repeatedly reading the status register; we call this polling the device (basically, just asking it what is going on). Second, the OS sends some data down to the data regis­ter; one can imagine that if this were a disk, for example, thatmulti­ple writes would need to take place to transfer a disk block (say 4KB) to the device. When the main CPU is involved with the data move­ment (as in this example protocol), we refer to it as programmed I/O (PIO).Third,the OS writesa command to the command register; do­ing so implicitly lets the device know that both the data is present and that it should begin working on the command. Finally, the OS waits for the device to .nish by again polling it in a loop, waiting to see if it is .nished (it may then get an error code to indicate success or failure). 
This basic protocol has the positive aspect of being simple and working. However, there are some inef.ciencies and inconveniences involved. The .rst problem you might notice in the protocol isthat polling seems inef.cient; speci.cally, it wastes a great deal of CPU time just waiting for the (potentially slow) device to complete its ac­tivity, instead of switching to another ready process and thus better utilizing the CPU. 
THE CRUX:HOW TO AVOID THE COSTS OF POLLING How can the OS check device status without frequent polling, and thus lower the CPU overhead required to manage the device? 

35.4 Lowering CPU Overhead with Interrupts 
The invention that many engineers came upon years ago to im­prove this interaction is something we’ve seen already: the interrupt. Instead of polling the device repeatedly, the OS can issue a request, put the calling process to sleep, and context switch to another task. When the device is .nally .nished with the operation, it will raise a hardware interrupt, causing the CPU to jump into the OS at a pre­determined interrupt service routine (ISR) or more simply an inter­rupt handler.The handler is just a piece of operating system code that will .nish the request (for example, by reading data and per­haps an error code from the device) and wake the process waiting 
OPERATING SYSTEMS ARPACI-DUSSEAU 
for the I/O, which can then proceed as desired.  
Interrupts thus allow for overlap of computation and I/O, which  
is key for improved utilization. This timeline shows the problem:  
CPU Disk  1111111111ppppppppppp111111111111----------11111111111-----------­ 
In the diagram, process 1 runs on the CPU for some time (indi­cated by a repeated 1 on the CPU line), and then issues an I/O re­ 
quest to the disk to read some data. Without interrupts, the system  
simply spins, polling the status of the device repeatedly until the I/O  
is complete (indicated by a p). The disk then services the request and  
.nally process 1 can run again.  
If instead we utilize interrupts and allow for overlap, the OScan  
do something else while waiting for the disk:  
CPU  111111111122222222222111111111111  
Disk  ----------11111111111-----------­ 
In this example, the OS runs process 2 on the CPU while the disk  
services 1’s request. When the disk request is .nished, an interrupt  
occurs, and the OS wakes up 1 and runs it again. Thus, both the CPU  
and the disk are properly utilized during the middle stretch of time.  
Note that using interrupts is not always the best solution. For ex­ 
ample, imagine a device that performs its tasks very quickly:the .rst  
poll usually .nds the device to be done with task. Using an interrupt  
in this case will actually slow down the system: switching to another  
process, handling the interrupt, and switching back to the issuing  
process is expensive. Thus, if a device is fast, it may be best to poll;  
if it is slow, interrupts, which allow overlap, are best. If the speed  
of the device is not known, or sometimes fast and sometimes slow,  
it may be best to use a hybrid that polls for a little while and then,  
if the device is not yet .nished, uses interrupts.  This two-phased  
approach may achieve the best of both worlds.  
Another reason not to use interrupts arises in networks [MR96].  
When a huge stream of incoming packets each generate an interrupt,  
it is possible for the OS to livelock,that is,.nd itself only processing  
interrupts and never allowing a user-level process to run andactu­ 
ally service the requests. For example, imagine a web server that  
suddenly experiences a high load due to the “slashdot effect”. In this  
case, it is better to occasionally use polling to better control what is  
THREE  
ARPACI-DUSSEAU  EASY PIECES  
(V0.5)  

ASIDE:INTERRUPTS NOT ALWAYS BETTER THAN PIO 
Although interrupts allow for overlap of computation and I/O, they 
only really make sense for slow devices. Otherwise, the cost of in­
terrupt handling and context switching may outweigh the bene.ts 
interrupts provide. There are also cases where a .ood of interrupts 
may overload a system and lead it to livelock [MR96]; in such cases, 
polling provides more control to the OS in its scheduling and thus is 
again useful. 
happening in the system and allow the web server to service some requests before going back to the device to check for more packet arrivals. 
Another interrupt-based optimization is coalescing.In such a setup, a device which needs to raise an interrupt .rst waits for a bit before delivering the interrupt to the CPU. While waiting, other requests may soon complete, and thus multiple interrupts canbe co­alesced into a single interrupt delivery, thus lowering the overhead of interrupt processing. Of course, waiting too long will increase the latency of a request, a common trade-off in systems. See Ahmadet al. [A+11] for an excellent summary. 

35.5 More Ef.cient Data Movement with DMA 
Unfortunately, there is one other aspect of our canonical protocol that requires our attention. In particular, when using programmed I/O (PIO) to transfer a large chunk of data to a device, the CPU is once again overburdened with a rather trivial task, and thus wastes alot of time and effort that could better be spent running other pro­cesses. This timeline illustrates the problem: 
CPU 1111111111cccccc22222222222111111111111 
Disk ----------------11111111111-----------­
In the timeline, process 1 is running and then wishes to write some data to the disk. It then initiates the I/O, which must copy the data from memory to the device explicitly, one word at a time (marked c in the diagram). When the copy is complete, the I/O be­gins on the disk and the CPU can .nally be used for something else. 
OPERATING SYSTEMS ARPACI-DUSSEAU THE CRUX:HOW TO LOWER PIO OVERHEADS 
With PIO, the CPU spends too much time moving data to and 
from devices by hand. How can we of.oad this work and thus allow 
the CPU to be more effectively utilized? 
The solution to this problem is something we refer to as Direct Memory Access (DMA).A DMA engine is essentially a very speci.c device within a system that can orchestrate transfers between devices and main memory without much CPU intervention. 
The process is as follows. To transfer data to the device, for ex­ample, the OS would program the DMA engine by telling it where the data lives in memory, how much data to copy, and which device to send it to. At that point, the OS is done with the transfer andcan proceed with other work. When the DMA is complete, the DMA controller raises an interrupt, and the OS thus knows the transfer is complete. The revised timeline: 
CPU 111111111122222222222222222111111111111 DMA ----------cccccc----------------------­Disk ----------------11111111111-----------­
From the timeline, you can see that the copying of data is now handled by the DMA controller. Because the CPU is free during that time, the OS can do something else, here choosing to run process 2. Process 2 thus gets to use more CPU before process 1 runs again. 

35.6 Methods of Device Interaction 
Now that we have some sense of the ef.ciency issues involved with performing I/O, there are a few other problems we need to handle to incorporate devices into modern systems. One problem you may have noticed thus far: we have not really said anything about how the OS actually communicates with the device! Thus,the problem we need to deal with is: 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
THE CRUX:HOW TO COMMUNICATE WITH DEVICES How should the hardware communicate with a device? Should there be explicit instructions? Or are there other ways to do it? 
Over time, two primary methods of device communication have developed. The .rst, oldest method (used by IBM mainframes for many years) is to have explicit I/O instructions.These instructions specify a way for the OS to send data to speci.c device registers and thus allow the construction of the protocols described above. 
For example, on x86, the in and out instructions can be used to communicate with devices. For example, to send data to a device, the caller speci.es a register with the data in it, and a speci.c port which names the device in question. Executing the instruction leads to the desired behavior. 
Such instructions are usually privileged.The OS controls devices, and the OS thus is the only entity allowed to directly communicate with them. Imagine if any program could read or write the disk,for example: total chaos (as always), as any user program could use such aloophole to gain complete control over the machine. 
The second method to interact with devices is known as memory-mapped I/O.With thisapproach,the hardware makes device regis­ters available as if they were memory locations. To access a particular register, then, the OS would issue a load (to read) or a store (to write) to that address; the hardware then routes the load/store to the device instead of main memory. 
There is not some great advantage to one approach or the other. 
The memory-mapped approach is nice in that no new instructions 
are needed to support it, but both approaches are still in use today. 

35.7 Fitting into the OS: The Device Driver 
One .nal problem we will discuss: how to .t devices, each of which have very speci.c interfaces, into the OS, which we would like to keep as general as possible. For example, consider a .le system. We’d like to build a .le system that worked on top of SCSI disks, IDE disks, USB keychain drives, and so forth, and we’d like the.le system to be relatively oblivious to all of the details of how to issue 
OPERATING SYSTEMS ARPACI-DUSSEAU 
aread or write request to these difference types of drives. Thus, our problem: 
THE CRUX:HOW TO BUILD A DEVICE-NEUTRAL OS How can we keep most of the OS device-neutral, thus hiding the details of device interactions from major OS subsystems? 
The problem is solved through the age-old technique abstraction. At the lowest level, a piece of software in the OS must know in detail how a device works. We call this piece of software a device driver, and any speci.cs of device interaction are encapsulated within. 
Let us see how this abstraction might help OS design and imple­mentation by examining the Linux .le system software stack. Figure 
35.3 is a rough and approximate depiction of the Linux software or­ganization. As you can see from the diagram, a .le system (and cer­tainly, an application above) is completely oblivious to thespeci.cs of which disk class it is using; it simply issues block read andwrite requests to the generic block layer, which routes them to the appro­priate device driver, which handles the details of issuing the speci.c request. Although overly simpli.ed, the diagram shows how such detail can be successfully hidden from most of the OS. 
Note that such encapsulation can have its downside as well. For example, if there is a device that has many special capabilities, but has to present a generic interface to the rest of the kernel, those spe­cial capabilities will go unused. This situation arises, forexample, in Linux with SCSI devices, which have very rich error reporting; be­cause other block devices (e.g., ATA/IDE) have much simpler error handling, all that higher levels of software ever receive is agenericEIO (generic IO error) error code; any extra detail that SCSI may have provided is thus lost to the .le system [G08]. 
Interestingly, because device drivers are needed for any device you might plug into your system, over time they have come to rep­resent a huge percentage of kernel code. Studies of the Linux kernel reveal that over 70% of OS code is found in device drivers [C01]; for Windows-based systems, it is undoubtedly a much higher percent­age as Windows-based operating systems support many more de­vices. Thus, when people tell you that the OS has millions of lines of code, what they are really saying is that the OS has millions oflines 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
Application 

Figure 35.3: The File System Stack 
of device-driver code. Of course, for any given installation, most of that code may not be active (i.e., only a few devices are connected to the system at a time). Perhaps more depressingly, as drivers are often written by “amateurs” (instead of full-time kernel developers), they tend to have many more bugs and thus are a primary contributor to kernel crashes [S03]. 

35.8 Case Study: A Simple IDE Disk Driver 
To dig a little deeper here, let’s take a quick look at an actualde­vice: an IDE disk drive [L94]. We summarize the protocol as de­scribed in this reference [W10]; we’ll also peek at the xv6 source code for a simple example of a working IDE driver [CK+08]. 
An IDE disk presents a simple interface to the system, consisting of four types of register: control, command block, status, and error. These registers are available by reading or writing to speci.c “I/O addresses” (such as 0x3F6 below) using (on x86) the in and out I/O instructions. 
Control Register:Address 0x3F6 = 0x80 (0000 1RE0): R=reset, E=0 means "enable interrupt" 
Command Block Registers:Address 0x1F0 = Data Port Address 0x1F1 = Error Address 0x1F2 = Sector Count Address 0x1F3 = LBA low byte Address 0x1F4 = LBA mid byte Address 0x1F5 = LBA hi byteAddress 0x1F6 = 1B1D TOP4LBA: B=LBA, D=drive 
OPERATING SYSTEMS ARPACI-DUSSEAU 
Address 0x1F7 = Command/status  
Status Register (Address 0x1F7):7 6 5 4 3 2 1 0 BUSY READY FAULT SEEK DRQ CORR IDDEX ERROR  
Error Register (Address 0x1F1): (check when Status ERROR==1)7 6 5 4 3 2 1 0 BBK UNC MC IDNF MCR ABRT T0NF AMNF  
BBK = Bad Block UNC = Uncorrectable data error MC = Media Changed IDNF = ID mark Not Found MCR = Media Change Requested ABRT = Command aborted T0NF = Track 0 Not Found AMNF = Address Mark Not Found  
The basic protocol to interact with the device is as follows, assum­ing it has already been initialized.  
• Wait for drive to be ready. Read Status Register (0x1F7) until drive is not busy and READY. • Write parameters to command registers. Write the sector count, logical block address (LBA) of the sectors to be accessed, and drive number (master=0x00 or slave=0x10, as IDE permits just two drives) to command registers (0x1F2-0x1F6). • Start the I/O. by issuing read/write to command register. Write READ—WRITE command to command register (0x1F7). • Data transfer (for writes): Wait until drive status is READY and DRQ (drive request for data); write data to data port. • Handle interrupts. In the simplest case, handle an interrupt for each sector transferred; more complex approaches allow some batching and thus one .nal interrupt when the entire transfer is complete. • Error handling. After each operation, read the status register. If the ERROR bit is on, read the error register for details.  
Most of this protocol is found in the xv6 IDE driver, which (af­ter initialization) works through four primary functions. The .rst is ide rw(),which queues a request (if there are others pending), or issues it directly to the disk (via ide start request()); in either case, the routine waits for the request to complete and the calling pro- 
THREE  
ARPACI-DUSSEAU  EASY PIECES  
(V0.5)  

static int ide_wait_ready() {while (((int r = inb(0x1f7)) & IDE_BSY) || !(r & IDE_DRDY)) ; // loop until drive isn’t busy } 
static void ide_start_request(struct buf *b) { ide_wait_ready();outb(0x3f6, 0); // generate interrupt outb(0x1f2, 1); // how many sectors? outb(0x1f3, b->sector & 0xff); // LBA goes here ... outb(0x1f4, (b->sector >> 8) & 0xff); // ... and here outb(0x1f5, (b->sector >> 16) & 0xff); // ... and here! outb(0x1f6, 0xe0 | ((b->dev&1)<<4) | ((b->sector>>24)&0x0f));if(b->flags & B_DIRTY){
outb(0x1f7, IDE_CMD_WRITE); // this is a WRITE outsl(0x1f0, b->data, 512/4); // transfer data too! }else {outb(0x1f7, IDE_CMD_READ); // this is a READ (no data) }} 
void ide_rw(struct buf *b) { acquire(&ide_lock);for (struct buf **pp = &ide_queue; *pp; pp=&(*pp)->qnext) 
; // walk queue *pp = b; // add request to end if (ide_queue == b) // if q is empty 
ide_start_request(b); // send req to disk while ((b->flags & (B_VALID|B_DIRTY)) != B_VALID)sleep(b, &ide_lock); // wait for completion release(&ide_lock);} 
void ide_intr() { struct buf *b; acquire(&ide_lock);if (!(b->flags & B_DIRTY) && ide_wait_ready(1) >= 0)
insl(0x1f0, b->data, 512/4); // if READ: get data b->flags |= B_VALID;b->flags &= ˜B_DIRTY;wakeup(b); // wake waiting process if ((ide_queue = b->qnext) != 0) // start next request
ide_start_request(ide_queue); // (if one exists)release(&ide_lock);} 
Figure 35.4: The xv6 IDE Disk Driver (Simpli.ed) 
OPERATING SYSTEMS ARPACI-DUSSEAU 
cess is put to sleep. The second is ide start request(),which is used to send a request (and perhaps data, in the case of a write)to the disk; the in and out x86 instructions are called to read and write device registers, respectively. The start request routine uses the third function, ide wait ready(),to ensure the drive is ready before issuing a request to it. Finally, ide intr() is invoked when an in­terrupt takes place; it reads data from the device (if the request is a read, not a write), wakes the process waiting for the I/O to complete, and (if there are more requests in the I/O queue), launches thenext I/O via ide start request(). 

35.9 Historical Notes 
Before ending, we include a brief historical note on the origin of 
some of these fundamental ideas. If you are interested in learning 
more, read Smotherman’s excellent summary [S08]. 
Interrupts are an ancient idea, existing on the earliest of machines. For example, the UNIVAC in the early 1950’s had some form of inter­rupt vectoring, though it is unclear in exactly which year this feature was available [S08]. 
There is also some debate as to which machine .rst introduced the idea of DMA. For example, Knuth and others point to the DYSEAC (a “mobile” machine, which at the time meant it could be hauledin a trailer), whereas others think the IBM SAGE may have been the .rst [S08]. Either way, by the mid 1950’s, systems with I/O devicesthat communicated directly with memory and then interrupted the CPU when .nished were being built. 
The history here is dif.cult to trace because the inventions are tied to real, and sometimes obscure, machines. For example, some think that the Lincoln Labs TX-2 machine was .rst with vectored interrupts [S08], but this is hardly clear. 
Because the ideas are relatively obvious – no Einsteinian leap is required to come up with the idea of letting the CPU do something else while a slow I/O is pending – perhaps our focus on “who .rst?” is misguided. What is certainly clear: as people built these early ma­chines, it became obvious that some support was needed for I/O. Interrupts, DMA, and related ideas are all direct outcomes ofthe na­ture of fast CPUs and slow devices; if you were there at the time, you might have had similar ideas. 
ARPACI-DUSSEAU 







THREE EASY PIECES (V0.5) 
35.10  Summary  
You should now have a very basic understanding of how an OS  
interacts with a device.  Two techniques, the interrupt and DMA,  
have been introduced to help with device ef.ciency, and two ap­ 
proaches to accessing device registers, explicit I/O instructions and  
memory-mapped I/O, have been described. Finally, the notionof a  
device driver has been presented, showing how the OS itself can en­ 
capsulate low-level details and thus make it easier to build the rest  
of the OS in a device-neutral fashion.  
OPERATING  
SYSTEMS  ARPACI-DUSSEAU  


References 
[A+11] “vIC: Interrupt Coalescing for Virtual Machine Storage Device IO” Irfan Ahmad, Ajay Gulati, Ali Mashtizadeh USENIX ’11 
Aterri.c survey of interrupt coalescing in traditional and virtualized environments. 
[C01] “An Empirical Study of Operating System Errors” Andy Chou, Junfeng Yang, Benjamin Chelf, Seth Hallem, DawsonEngler SOSP ’01 
One of the .rst papers to systematically explore how many bugsarein modern operating systems. Among other neat .ndings, the authors show that device drivers have something like seven times more bugs than mainline kernel code. 
[CK+08] “The xv6 Operating System” Russ Cox, Frans Kaashoek, Robert Morris, Nickolai Zeldovich From: http://pdos.csail.mit.edu/6.828/2008/index.html 
See ide.c for the IDE device driver, with a few more details therein. 
[D07] “What Every Programmer Should Know About Memory” Ulrich Drepper November, 2007 Available: http://www.akkadia.org/drepper/cpumemory.pdf 
Afantastic read about modern memory systems, starting at DRAM and going all the way up to virtualization and cache-optimized algorithms. 
[G08] “EIO: Error-handling is Occasionally Correct” 
Haryadi Gunawi, Cindy Rubio-Gonzalez, Andrea Arpaci-Dusseau, Remzi Arpaci-Dusseau, 
Ben Liblit 
FAST ’08 

[L94] “AT Attachment Interface for Disk Drives” 
Lawrence J. Lamers, X3T10 Technical Editor 
Available: ftp://ftp.t10.org/t13/project/d0791r4c-ATA-1.pdf 
Reference number: ANSI X3.221 -1994 

[MR96] “Eliminating Receive Livelock in an Interrupt-driven Kernel” 
Jeffrey Mogul and K. K. Ramakrishnan 
USENIX ’96 

THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
[S08] “Interrupts” Mark Smotherman, as of July ’08 Available: http://www.cs.clemson.edu/ mark/interrupts.html 
Atreasure trove of information on the history of interrupts,DMA, and related early ideas in computing. 
[S03] “Improving the Reliability of Commodity Operating Systems” Michael M. Swift, Brian N. Bershad, and Henry M. Levy SOSP ’03 
Swift’s work revived interest in a more microkernel-like approach to operating systems; mini­mally, it .nally gave some good reasons why address-space based protection could be useful in a modern OS. 
[W10] “Hard Disk Driver” Washington State Course Homepage Available: http://eecs.wsu.edu/ cs460/cs560/HDdriver.html 
Anice summary of a simple IDE disk drive’s interface and howtobuild a device driver for it. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
36 



Hard Disk Drives 
The last note introduced the general concept of an I/O device and showed you how the OS might interact with such a beast. In this note, we dive into more detail about one device in particular:the hard disk drive.These drives have been the main form of persistent data storage in computer systems for decades and much of the de­velopment of .le system technology (coming soon) is predicated on their behavior. Thus, it is worth understanding the details of a disk’s operation before building the .le system software that manages it. Many of these details are available in excellent papers by Ruemmler et al. [RW92] and Anderson et al. [ADR03]. 
36.1 The Interface 
Let’s start by understanding the interface to a modern disk drive. The basic interface for all modern drives is straightforward. The drive consists of a large number of sectors (512-byte blocks), each of which can be read or written. The sectors are numbered from 0 to n- 1 on a disk with n sectors. Thus, we can view the disk as an array of sectors; 0 to n - 1 is thus the address space of the drive. 
Multi-sector operations are possible; indeed, many .le systems will read or write 4KB at a time (or more). However, when updating the disk, the only guarantee drive manufactures make is that asingle 512-byte write is atomic (i.e., it will either complete in its entirety or it won’t complete at all); thus, if an untimely power loss occurs, only aportion of alarger write may complete (sometimes called a torn write). 
433 There are some assumptions most clients of disk drives make, but 
that are not speci.ed directly in the interface; Schlosser and Ganger 
have called this the “unwritten contract” of disk drives [SG04]. Specif­
ically, one can usually assume that accessing two blocks thatare near 
one-another within the drive’s address space will be faster than ac­
cessing two blocks that are far apart. One can also usually assume 
that accessing blocks in a contiguous chunk (i.e., a sequential read or 
write) is the fastest access mode, and usually much faster than any 
more random access pattern. 

36.2 Basic Geometry 
Let’s start to understand some of the components of a modern disk. We start with a platter,a circular hardsurface on which data is stored persistently by inducing magnetic changes to it. A disk may have one or more platters; each platter has 2 sides, each of which is called a surface.These platters are usually made of some hard material (such as aluminum), and then coated with a thin magnetic layer that enables the drive to persistently store bits even when the drive is powered off. 
The platters are all bound together around the spindle,which is connected to a motor that spins the platters around and around (while the drive is powered on) at a constant .xed rate. The rate of rotation is often measured in rotations per minute (RPM),and typ­ical modern values are in the 7,200 RPM to 15,000 RPM range. Note that we will often be interested in the time of a single rotation, e.g., a drive that rotates at 10,000 RPM means that a single rotation takes 6 milliseconds (6 ms). 
Data is encoded on each surface in concentric circles of sectors; we call one such concentric circle a track.A single surface contains many thousands and thousands of tracks, tightly packed together, with hundreds of tracks .tting into the width of a human hair. 
To read and write from the surface, we need a mechanism that allows us to either sense (i.e., read) the magnetic patterns on the disk or to induce a change in (i.e., write) them. This process of reading and writing is accomplished by the disk head;there is one such head per surface of the drive. The disk head is attached to a single disk arm,which moves across the surface to position the head over the desired track. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

36.3 A Simple Disk Drive 
Let us now understand how this all works by building up our 
model of how a disk drive works, one track at a time. Assume we 
have a very simple disk, with only a single track (Figure 36.1): 

Figure 36.1: A disk with just a single track 
This track has just 12 sectors, each of which is 512 bytes in size (our typical sector size, recall) and addressed therefore bythe num­bers 0 through 11. The single platter we have here rotates around the spindle, to which a motor is attached. Of course, the track by itself isn’t too interesting; we want to be able to read or write thosesectors, and thus we need a disk head, attached to a disk arm, as we now see (Figure 36.2). 

THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
OPERATING SYSTEMS 

Single-track Latency: The Rotational Delay 
To understand how a request would be processed on our simple, one-track disk, imagine we now receive a request to read block0. How should the disk service this request? 
In our simple disk, the disk doesn’t have to do much. In par­ticular, it must just wait for the desired sector to rotate under the disk head. This wait happens often enough in modern drives, and is an important enough component of I/O service time, that it has aspecial name: rotational delay (sometimes rotation delay,though doesn’t that sound weird?). In the example, if the full rotational de­
lay is R,the disk would have to incur a rotational delay of about R 
2 
to wait for 0 to come under the read/write head (if we start at 6). A worst-case request on this single track would be to sector 5, causing nearly a full rotational delay in order to service such a request. 

Multiple Tracks: Seek Time 
So far our disk just has a single track, which is not too realistic; mod­ern disks of course have many thousands of tracks. Let’s thus look at ever-so-slightly more realistic disk surface, this one withthree tracks (Figure 36.3). 

Figure 36.3: Three tracks plus a head 
ARPACI-DUSSEAU 
In the .gure, the head is currently positioned over the innermost track (which contains sectors 24 through 35); the next track over con­tains the next set of sectors (12 through 23), and the outermost track contains the .rst sectors (0 through 11). 
To understand how the drive might access a given sector, we now trace what would happen on a request to a distant sector, e.g.,a read to sector 11. To accomplish this read, the drive has to .rst move the disk arm to the correct track (in this case, the outermost track), in a process known as a seek.Seeks, along with rotations, are one of the most costly disk operations. 
The seek, it should be noted, has many phases: .rst an acceleration phase as the disk arm gets moving; then coasting as the arm is moving at full speed, then deceleration as the arm slows down; .nally settling as the head is carefully positioned over the correct track. The settling time is often quite signi.cant, e.g., 0.5 to 2 ms, as the drive must be certain to .nd the right track (imagine if it just got close instead!). 
After the seek, the disk arm has positioned the head over the right track. Thus, the state of the disk might look like this (Figure36.4). 

Figure 36.4: Three tracks plus a head: After seeking 
As we can see in the diagram, during the seek, the arm has been moved to the desired track, and the platter of course has rotated, in this case about 3 sectors. Thus, sector 9 is just about to pass under the disk head, and we must only endure a short rotational delayto complete the transfer. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
OPERATING SYSTEMS 
When sector 11 passes under the disk head, the .nal phase of I/O will take place, which is known as the transfer,where data is either read from or written to the surface. And thus, we have a complete picture of I/O time: .rst a seek, then waiting for the rotational delay, and .nally the transfer. 

Some Other Details 
Though we won’t spend too much time on it, there are some other interesting details about how hard drives operate. Many drives em­ploy some kind of track skew to make sure that sequential reads can be properly serviced even when crossing track boundaries. Inour simple example disk, this might appear as seen in Figure 36.5. 

Figure 36.5: Three tracks: Track Skew of 2 Sectors are often skewed like this because when switching from one track to another, the disk needs time to reposition the head (even to neighboring tracks). Without such skew, the head would be moved to the next track but the desired next block would have alreadyro­tated under the head, and thus the drive would have to wait almost the entire rotational delay to access the next block. Another reality is that outer tracks tend to have more sectorsthan inner tracks, which is a result of geometry; there is simply more room out there. These tracks are often referred to as multi-zoned disk drives, where the disk is organized into multiple zones, and where a 
ARPACI-DUSSEAU 
zone is consecutive set of tracks on a surface. Each zone has the same number of sectors per track, and outer zones have more sectorsthan inner zones. 
Finally, an important part of any modern disk drive is its cache, for historical reasons sometimes called a track buffer.This cache is just some small amount of memory (usually around 8 or 16 MB) which the drive can use to hold data read from or written to the disk. For example, when reading a sector from the disk, the drive might decide to read in all of the sectors on that track and cache themin its memory; doing so allows the drive to quickly respond to any subse­quent requests to the same track. 
On writes, the drive has a choice: should it acknowledge the write has completed when it has put the data in its memory, or after the write has actually been written to disk? The former is called write back caching (or sometimes immediate reporting), and the latter write through.Write back caching sometimes makes the drive ap­pear “faster”, but can be dangerous; if the .le system or applications require that data be written to disk in a certain order for correctness, write-back caching can lead to problems (e.g., read about journaling). 


36.4 I/O Time: Doing The Math 
Now that we have an abstract model of the disk, we can use a little analysis to better understand disk performance. In particular, we can now represent I/O time as the sum of the three major componentsof I/O time: 
TI/O = Tseek + Trotation + Ttransfer (36.1) 
Note that the rate of I/O (RI/O), which is often more easily used for comparison between drives (as we will do below), is easilycom­puted from the time. Simply divide the size of the transfer by the time it took: 
SizeTransfer 
RI/O = (36.2) TI/O 
To get a better feel for I/O time, let us perform the following cal­culation. Assume there are two workloads we are interested in. The 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
Cheetah 15K.5 Barracuda 
Capacity 300 GB 1 TB RPM 15,000 7,200 Average Seek 4 ms 9 ms Max Transfer 125 MB/s 105 MB/s Platters 4 4 Cache 16 MB 16/32 MB Connects via SCSI SATA 
Table 36.1: Disk Drives: SCSI vs. SATA 
.rst, known as the random workload, issues small (e.g., 4KB) reads 
to random locations on the disk. Random workloads are common in 
many important applications, including database management sys­
tems. The second, known as the sequential workload, simply reads 
alarge number of sectors consecutively from the disk, without jump­
ing around. Sequential access patterns are quite common and thus 
important as well. 
To understand the difference in performance between random and sequential workloads, we need to make a few assumptions about the disk drive .rst. Let’s look at a couple of modern disks from Seagate. The .rst, known as the Cheetah 15K.5 [S09b], is a high-performance SCSI drive. Its performance characteristics are found in Table 36.1. The second, the Barracuda [S09a], is a drive built for capacity; its characteristics are also found in the table. 
As you can see, the drives have quite different characteristics, and in many ways nicely summarize two important components of the disk drive market. The .rst is the “high performance” drive market, where drives are engineered to spin as fast as possible, deliver low seek times, and transfer data quickly. The second is the “capacity” market, where cost per byte is the most important aspect; thus, the drives are slower but pack as many bits as possible into the space available. 
From these numbers, we can start to calculate how well the drives would do under our two workloads outlined above. Let’s start by looking at the random workload. Assuming each 4 KB read occurs at a random location on disk, we can calculate how long each such read would take. On the Cheetah: 
Tseek =4ms, Trotation =2ms, Ttransfer =30microsecs (36.3) 
OPERATING SYSTEMS ARPACI-DUSSEAU 
ASIDE:COMPUTING THE “AVERAGE”SEEK In many books and papers, you will see average disk-seek time cited as being roughly one-third of the full seek time. Where does thiscome from? 
Turns out it arises from a simple calculation based on averageseek dis-tance,not time. Imagine the disk as a set of tracks,from 0 to N .The seek distance between any two tracks x and y is thus computed as the absolute value of the difference between them: |x - y|. 
To compute the average seek distance, all you need to do is to .rst add up all possible seek distances: 
NN
  
|x - y|. (36.4) 
x=0 y=0 Then, divide this by the number of different possible seeks: N2.To com-
Solving this leads to (xy - 12)+(12 - xy)
pute the sum, we’ll just use the integral form: 
 N   N  
|x - y| dy dx.  (36.5)  
x=0 y=0  
To compute the inner integral, let’s break out the absolute value: 
 x   N  
(x - y)dy + (y - x)dy.  (36.6)  
y=0  y=x  
 x   N  

y y which can be simpli­
22
0 x .ed to (x2 - Nx + 1 N2). Now we have to compute the outer integral:
2 
 N 
(x 2 - Nx +1 N2)dx, (36.7) 
2 
x=0 
which results in: 
 N 
1 N 2 N2 N3 
3 -   
( xx + x)= . (36.8) 
32 2  3 
0 Remember that we still have to divide by the total number of seeks (N2) 1
to compute the average seek distance: (N3 )/(N 2 )= N.Thus the average 
33 
seek distance on a disk, over all possible seeks, is one-thirdthe full distance. And now when you hear that an average seek is one-third of a fullseek, you’ll know where it came from. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
Cheetah Barracuda 
RI/O Random 0.66 MB/s 0.31 MB/s RI/O Sequential 125 MB/s 105 MB/s 
Table 36.2: Disk Drives: SCSI vs. SATA The average seek time (4 milliseconds) is just taken as the average time reported by the manufacturer; note that a full seek (fromone end of the surface to the other) would likely take two or three times longer. The average rotational delay is calculated from the RPM di­rectly. 15000 RPM is equal to 250 RPS (rotations per second); thus, each rotation takes 4 ms. On average, the disk will encounter ahalf rotation and thus 2 ms is the average time. Finally, the transfer time is just the size of the transfer over the peak transfer rate; here it is vanishingly small (30 microseconds;note that we need 1000 microsec­onds just to get 1 millisecond!). Thus, from our equation above, TI/O for the Cheetah roughly equals 6 ms. To compute the rate of I/O, we just divide the size of the transfer by the average time, and thus arrive at RI/O for the Cheetah under the random workload of about 0.66 MB/s. The same calculation for the Barracuda yields a TI/O of about 13.2 ms, more than twice as slow, and thus a rate of about 0.31 MB/s. Now let’s look at the sequential workload. Here we can assume there is a single seek and rotation before a very long transfer. For sim­plicity, assume the size of the transfer is 100 MB. Thus, TI/O for the Barracuda and Cheetah is about 800 ms and 950 ms, respectively. The rates of I/O are thus very nearly the peak transfer rates of 125MB/s and 105 MB/s, respectively. Table 36.2 summarizes these numbers. The table shows us a number of important things. First, and most importantly, there is a huge gap in drive performance betweenran­dom and sequential workloads, almost a factor of 200 or so for the Cheetah and more than a factor 300 difference for the Barracuda. And thus we arrive at the most obvious design tip in the historyof computing. Asecond, more subtle point: there is a large difference in perfor­mance between high-end “performance” drives and low-end “capac­ity” drives. For this reason (and others), people are often willing to pay top dollar for the former while trying to get the latter as cheaply as possible. 
OPERATING SYSTEMS ARPACI-DUSSEAU DESIGN TIP:USE DISKS SEQUENTIALLY 
When at all possible, transfer data to and from disks in a sequential 
manner. If I/O is done in little random pieces, I/O performance will 
suffer dramatically. Also, users will suffer. Also, you willsuffer, 
knowing what suffering you have wrought with your random I/Os. 

36.5 Disk Scheduling 
Because of the high cost of I/O, the OS has historically playeda role in deciding the order of I/Os issued to the disk. More speci.­cally, given a set of I/O requests, the disk scheduler examines the requests and decides which one to schedule next. 
Unlike job scheduling, where the length of each job is usuallyun­known, with disk scheduling, we can make a good guess at how long a“job”(i.e., disk request) will take. By estimating the seekand pos­sible the rotational delay of a request, the disk scheduler can know how long each request will take, and thus (greedily) pick the one that will take the least time to service .rst. Thus, the disk scheduler will try to follow the principle of SJF (shortest job .rst) in its operation. For more details on disk scheduling, see either [SCO90] or [JW91], which both provide good overviews. 
SSTF: Shortest Seek Time First 
One early disk scheduling approach is known as shortest-seek-time­.rst (SSTF)(also called shortest-seek-.rst or SSF). SSTF orders the queue of I/O requests by track, and picks the request on the nearest track to complete .rst. For example, assuming the current position of the head is over the inner track, and we have requests for sectors 21 (middle track) and 2 (outer track), we would then issue the request to 21 .rst, wait for it to complete, and then issue the request to 2. 
SSTF works well in this example, seeking to the middle track .rst and then the outer track. However, there are some problems with SSTF that this example does not make clear. First, as we discussed above, the drive geometry is not available to the host OS; rather, it sees an array of blocks. Fortunately, this problem is rather easily .xed. Instead of SSTF, an OS can simply implement nearest-block-
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 

.rst (NBF), which schedules the request with the nearest block ad­dress to the last request next. 
The second problem is more fundamental: starvation.Imagine in our example above if there were a steady stream of requests to the inner track, where the head currently is positioned. Requests to any other tracks would then be ignored completely by a pure SSTF approach. And thus the crux of the problem: 

CRUX:HOW TO HANDLE DISK STARVATION How can we implement a SSTF-like scheduling algorithm but avoid starvation? 

Elevator (a.k.a. SCAN or C-SCAN) 
The answer to this query was developed some time ago (see [CKR72] for example), and is relatively straightforward. The algorithm, origi­nally called SCAN,simply movesacross the disk servicing requests in order across the tracks. Let us call a single pass across thedisk a sweep.Thus, if a request comes for a block on a track that has already 
OPERATING SYSTEMS ARPACI-DUSSEAU 
been serviced on this sweep of the disk, it is not handled immedi­ately, but rather queued until the next sweep. 
SCAN has a number of variants, all of which do about the same thing. For example, Coffman et al. introduced F-SCAN,which freezes the queue to be serviced when it is doing a sweep [CKR72]; this ac­tion places requests that come in during the sweep into a queueto be serviced later. Doing so avoids starvation of far-away requests, by delaying the servicing of late-arriving (but nearer by) requests. 
C-SCAN is another common variant, short for Circular SCAN. Instead of sweeping in one direction across the disk, the algorithm sweeps from outer-to-inner, and then inner-to-outer, etc. 
For reasons that should now be obvious, this algorithm (and its variants) is sometimes referred to as the elevator algorithm, because it behaves like an elevator which is either going up or down andnot just servicing requests to .oors based on which .oor is closer. Imag­ine how annoying it would be if you were going down from .oor 10 to 1, and somebody got on at 3 and pressed 4, and the elevator went up to 4 because it was “closer” than 1! As you can see, the elevator algorithm, when used in real life, prevents .ghts from takingplace on elevators. In disks, it just prevents starvation. 
Unfortunately, SCAN and its cousins do not represent the best scheduling technology. In particular, SCAN (or SSTF even) donot actually adhere as closely to the principle of SJF as they could. In particular, they ignore rotation. And thus, another crux: 
CRUX:HOW TO ACCOUNT FOR DISK ROTATION COSTS How can we implement an algorithm that more closely approxi­mates SJF by taking both seek and rotation into account? 

SPTF: Shortest Positioning Time First 
Before discussing shortest positioning time .rst or SPTF schedul­ing (sometimes also called shortest access time .rst or SATF), which is the solution to our problem, let us make sure we understand the problem in more detail. Figure 36.7 presents an example. 
In the example, the head is currently positioned over sector 30 on the inner track. The scheduler thus has to decide: should it schedule 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

sector 16 (on the middle track) or sector 8 (on the outer track)for its next request. So which should it service next? 
The answer, of course, is “it depends”. In engineering, it turns out this is almost always the answer, so if you don’t know an answer, you might want to go with it. However, it is almost always better toknow why it depends. 
What it depends on here is the relative time of seeking as com­pared to rotation. If, in our example, seek time is much higherthan rotational delay, then SSTF (and variants) are just .ne. However, imagine if seek is quite a bit faster than rotation. Then, in our exam­ple, it would make more sense to seek further to service request 8 on the outer track than it would to perform the shorter seek to themid­dle track to service 16, which has to rotate all the way around before passing under the disk head. 
On modern drives, as we saw above, both seek and rotation are roughly equivalent (depending, of course, on the exact requests), and thus SPTF is useful. However, it is even more dif.cult to implement in an OS, which generally does not have a good idea where track boundaries are or where the disk head currently is (in a rotational sense). 
OPERATING SYSTEMS ARPACI-DUSSEAU DESIGN TIP:IT DEPENDS (ALWAYS) 
Almost any question can be answered with “it depends”. However, 
use with caution, as if you answer too many questions this way,peo­
ple will stop asking you questions altogether. For example, some­
body asks: “want to go to lunch?” You reply: “it depends, are you 
coming along?” 

Modern Scheduling Issues 
One .nal issue we’d like to discuss before ending this note is how disk scheduling is done on modern systems. Older systems assumed the OS did all the scheduling, and the OS would only issue a single request at a time. 
In modern systems, disks can accommodate multiple outstand­ing requests, and have sophisticated internal schedulers themselves (which can implement SPTF accurately, for example, as in the disk you know all of the relevant details). Thus, the OS scheduler usu­ally picks what it thinks the best few requests are and issues them to disk; the disk then uses its internal knowledge of head position and detailed track layout to service said requests in the bestpossible (SPTF) order. 


36.6 Summary 
We have presented a summary of how disks work. The summary is actually a detailed functional model; it does not describethe amaz­ing physics, electronics, and material science that goes into actual drive design. For those interested in even more details of that na­ture, we suggest a different major (or perhaps minor); for those that are happy with this model, good! We can now proceed to using the model to build more interesting systems on top of these incredible devices. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

References 
[ADR03] “More Than an Interface: SCSI vs. ATA” 
Dave Anderson, Jim Dykes, Erik Riedel 
FAST ’03, 2003 

One of the best recent-ish references on how modern disk drives really work; a must read for anyone interested in knowing more. 
[CKR72] “Analysis of Scanning Policies for Reducing Disk Seek Times” 
E.G. Coffman, L.A. Klimko, B. Ryan 
SIAM Journal of Computing, September 1972, Vol 1. No 3. 

Some of the early work in the .eld of disk scheduling. 
[JW91] “Disk Scheduling Algorithms Based On Rotational Position” 
D. Jacobson, J. Wilkes 
Technical Report HPL-CSP-91-7rev1, Hewlett-Packard (February 1991) 

Amore modern take on disk scheduling. It remains a technical report (and not a published paper) because the authors were scooped by Seltzer et al. [SCO90]. 
[RW92] “An Introduction to Disk Drive Modeling” 
C. Ruemmler, J. Wilkes 
IEEE Computer, 27:3, pp. 17-28, March 1994 

Aterri.c introduction to the basics of disk operation. Some pieces are out of date, but most of the basics remain. 
[SCO90] “Disk Scheduling Revisited” 
Margo Seltzer, Peter Chen, John Ousterhout 
USENIX 1990 

Apaper that talks about howrotation matters too in the world of disk scheduling. 
[SG04] “MEMS-based storage devices and standard disk interfaces: A square peg in a round hole?” Steven W. Schlosser, Gregory R. Ganger FAST ’04, pp. 87-100, 2004 
While the MEMS aspect of this paper hasn’t yet made an impact, the discussion of the contract between .le systems and disks is wonderful and a lasting contribution. 
[S09a] “Barracuda ES.2 data sheet”
http://www.seagate.com/docs/pdf/datasheet/disc/ds cheetah 15k 5.pdf 
Adata sheet; read at your own risk. Risk of what? Boredom. 
[S09b] “Cheetah 15K.5” 
http://www.seagate.com/docs/pdf/datasheet/disc/ds barracuda es.pdf 
See above commentary on data sheets. 
OPERATING SYSTEMS ARPACI-DUSSEAU 





Homework  
This homework uses disk.py to familiarize you with how a mod­ern hard drive works. It has a lot of different options, and unlike most of the other simulations, has a graphical animator to show you exactly what happens when the disk is in action. Let’s do a simple example .rst. To run the simulator and compute some basic seek, rotation, and transfer times, you .rst have to give alistof requests to the simulator. This can either be done by spec­ifying the exact requests, or by having the simulator generate some randomly. We’ll start by specifying a list of requests ourselves. Let’sdo a single request .rst:  
prompt> disk.py -a 10  
At this point you’ll see:  
... REQUESTS [’10’]  
For the requests above, compute the seek, rotate, and transfer times. Use -c or the graphical mode (-G) to see the answers.  
To be able to compute the seek, rotation, and transfer times for this request, you’ll have to know a little more information about the layout of blocks, the starting position of the disk head, and so forth. To see much of this information, run the simulator in graphical mode (-G):  
prompt> disk.py -a 10 -G  
At this point, a window should appear with our simple disk on it. The disk head is positioned on the outside track, halfway through block 6. As you can see, block 10 (our example block) is on the same track, about a third of the way around. The direction of rotation is counter-clockwise. To run the simulation, press the “s” key while the simulator window is highlighted. When the simulation completes, you should be able to see that the disk spent 105 time units in rotation and 30 in transfer in order to access block 10, with no seek time. Press “q” to close the simulator window.  
ARPACI-DUSSEAU  THREE EASY PIECES (V0.5)  

To calculate this (instead of just running the simulation), you would 
need to know a few details about the disk. First, the rotational speed 
is by default set to 1 degree per time unit. Thus, to make a com­
plete revolution, it takes 360 time units. Second, transfer begins and 
ends at the halfway point between blocks. Thus, to read block 10, 
the transfer begins halfway between 9 and 10, and ends halfwaybe­
tween 10 and 11. Finally, in the default disk, there are 12 blocks per 
track, meaning that each block takes up 30 degrees of the rotational 
space. Thus, to read a block, it takes 30 time units (given our default 
speed of rotation). 
With this information in hand, you now should be able to com­pute the seek, rotation, and transfer times for accessing block 10. Be­cause the head starts on the same track as 10, there is no seek time. Because the disk rotates at 1 degree / time unit, it takes 105 time units to get to the beginning of block 10, halfway between 9 and 10 (note that it is exactly 90 degrees to the middle of block 9, and another 15 to the halfway point). Finally, to transfer the block takes 30 time units. 
Now let’s do a slightly more complex example: 
prompt> disk.py -a 10,11 -G 
In this case, we’re transferring two blocks, 10 and 11. How long will it take? Try guessing before running the simulation! 
As you probably guessed, this simulation takes just 30 time units longer, to transfer the next block 11. Thus, the seek and rotate times remain the same, but the transfer time for the requests is doubled. You can in fact see these sums across the top of the simulator win­dow; they also get printed out to the console as follows: 
... 
Block: 10 Seek: 0 Rotate:105 Transfer: 30 Total: 135 
Block: 11 Seek: 0 Rotate: 0 Transfer: 30 Total: 30 
TOTALS Seek: 0 Rotate:105 Transfer: 60 Total: 165 
Now let’s do an example with a seek. Try the following set of requests: 
prompt> disk.py -a 10,18 -G 
To compute how long this will take, you need to know how long 
aseek will take. The distance between each track is by default40 
distance units, and the default rate of seeking is 1 distance unit per 
OPERATING SYSTEMS ARPACI-DUSSEAU 
unit time. Thus, a seek from the outer track to the middle tracktakes 40 time units. 
You’d also have to know the scheduling policy. The default is FIFO, though, so for now you can just compute the request times assuming the processing order matches the list speci.ed via the -a .ag. 
To compute how long it will take the disk to service these re­quests, we .rst compute how long it takes to access block 10, which we know from above to be 135 time units (105 rotating, 30 transfer­ring). Once this request is complete, the disk begins to seek to the middle track where block 18 lies, taking 40 time units. Then the disk rotates to block 18, and transfers it for 30 time units, thus completing the simulation. But how long does this .nal rotation take? 
To compute the rotational delay for 18, .rst .gure out how long the disk would take to rotate from the end of the access to block10 to the beginning of the access to block 18, assuming a zero-cost seek. As you can see from the simulator, block 10 on the outer track is lined up with block 22 on the middle track, and there are 7 blocks separating 22 from 18 (23, 12, 13, 14, 15, 16, and 17, as the disk spins counter­clockwise). Rotating through 7 blocks takes 210 time units (30 per block). However, the .rst part of this rotation is actually spent seek­ing to the middle track, for 40 time units. Thus, the actual rotational delay for accessing block 18 is 210 minus 40, or 170 time units.Run the simulator to see this for yourself; note that you can run without graphics and with the -c .ag to just see the results without seeing the graphics. 
prompt> ./disk.py -a 10,18 -c ... Block: 10 Seek: 0 Rotate:105 Transfer: 30 Total: 135 Block: 18 Seek: 40 Rotate:170 Transfer: 30 Total: 240 TOTALS Seek: 40 Rotate:275 Transfer: 60 Total: 375 
You should now have a basic idea of how the simulator works. The questions below will explore some of the different options, to better help you build a model of how a disk really works. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
Questions 
1. 
Compute the seek, rotation, and transfer times for the follow­ing sets of requests: -a 0, -a 6, -a 30, -a 7,30,8, -a 10,11,12,13. 

2. 
Do the same requests above, but change the seek rate to differ­ent values: -S 2, -S 4, -S 8, -S 10, -S 40, -S 0.1.How do the times change? 

3. 
Do the same requests above, but change the rotation rate: 	-R 0.1, -R 0.5, -R 0.01.How do the times change? 

4. 
You might have noticed that some request streams would be better served with a policy better than FIFO. For example, with the request stream -a 7,30,8,what order should the requests be processed in? Now run the shortest seek-time .rst (SSTF) scheduler (-p SSTF)on the same workload; how long should it take (seek, rotation, transfer) for each request to be served? 

5. 
Now do the same thing, but using the shortest access-time .rst (SATF) scheduler (-p SATF). Does it make any difference for the set of requests as speci.ed by -a 7,30,8?Find a set of requests where SATF does noticeably better than SSTF; what are the conditions for a noticeable difference to arise? 

6. 
You might have noticed that the request stream -a 10,11,12,13 wasn’t particularly well handled by the disk. Why is that? Can you introduce a track skew to address this problem (-o skew, where skew is a non-negative integer)? Given the default seek rate, what should the skew be to minimize the transfer time for this set of requests? What about for different seek rates (e.g.,-S 2, -S 4)? In general, could you write a formula to .gure out the skew, given the seek rate and block layout information? 

7. 
Multi-zone disks pack more blocks into the outer tracks. 	To con.gure this disk in such a way, run with the -z .ag. Specif­ically, try running some requests against a disk run with -z 10,20,30 (the numbers specify the angular space occupied by a block, per track; in this example, the outer track will be packed with a block every 10 degrees, the middle track every 20 degrees, and the inner track with a block every 30 degrees). Run some random requests (e.g., -a -1 -A 5,-1,0,which 


OPERATING SYSTEMS ARPACI-DUSSEAU 
speci.es that random requests should be used via the -a -1 .ag and that .ve requests ranging from 0 to the max be gener­ated), and see if you can compute the seek, rotation, and trans­fer times. Use different random seeds (-S 1, -S 2,etc.). What is the bandwidth (in blocks per unit time) on the outer, middle, and inner tracks? 
8. 
Scheduling windows determine how many block requests a disk can examine at once in order to determine which block to serve next. Generate some random workloads of a lot of requests (e.g., -A 1000,-1,0,with different seeds perhaps) and see how long the SATF scheduler takes when the schedul­ing window is changed from 1 up to the number of requests (e.g., -w 1 up to -w 1000,and some values in between). How big of scheduling window is needed to approach the best pos­sible performance? Make a graph and see. Hint: use the -c .ag and don’t turn on graphics with -G to run these more quickly. When the scheduling window is set to 1, does it matter which policy you are using? 

9. 
Avoiding starvation is important in a scheduler. Can you think of a series of requests such that a particular block is delayed for a very long time given a policy such as SATF? Given that sequence, how does it perform if you use a bounded SATF or BSATF scheduling approach? In this approach, you specify the scheduling window (e.g., -w 4)as well as the BSATF policy (-p BSATF); the scheduler then will only move onto the next window of requests when all of the requests in the current win­dow have been serviced. Does this solve the starvation prob­lem? How does it perform, as compared to SATF? In general, how should a disk make this trade-off between performance and starvation avoidance? 

10. 
All the scheduling policies we have looked at thus far are greedy, in that they simply pick the next best option instead of looking for the optimal schedule over a set of requests. Can you .nd a set of requests in which this greedy approach is not optimal? 


THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
37 




Redundant Arrays of Inexpensive Disks(RAIDs) 
When we use a disk, we sometimes wish it to be faster; I/O opera­tions are slow and thus can be the bottleneck for the entire system. When we use a disk, we sometimes wish it to be larger; more and more data is being put online and thus our disks are getting fuller and fuller. When we use a disk, we sometimes wish for it to be more reliable; when a disk fails, if our data isn’t backed up, all that valu­able data is gone. 
In this note, we introduce the Redundant Array of Inexpensive Disks better known as RAID [P+88], a technique to use multiple disks in concert to build a faster, bigger, and more reliable disk sys­tem. The term was introduced in the late 1980s by a group of re­searchers at U.C. Berkeley (led by Professors David Patterson and Randy Katz and then student Garth Gibson); it was around this time that many different researchers simultaneously arrived upon the ba­sic idea of using multiple disks to build a better storage system [BG88, K86,K88,PB86,SG86]. 
From the outside, a RAID looks like a disk: a group of blocks each of which one can read or write. Internally, however, the RAID is a complex beast, consisting of multiple disks, memory (both volatile and non-volatile), and one or more processors to manage the system. Thus, a hardware RAID box is very much like a computer system, but just specialized for the task of managing a group of disks. 
RAIDs offer a number of advantages over a single disk. One ad­vantage is performance.Using multiple disks in parallel can greatly speed up I/O times. Another bene.t is capacity.Large data sets 
455 
demand large disks. Finally, RAIDs can improve reliability;spread­ing data across multiple disks (without RAID techniques) makes the data vulnerable to the loss of a single disk; with some form of redun-dancy,RAIDs can tolerate the loss of a disk and keep operating as if nothing were wrong. 
Amazingly, RAIDs provide these advantages transparently to sys­tems that use them, i.e., a RAID just looks like a big disk to thehost system. The beauty of transparency, of course, is that it enables one to simply replace a disk with a RAID and not change a single line of software; the operating system and client applications continue to operate without modi.cation. In this manner, transparency greatly improves the deployability of RAID, enabling users and administra­tors to put a RAID to use without worries of software compatibility. 
DESIGN TIP:TRANSPARENCY 
When considering how to add new functionality to a system, one should always consider whether such functionality can be added transparently,in a way that demands no changes to the rest of the system. Requiring a complete rewrite of the existing software (or radical hardware changes) lessens the chance of impact of an idea. 
We now discuss some of the important aspects of RAIDs. We be­gin with the interface, fault model, and then discuss how one can evaluate a RAID design along three important axes: capacity,relia­bility, and performance. We then discuss a number of other issues that are important to RAID design and implementation. 
37.1 Interface and RAID Internals 
To a .le system above, a RAID looks like a big, (hopefully) fast, and (hopefully) reliable disk. Just as with a single disk, it presents itself as a linear array of blocks, each of which can be read or written by the .le system (or other client). 
When a .le system issues a logical I/O request to the RAID, the RAID internally must calculate which disk (or disks) to access in or­der to complete the request, and then issue one or more physical I/Os to do so. The exact nature of these physical I/Os depends on the RAID level, as we will discuss in detail below. However, as a simple 
OPERATING SYSTEMS ARPACI-DUSSEAU 
example, consider a RAID that keeps two copies of each block (each one on a separate disk); when writing to such a mirrored RAID sys­tem, the RAID will have to perform two physical I/Os for every one logical I/O it is issued. 
ARAID system is often built as a separate hardware box, with a standard connection (e.g., SCSI, or SATA) to a host. Internally, how­ever, RAIDs are fairly complex, consisting of a microcontroller that runs .rmware to direct the operation of the RAID, volatile memory such as DRAM to buffer data blocks as they are read and written, and in some cases, non-volatile memory to buffer writes safely and perhaps even specialized logic to perform parity calculations (useful in some RAID levels, as we will also see below). At a high level,a RAID is very much a specialized computer system: it has a proces­sor, memory, and disks; however, instead of running applications, it runs specialized software designed to operate the RAID. 

37.2 Fault Model 
To understand RAID and compare different approaches, we must have a fault model in mind. RAIDs are designed to detect and re­cover from certain kinds of disk faults; thus, knowing exactly which faults to expect is critical in arriving upon a working design. 
The .rst fault model we will assume is quite simple, and has been called the fail-stop fault model [S84]. In this model, a disk can be in exactly one of two states: working or failed. With a working disk, all blocks can be read or written. In contrast, when a disk has failed, we assume it is permanently lost. 
One critical aspect of the fail-stop model is what it assumes about fault detection. Speci.cally, when a disk has failed, we assume that this is easily detected. For example, in a RAID array, we wouldas­sume that the RAID controller hardware (or software) can immedi­ately observe when a disk has failed. 
Thus, for now, we do not have to worry about more complex “silent” failures such as disk corruption. We also do not haveto worry about a single block becoming inaccessible upon an other­wise working disk (sometimes called a latent sector error). We will consider these more complex (and unfortunately, more realistic) disk faults later. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

37.3 How to Evaluate a RAID 
As we will soon see, there are a number of different approaches to building a RAID. Each of these approaches has different charac­teristics which are worth evaluating, in order to understandtheir strengths and weaknesses. 
Speci.cally, we will evaluate each RAID design along three axes. The .rst axis is capacity;given a set of N disks, how much useful capacity is available to systems that use the RAID? Without redun­dancy, the answer is obviously N; however, if we have a system that keeps a two copies of each block, we will obtain a useful capacity of N/2. 
The second axis of evaluation is reliability.How many disk faults 
can the given design tolerate? In alignment with our fault model, we 
assume only that an entire disk can fail. 
Finally, the third axis is performance.Performance is somewhat challenging to evaluate, because it depends heavily on the workload presented to the disk array. Thus, before evaluating performance, we will .rst present a set of typical workloads that one should consider. 
We now consider three important RAID designs: RAID Level 0 (striping), RAID Level 1 (mirroring), and RAID Levels 4/5 (parity­based redundancy). The naming of each of these designs as a “level” stems from the pioneering work of Patterson, Gibson, and Katzat Berkeley [P+88]. 

37.4 RAID Level 0: Striping 
The .rst RAID level is actually not a RAID level at all, in that there is no redundancy. However, RAID level 0, or striping as itis better known, serves as an excellent upper-bound on performance and capacity and thus is worth understanding. 
The simplest form of striping will stripe blocks across the disks of the system as follows (assume here a 4-disk array): 
From Table 37.1, you get the basic idea: spread the blocks of the array across the disks in a round-robin fashion. This approach is de­signed to extract the most parallelism from the array when requests are made for contiguous chunks of the array (as in a large, sequential read, for example). We call the blocks in the same row a stripe;thus, blocks 0, 1, 2, and 3 are in the same stripe above. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
Disk 0 Disk 1 Disk 2 Disk 3 
0  1  2  3  
4  5  6  7  
8  9  10  11  
12  13  14  15  

Table 37.1: RAID-0: Simple Striping 
In the example, we have made the simplifying assumption that only 1 block (each of say size 4KB) is placed on each disk beforemov­ing on to the next. However, this arrangement need not be the case. For example, we could arrange the blocks across disks as in Table 
37.2: Disk 0 Disk 1 Disk 2 Disk 3 
024 
135 
8 10 12 14 
9 11 13 15 


Table 37.2: Striping with a Bigger Chunk Size 
In this example, we place two 4KB blocks on each disk before moving on to the next disk. Thus, the chunk size of this RAID ar­ray is 8KB, and a stripe thus consists of 4 chunks or 32KB of data. 

Chunk Sizes 
Chunk size mostly affects performance of the array. For example, a small chunk size implies that many .les will get striped across many disks, thus increasing the parallelism of reads and writes toa single .le; however, the positioning time to access blocks across multiple disks increases, because the positioning time for the entirerequest is determined by the maximum of the positioning times of the requests across all drives. 
Abig chunk size, on the other hand, reduces such intra-.le par­allelism, and thus relies on multiple concurrent requests toachieve high throughput. However, large chunk sizes reduce positioning time; if, for example, a single .le .ts within a chunk and thus is 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
placed on a single disk, the positioning time incurred while accessing it will just be the positioning time of a single disk. 
Thus, determining the “best” chunk size is hard to do, as it re­quires a great deal of knowledge about the workload presentedto the disk system [CL95]. For the rest of this discussion, we will as­sume that the array uses a chunk size of a single block (4KB); most arrays use larger chunk sizes (say around 64 KB), but for the issues we discuss below, the exact chunk size does not matter and thuswe use a single block for the sake of simplicity. 
ASIDE:THE RAID MAPPING PROBLEM 
Before studying the capacity, reliability, and performancecharac­teristics of the RAID, we .rst present an aside on what we call the mapping problem.This problem arises in all RAID arrays; simply put, given a logical block to read or write, how does the RAID know exactly which physical disk and offset to access? 
For these simple RAID levels, we do not need much sophistication in order to correctly map logical blocks onto their physical locations. Take the .rst striping example above (chunk size = 1 block = 4KB). In this case, given a logical block address A, the RAID can easily compute the desired disk and offset with two simple equations: 
Disk = A % number_of_disks 
Offset = A / number_of_disks 
Note that these are all integer operations (e.g., 4 / 3 = 1 not 1.33333...). 
Let’s see how these equations work for a simple example. Imagine in the .rst RAID above that a request arrives for block 14. Given that there are 4 disks, this would mean that the disk we are interested in is (14 The exact block is calculated as (14 / 4 = 3): block 3. Thus, block 14 should be found on the fourth block (block 3, starting at 0) of the third disk (disk 2, starting at 0), which is exactly where it is. 
You can think about how these equations would be modi.ed to support different chunk sizes. Try it! It’s not too hard. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

Back to RAID-0 Analysis 
Let us now evaluate striping. From the perspective of capacity, it is perfect: given N disks, striping delivers N disks worth of useful capacity. From the standpoint of reliability, striping is also perfect, but in the bad way: any disk failure will lead to data loss. Finally, performance is excellent. 

Evaluating RAID Performance 
In analyzing RAID performance, one can consider two different per­formance metrics. The .rst is single-request latency.Understanding the latency of a single I/O request to a RAID is useful as it reveals how much parallelism can exist during a single logical I/O opera­tion. The second is steady-state throughput of the RAID, i.e., the total bandwidth of many concurrent requests. Because RAIDs are often used in high-performance environments, the steady-state bandwidth is critical, and thus will be the main focus of our analyses. 
To understand throughput in more detail, we need to put forth some workloads of interest. We will assume, for this discussion, that there are two types of workloads: sequential and random.With asequential workload, we assume that requests to the array come in large contiguous chunks; for example, a request (or seriesof re­quests) that accesses 1 MB of data, starting at block (B) and end­ing at block (B + 1 MB), would be deemed sequential. Sequential workloads are common in many environments (think of searching through a large .le for a keyword), and thus are considered impor­tant. 
For random workloads, we assume that each request is rather small, and that each request is to a different random locationon disk. For example, a random stream of requests may .rst access 4KB at logical address 10, then at logical address 55000, then at 20100, and so forth. Some important workloads, like transactional workloads on a database, exhibit this type of access pattern, and thus itis con­sidered an important workload as well. 
Of course, real workloads are not so simple, and often have a mix of sequential and random-seeming components as well as behaviors in-between the two. However, for now, we will just consider these two possibilities. 
As you can tell, sequential and random workloads will result in 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
OPERATING SYSTEMS 
widely different performance characteristics from a disk. With se­quential access, a disk operates in its most ef.cient mode, spending little time seeking and waiting for rotation and most of its time trans­ferring data. With random access, just the opposite is true: most time is spent seeking and waiting for rotation and relatively little time is spent transferring data. To capture this difference in our analysis, we will assume that a disk can transfer data at S MB/s under a se­quential workload, and R MB/s when under a random workload. In general, S is much greater than R. 
To make sure we understand this difference, let’s do a simple ex­ercise. Speci.cally, lets calculate S and R given the following disk characteristics. Assume a sequential transfer of size 10 MB on aver­age, and a random transfer of 10 KB on average. Also, assume the following disk characteristics: 
Average seek time  7 ms  
Average rotational delay  3 ms  
Transfer rate of disk  50 MB/s  

To compute S, we need to .rst .gure out how time is spent in a typical 10 MB transfer. First, we spend 7 ms seeking, and then 3ms rotating. Finally, transfer begins; 10 MB @ 50 MB/s leads to 1/5th of asecond, or 200ms, spent in transfer. Thus, for each 10 MB request, we spend 210 ms completing the request. To compute S, we just need to divide: 
Amount of Data 10 MB
S = = =47.62 MB/s 
Time to access 210 ms 
As we can see, because of the large time spent transferirng data, S is very near the peak bandwidth of the disk (the seek and rotational costs have been amortized). 
We can compute R similarly. Seek and rotation are the same; we then compute the time spent in transfer, which is 10 KB @ 50 MB/s, or 0.195 ms. 
Amount of Data 10 KB
R = ==0.981 MB/s 
Time to access 10.195 ms 
As we can see, R is less than 1 MB/s, and S/R is almost 50. 

Back to RAID-0 Analysis, Again 
Let’s now evaluate the performance of striping. As we said above, it is generally good. From a latency perspective, for example, the 
ARPACI-DUSSEAU 
latency of a single-block request should be just about identical to that of a single disk; after all, RAID-0 will simply redirect that request to one of its disks. 
From the perspective of steady-state throughput, we’d expect to get the full bandwidth of the system. Thus, throughput equalsN (the number of disks) multiplied by S (the sequential bandwidth of asingle disk). For a large number of random I/Os, we can again use all of the disks, and thus obtain N · R MB/s. As we will see below, these values are both the simplest to calculate and will serveas an upper bound in comparison with other RAID levels. 


37.5 RAID Level 1: Mirroring 
Our .rst RAID level beyond striping is known as RAID level 1, or mirroring. With a mirrored system, we simply make more than one copy of each block in the system; each copy should be placed on a separate disk, of course. By doing so, we can tolerate disk failures. 
In a typical mirrored system, we will assume that for each logical 
block, the RAID makes two physical copies of the block. Here isa 
simple example: 
Disk 0 Disk 1 Disk 2 Disk 3 
0  0  1  1  
2  2  3  3  
4  4  5  5  
6  6  7  7  

Table 37.3: Simple RAID-1: Mirroring 
In the example, disk 0 and disk 1 have identical contents, and disk 2and disk 3do as well; the data is striped across these mirror pairs. In fact, you may have noticed that there are a number of different ways to place block copies across the disks. The arrangement above is a common one and is sometimes called RAID-10 or (RAID 1+0) because it uses mirrored pairs (RAID-1) and then stripes (RAID-0) on top of them; another common arrangement is RAID-01 (or RAID 0+1), which contains two large striping (RAID-0) arrays, and then mirrors (RAID-1) on top of them. For now, we will just talk about mirroring assuming the above layout. 
When reading a block from a mirrored array, the RAID has a choice: it can read either copy. For example, if a read to logical block 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
5is issued to the RAID, it is free to read itfrom either disk 2ordisk 
3. When writing a block, though, no such choice exists: the RAID must update both copies of the data, in order to preserve reliability. Do note, though, that these writes can take place in parallel;for ex­ample, a write to logical block 5 could proceed to disks 2 and 3 at the same time. 
ASIDE:THE RAID CONSISTENT UPDATE PROBLEM 
Before analyzing RAID-1, let us .rst discuss a problem that arises in any multi-disk RAID system, known as the consistent update problem [DAA05]. The problem occurs on a write to any RAID that has to update multiple disks during a single logical operation. In this case, let us assume we are considering a mirrored disk array. 
Imagine the write is issued to the RAID, and then the RAID de­cides that it must be written to two disks, disk 0 and disk 1. The RAID then issues the write to disk 0, but just before the RAID can issue the request to disk 1, a power loss (or system crash) occurs. In this unfortunate case, let us assume that the request to disk 0com­pleted (but clearly the request to disk 1 did not, as it was never is­sued). 
The result of this untimely power loss is that the two copies ofthe block are now inconsistent;the copy on disk 0 is the new version, and the copy on disk 1 is the old. What we would like to happen is for the state of both disks to change atomically,i.e.,either both should end up as the new version or neither. 
The general way to solve this problem is to use a write-ahead log of some kind to .rst record what the RAID is about to do (i.e., update two disks with a certain piece of data) before doing it.By taking this approach, we can ensure that in the presence of a crash, the right thing will happen; by running a recovery procedure that replays all pending transactions to the RAID, we can ensure that no two mirrored copies (in the RAID-1 case) are out of sync. 
One last note: because logging to disk on every write is pro­hibitively expensive, most RAID hardware includes a small amount of non-volatile RAM (e.g., battery-backed) where it performs this type of logging. Thus, consistent update is provided withoutthe high cost of logging to disk. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
RAID-1 Analysis 
Let us now assess RAID-1. From a capacity standpoint, RAID-1 is pretty expensive; with the mirroring level = 2, we only obtainhalf of our peak useful capacity. Thus, with N disks, the useful capacity of mirroring is N/2. 
From a reliability standpoint, RAID-1 does well. It can tolerate the failure of any one disk. However, you may notice RAID-1 can actually do better than this, with a little luck. Imagine, in the .gure above, that disk 0 and disk 2 both failed. In such a situation, there is still no data loss! More generally, a mirrored system (with mirroring level = 2) can tolerate 1 disk failure for certain, and up to N/2failures depending on which disks fail. In real life, however, we generally don’t like to leave things like this to chance, and thus most people consider mirroring to be good for handling a single failure. 
Finally, we analyze performance. From the perspective of thela­tency of a single read request, we can see it is the same as the latency on a single disk; all the RAID-1 does is direct the read to one ofits copies. A write is a little different: it requires two physical writes to complete before it is done. These two writes happen in parallel, and thus the time will be roughly equivalent to the time of a single write; however, because the logical write must wait for both physical writes to complete, it suffers from the worst-case seek and rotational delay of the two requests, and thus (on average) will be just a little bit higher than a single write to a single disk. 
To analyze steady-state throughput, let us start with the sequen­tial workload. When writing out to disk sequentially, each logical write must result in two physical writes; for example, when wewrite logical block 0 (in the .gure above), the RAID internally would write it to both disk 0 and disk 1. Thus, we can conclude that the maximum bandwidth obtained during sequential writing to a mirrored array is ( N · S), or half the peak bandwidth. 
2 
Unfortunately, we obtain the exact same performance during ase­quential read. One might think that a sequential read could dobetter, because it only needs to read one copy of the data, not both. How­ever, let’s use an example to illustrate why this doesn’t helpmuch. Imagine we need to read blocks 0, 1, 2, 3, 4, 5, 6, and 7. Let’s saywe issue the read of 0 to disk 0, the read of 1 to disk 2, the read of 2 to disk 1, and the read of 3 to disk 3. We continue by issuing reads to 4, 5, 6, and 7 to disks 0, 2, 1, and 3, respectively. One might naively 

THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
think that because we are utilizing all the disks in this example, we are achieving the full bandwidth of the array. 
To see that this is not the case, however, consider the requests a single disk receives (say disk 0). First, it gets a request forblock0; then, it gets a request for block 4 (skipping block 2). In fact,each disk receives a request for every other block. While it is rotating over the skipped block, it is not delivering useful bandwidth to the client. Thus, each disk will only deliver half its peak bandwidth. Andthus, the sequential read will only obtain a bandwidth of ( N 2 · S)MB/s. 
Random reads are the best case for a mirrored RAID. In this case, we can distribute the reads across all the disks, and thus obtain the full possible bandwidth. Thus, for random reads, RAID-1 delivers N · R MB/s. 
Finally, random writes perform as you might expect: N 2 · R MB/s. Each logical write must turn into two physical writes, and thus while all the disks will be in use, the client will only perceive thisas half the available bandwidth. Even though a write to logical block X turns into two parallel writes to two different physical disks, theband­width of many small requests only achieves half of what we saw with striping. As we will soon see, getting half the available bandwidth is actually pretty good! 

37.6 RAID Level 4: Saving Space with Parity 
We now present a different method of adding redundancy to a disk array known as parity.Parity-based approaches attempt to use less capacity and thus overcome the huge space penalty paid bymir­rored systems. They do so at a cost, however: performance. 
In a .ve-disk RAID-4 system, we might observe the following data layout: 
Disk 0 Disk 1 Disk 2 Disk 3 Disk 4 
0123 4567 8 9 10 11 12 13 14 15 

As you can see, for each stripe of data, we have added a single parity block that stores the redundant information for that stripe of 
OPERATING SYSTEMS ARPACI-DUSSEAU 
blocks. For example, parity block P1 has redundant information that it calculated from blocks 4, 5, 6, and 7. 
To compute parity, we need to use some kind of mathematical function that enables us to withstand the loss of any one blockfrom our stripe. It turns out the simple function XOR does the trick quite nicely. For a given set of bits, the XOR of all of those bits returns a 0 if there are an even number of 1’s in the bits, and a 1 if there arean odd number of 1’s. For example: 
C0C1C2C3 P 
0 0 1 1 XOR(0,0,1,1) = 0 0 1 0 0 XOR(0,1,0,0) = 1 
In the .rst row (0,0,1,1), there are two 1’s (C2, C3), and thus XOR of all of those values will be 0 (P); similarly, in the second row there is only one 1 (C1), and thus the XOR must be 1 (P). You can remember this in a very simple way: that the number of 1’s in any row must be an even (not odd) number; that is the invariant that the RAID must maintain in order for parity to be correct. 
From the example above, you might also be able to guess how parity information can be used to recover from a failure. Imagine the column labeled C2 is lost. To .gure out what values must have been in the column, we simply have to read in all the other values in that row (including the XOR’d parity bit) and reconstruct the right answer. Speci.cally, assume the .rst row’s value in column C2is lost (it is a 1); by reading the other values in that row (0 from C0, 0 from C1, 1 from C3, and 0 from the parity column P), we get the values 0, 0, 1, and 0. Because we know that XOR keeps an even number of 1’s in each row, we know what the missing data must be: a 1. And that is how reconstruction works in a XOR-based parity scheme! Note also how we compute the reconstructed value: we just XOR the data bits and the parity bits together, in the same way that we calculated the parity in the .rst place. 
Now you might be wondering: we are talking about XORing all of these bits, and yet above we know that the RAID places 4KB (or larger) blocks on each disk; how do we apply XOR to a bunch of blocks to compute the parity? It turns out this is easy as well.Simply perform a bitwise XOR across each bit of the data blocks; put the re­sult of each bitwise XOR into the corresponding bit slot in theparity block. For example, if we had blocks of size 4 bits (yes, this isstill quite a bit smaller than a 4KB block, but you get the picture), they 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
might look something like this: 
Block0 Block1 Block2 Block3 Parity 

00 10 11 1011 
10 01 00 0110 
As you can see from the .gure, the parity is computed for each bit of each block and the result placed in the parity block. 
RAID-4 Analysis 
Let us now analyze RAID-4. From a capacity standpoint, RAID-4 uses 1 disk for parity information for every group of disks it is pro­tecting. Thus, our useful capacity for a RAID group is (N-1). 
Reliability is also quite easy to understand: RAID-4 tolerates 1 disk failure and no more. If more than one disk is lost, there issimply no way to reconstruct the lost data. 
Finally, there is performance. This time, let us start by analyzing steady-state throughput. Sequential read performance can utilize all of the disks except for the parity disk, and thus deliver a peakeffec­tive bandwidth of (N - 1) · S MB/s (an easy case). 
To understand the performance of sequential writes, we must .rst understand how they are done. When writing a big chunk of data to disk, RAID-4 can perform a simple optimization known as a full-stripe write.For example, imagine the case where the blocks 0, 1, 2, and 3 have been sent to the RAID as part of a write request (Table 37.4). 
Disk 0 Disk 1 Disk 2 Disk 3 Disk 4 


4 8 12  5 9 13  6 10 14  7 11 15  P1 P2 P3  
Table 37.4: Full-stripe Writes In RAID-4  
In this case, the RAID can simply calculate the new value of P0 (by performing an XOR across the blocks 0, 1, 2, and 3) and then write all of the blocks (including the parity block) to the .ve disksabove in parallel (highlighted in gray in the .gure). Thus, full-stripe writes are the most ef.cient way for RAID-4 to write to disk.  
OPERATING SYSTEMS  ARPACI-DUSSEAU  

Once we understand the full-stripe write, calculating the perfor­mance of sequential writes on RAID-4 is easy; the effective band­width is also (N - 1) · S MB/s. Even though the parity disk is con­stantly in use during the operation, the client does not gain any per­formance advantage from it. 
Now let us analyze the performance of random reads. As you can also see from the .gure above, a set of 1-block random reads will be spread across the data disks of the system but not the parity disk. Thus, the effective performance is: (N - 1) · R MB/s. 
Random writes, which we have saved for last, present the most interesting case for RAID-4. Imagine we wish to overwrite block 1 in the example above. We could just go ahead and overwrite it, butthat would leave us with a problem: the parity block P0 would no longer accurately re.ect the correct parity value for the stripe. Thus, in this example, P0 must also be updated. But how can we update it both correctly and ef.ciently? 
It turns out there are two methods. The .rst, known as additive parity,requires usto do the following. To compute the value of the new parity block, read in all of the other data blocks in the stripe in parallel (in the example, blocks 0, 2, and 3) and XOR those with the new block (1). The result is your new parity block. To complete the write, you can then write the new data and new parity to their respective disks, also in parallel. 
The problem with this technique is that it scales with the number of disks, and thus in larger RAIDs requires a high number of reads to compute parity. Thus, the subtractive parity method. 
For example, imagine this string of bits (4 data bits, and one parity bit): C0C1C2C3 P 
0 0 1 1 XOR(0,0,1,1) = 0 
Let’s imagine that we wish to overwrite bit C2 with a new value which we will call C2(new). The subtractive method works in three steps. First, we read in the old data at C2 (C2(old) = 1) and the old parity (P(old) = 0). Then, we compare the old data and the new data; if they are the same (e.g., C2(new) = C2(old)), then we know thepar­ity bit will also remain the same (i.e., P(new) = P(old)). If, however, they are different, then we must .ip the old parity bit to the opposite of its current state, that is, if (P(old) == 1), P(new) will be set to 0; if (P(old) == 0), P(new) will be set to 1. We can express this whole 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
mess neatly with XOR as it turns out (if you understand XOR, this will now make sense to you): 
P(new) = (C(old) XOR C(new)) XOR P(old) 
Because we are dealing with blocks, not bits, we perform this cal­culation over all the bits in the block (e.g., 4096 bytes in each block multipled by 8 bits per byte). Thus, in most cases, the new block will be different than the old block and thus the new parity block will too. 
You should now be able to .gure out when we would use the additive parity calculation and when we would use the subtractive method. Think about how many disks would need to be in the sys­tem so that the additive method performs fewer I/Os than the sub­tractive method, and vice-versa. 
For this performance analysis, let us assume we are using the sub­tractive method. Thus, for each write, the RAID has to perform4 physical I/Os (two reads and two writes). Now imagine there are lots of writes submitted to the RAID; how many can RAID-4 per­form in parallel? To understand, let us again look at the RAID-4 layout (Figure 37.5). 
Disk 0 Disk 1 Disk 2 Disk 3 Disk 4 
01 23P0 
*4 
567 
+P1 8 9 10 11 P2 
12 
*1314 15 
+P3 
Table 37.5: Example: Writes To 4, 13, And Respective Parity Blocks 
Now imagine there were 2 small writes submitted to the RAID-4 at about the same time, to blocks 4 and 13 (marked with * in the di­agram). The data for those disks is on disks 0 and 1, and thus the read and write to data could happen in parallel, which is good.The problem that arises is with the parity disk; both the requestshave to read the related parity blocks for 4 and 13, parity blocks 1 and 3 (marked with +). Hopefully, the issue is now clear: the parity disk is a bottleneck under this type of workload; we sometimes thuscall this the small-write problem for parity-based RAIDs. Thus, even though the data disks could be access in parallel, the parity disk pre­vents any parallelism from materializing; all writes to the system will be serialized because of the parity disk. Because the parity disk has 
OPERATING SYSTEMS ARPACI-DUSSEAU 
to perform two I/Os (one read, one write) per logical I/O, we can compute the performance of small random writes in RAID-4 by com­puting the parity disk’s performance on those two I/Os, and thus we achieve (R/2) MB/s. RAID-4 throughput under random small writes is terrible; it does not improve as you add disks to the system. 
We conclude by analyzing I/O latency in RAID-4. As you now know, a single read (assuming no failure) is just mapped to a single disk, and thus its latency is equivalent to the latency of a single disk request. The latency of a single write requires two reads and then two writes; the reads can happen in parallel, as can the writes, and thus total latency is about twice that of a single disk (with some dif­ferences because we have to wait for both reads to complete andthus get the worst-case positioning time, but then the updates don’t incur seek cost and thus may be a better-than-average positioning cost). 

37.7 RAID Level 5: Rotating Parity 
To address the small-write problem (at least, partially), Patterson, Gibson, and Katz introduced RAID-5. RAID-5 works almost iden­tically to RAID-4, except that it rotates the parity block across the drives (Figure 37.6). 
Disk 0 Disk 1 Disk 2 Disk 3 Disk 4 
0123 5 6 7
P14 10
11
P2 8 9 15P312 13 14 
P416 17 18 19 

Table 37.6: RAID-5 With Rotated Parity 
As you can see in the .gure, the parity block for each stripe is now rotated across the disks, in order to remove the parity-disk bottleneck for RAID-4. 
RAID-5 Analysis 
Much of the analysis for RAID-5 is identical to RAID-4. For example, the effective capacity and failure tolerance of the two levels are iden-
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

tical. So are sequential read and write performance. The latency of a single request (whether a read or a write) is also the same as RAID-4. 
Random read performance is a little better, because we can uti­lize all of the disks. Finally, random write performance improves noticeably over RAID-4, as it allows for parallelism across requests. Imagine a write to block 1 and a write to block 10; this will turninto requests to disk 1 and disk 4 (for block 1 and its parity) and requests to disk 0 and disk 2 (for block 10 and its parity). Thus, they can proceed in parallel. In fact, we can generally assume that that given alarge number of random requests, we will be able to keep all the disks about evenly busy. If that is the case, then our total bandwidth 
for small writes will be N 4 · R MB/s; the factor of four loss is due to the fact that each RAID-5 write still generates 4 total I/O operations. 
Because RAID-5 is basically identical to RAID-4 except in thefew cases where it is better, it has almost completely replaced RAID-4 in the marketplace. The only place where it has not is in systems that know they will never perform anything other than a large write, thus avoiding the small-write problem altogether [HLM94]; in those cases, RAID-4 is sometimes used as it is slightly simpler to build. 

37.8 RAID Comparison: A Summary 
We now summarize our simpli.ed comparison of RAID levels in Table 37.7. Note that we have omitted a number of details to simplify our analysis. For example, when writing in a mirrored system,the average seek time is a little higher than when writing to just asingle disk, because the seek time is the max of two seeks (one on each disk). Thus, random write performance to two disks will generally be a little less than random write performance of a single disk. Also, when updating the parity disk in RAID-4/5, the .rst read of theold parity will likely cause a full seek and rotation, but the second write of the parity will only result in rotation. 
However, our comparison does capture the essential differences, and thus is useful for understanding tradeoffs across RAID levels. We present a summary in the table below; for the latency analysis, we simply use D to represent the time that a request to a single disk would take. 
Thus, if you strictly want performance and do not care about re­liability, striping is obviously best. If, however, you wantrandom 
OPERATING SYSTEMS ARPACI-DUSSEAU 
RAID-0 RAID-1 RAID-4 RAID-5 
Capacity N N/2 N - 1 N - 1 
Reliability 01 (for sure) 11 
N 
(if lucky) 

212 
Throughput  
Sequential Read  N · S  (N/2) · S  (N - 1) · S  (N - 1) · S  
Sequential Write  N · S  (N/2) · S  (N - 1) · S  (N - 1) · S  
Random Read  N · R  N · R  (N - 1) · R  N · R  

N
Random Write Latency 
N · R (N/2) · R
· R 
R


4
Read DDD D Write DD 2D 2D 
Table 37.7: RAID Capacity, Reliability, and Performance 
I/O performance and reliability, mirroring is the best; the cost you pay is in lost capacity. If capacity and reliability are your main goals, then RAID-5 is the winner; the cost you pay is in small-write perfor­mance. Finally, if you are always doing sequential I/O and want to maximize capacity, RAID-5 also makes the most sense. 

37.9 Other Interesting RAID Issues 
There are a number of other interesting ideas that one could (and 
perhaps should) discuss when thinking about RAID. Here are some 
things we might eventually write about: 
• 	
Other RAID levels: Levels 2 and 3 from the original taxonomy, Level 6 to tolerate multiple disk faults. 

• 	
Reconstruction: What the RAID does when a disk fails and it has a hot spare sitting around to .ll in for the failed disk. What happens to performance under failure, and performance dur­ing reconstruction? 

• 	
More realistic fault models: Our own work on partial failures, including latent sector errors and block corruption. 

• 	
Ways of tolerating more realistic faults: Checksums and the many different approaches there. Again some of our own work. 

• 	
Software RAID: How to build the RAID as a software layer underneath the .le system. Cheaper, but less reliable? 


ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 

37.10 Summary 
We have discussed RAID. RAID transforms a number of indepen­dent disks into a large, more capacious, and more reliable single en­tity; importantly, it does so transparently, and thus hardware and software above is relatively oblivious to the change. 
There are many possible RAID levels to choose from, and the ex­act RAID level to use depends heavily on what is important to the end-user. For example, mirrored RAID is simple, reliable, and gen­erally provides good performance but at a high capacity cost.RAID­5, in contrast, is reliable and better from a capacity standpoint, but performs quite poorly when there are small writes in the workload. Picking a RAID and setting its parameters (chunk size, numberof disks, etc.) properly for a particular workload is challenging, and thus still remains more of an art than a science. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

References 
[BJ88] “Disk Shadowing” 
D. Bitton and J. Gray VLDB 1988 
One of the .rst papers to discuss mirroring, herein called “shadowing”. 
[CL95] “Striping in a RAID level 5 disk array” Peter M. Chen, Edward K. Lee SIGMETRICS 1995 
Anice analysis of some of the important parameters in a RAID-5disk array. 
[DAA05] “Journal-guided Resynchronization for Software RAID” Timothy E. Denehy, A. Arpaci-Dusseau, R. Arpaci-Dusseau FAST 2005 
Our own work on the consistent-update problem. Here we solve it for Software RAID by inte­grating the journaling machinery of the .le system above withthe software RAID beneath it. 
[HLM94] “File System Design for an NFS File Server Appliance” Dave Hitz, James Lau, Michael Malcolm USENIX Winter 1994, San Francisco, California, 1994 
The sparse paper introducing a landmark product in storage, the write-anywhere .le layout or WAFL .le system that underlies the NetApp .le server. 
[K86] “Synchronized Disk Interleaving” 
M.Y. Kim. 
IEEE Transactions on Computers, Volume C-35: 11, November 1986 

Some of the earliest work on RAID is found here. 
[K88] “Small Disk Arrays -The Emerging Approach to High Performance” 
F. Kurzweil. 
Presentation at Sping COMPCON ’88, March 1, 1988, San Francisco, California 

Another early RAID reference. 
[P+88] “Redundant Arrays of Inexpensive Disks” 
D. Patterson, G. Gibson, R. Katz. SIGMOD 1988 
This is considered the RAID paper, written by famous authors Patterson, Gibson, and Katz. The paper has since won many test-of-time awards and ushered in the RAID era, including the name RAID itself! 
[PB86] “Providing Fault Tolerance in Parallel Secondary Storage Systems” 
A. Park and K. Balasubramaniam Department of Computer Science, Princeton, CS-TR-O57-86, November 1986 
Another early work on RAID. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
[SG86] “Disk Striping”  
K. Salem and H. Garcia-Molina.  
IEEE International Conference on Data Engineering, 1986  
And yes, another early RAID work. There are a lot of these, which kind of came out of the  
woodwork when the RAID paper was published in SIGMOD.  
[S84] “Byzantine Generals in Action: Implementing Fail-Stop Processors”  
F.B. Schneider.  
ACM Transactions on Computer Systems, 2(2):145154, May 1984  
Finally, a paper that is not about RAID! This paper is actuallyabout how systems fail, andhow  
to make something behave in a fail-stop manner.  
OPERATING  
SYSTEMS  ARPACI-DUSSEAU  


Homework 
This section introduces raid.py,a simple RAID simulator you can use to shore up your knowledge of how RAID systems work. It has a number of options, as we see below: 
Usage: raid2.py [options] 
Options:-h, --help show this help message and exit -s SEED, --seed=SEED the random seed -D NUMDISKS, --numDisks=NUMDISKS 
number of disks in RAID 
-C CHUNKSIZE, --chunkSize=CHUNKSIZE 
chunk size of the RAID 
-n NUMREQUESTS, --numRequests=NUMREQUESTS
number of requests to simulate 
-S SIZE, --reqSize=SIZE
size of requests
-W WORKLOAD, --workload=WORKLOAD 
either "rand" or "seq" workloads 
-w WRITEFRAC, --writeFrac=WRITEFRAC 
write fraction (100->all writes, 0->all reads)
-R RANGE, --randRange=RANGE
range of requests (when using "rand" workload)
-L LEVEL, --level=LEVEL 
RAID level (0, 1, 4, 5)
-5 RAID5TYPE, --raid5=RAID5TYPE 

RAID-5 left-symmetric "LS" or left-asym "LA" 
-r, --reverse instead of showing logical ops, show physical 
-t, --timing use timing mode, instead of mapping mode 
-c, --compute compute answers for me 

In its basic mode, you can use it to understand how the different RAID levels map logical blocks to underlying disks and offsets. For example, let’s say we wish to see how a simple striping RAID (RAID­0) with four disks does this mapping. 
prompt> ./raid2.py -n 5 -L 0 -R 20 ... LOGICAL READ from addr:16 size:4096 
Physical reads/writes? 
LOGICAL READ from addr:8 size:4096 Physical reads/writes? 
LOGICAL READ from addr:10 size:4096 Physical reads/writes? 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
LOGICAL READ from addr:15 size:4096 
Physical reads/writes? 

LOGICAL READ from addr:9 size:4096 
Physical reads/writes? 

In this example, we simulate .ve requests (-n 5), specifying RAID 
level zero (-L 0), and restrict the range of random requests to just 
the .rst twenty blocks of the RAID (-R 20). The result is a series of 
random reads to the .rst twenty blocks of the RAID; the simulator 
then asks you to guess which underlying disks/offsets were accessed 
to service the request, for each logical read. 
In this case, calculating the answers is easy: in RAID-0, recall that 
the underlying disk and offset that services a request is calculated 
via modulo arithmetic: 
disk = address % number_of_disks 
offset = address / number_of_disks 
Thus, the .rst request to 16 should be serviced by disk 0, at offset 
4. And so forth. You can, as usual see the answers (once you’ve computed them!), by using the handy -c .ag to compute the results. 
prompt> ./raid2.py -R 20 -n 5 -L 0 -c 
... 
LOGICAL READ from addr:16 size:4096 
read [disk 0, offset 4] 
LOGICAL READ from addr:8 size:4096 
read [disk 0, offset 2] 

LOGICAL READ from addr:10 size:4096 
read [disk 2, offset 2] 

LOGICAL READ from addr:15 size:4096 
read [disk 3, offset 3] 

LOGICAL READ from addr:9 size:4096 
read [disk 1, offset 2] 

Because we like to have fun, you can also do this problem in re­verse, with the -r .ag. Running the simulator this way shows you the low-level disk reads and writes, and asks you to reverse engineer which logical request must have been given to the RAID: 
OPERATING SYSTEMS ARPACI-DUSSEAU 
prompt> ./raid2.py -R 20 -n 
... 
LOGICAL OPERATION is ? 

read [disk 0, offset 4] 
LOGICAL OPERATION is ? read [disk 0, offset 2] 
LOGICAL OPERATION is ? read [disk 2, offset 2] 
LOGICAL OPERATION is ? read [disk 3, offset 3] 
LOGICAL OPERATION is ? read [disk 1, offset 2] 
5 -L 0 -r 
You can again use -c to show the answers. To get more variety, a  
different random seed (-s)can be given.  
Even further variety is available by examing different RAID lev­ 
els. In the simulator, RAID-0 (block striping), RAID-1 (mirroring),  
RAID-4 (block-striping plus a single parity disk), and RAID-5 (block- 
striping with rotating parity) are supported.  
In this next example, we show how to run the simulator in mir­ 
rored mode. We show the answers to save space:  
prompt> ./raid2.py -R 20 -n 5 -L 1 -c  
...  
LOGICAL READ from addr:16 size:4096  
read [disk 0, offset 8]  
LOGICAL READ from addr:8 size:4096  
read [disk 0, offset 4]  
LOGICAL READ from addr:10 size:4096  
read [disk 1, offset 5]  
LOGICAL READ from addr:15 size:4096  
read [disk 3, offset 7]  
LOGICAL READ from addr:9 size:4096  
read [disk 2, offset 4]  
You might notice a few things about this example. First, the mir­ 
rored RAID-1 assumes a striped layout (which some might call RAID­ 
01), where logical block 0 is mapped to the 0th block of disks 0 and  
1, logical block 1 is mapped to the 0th blocks of disks 2 and 3, and  
THREE  
ARPACI-DUSSEAU  EASY PIECES  
(V0.5)  

so forth (in this four-disk example). Second, when reading a sin­gle block from a mirrored RAID system, the RAID has a choice of which of two blocks to read. In this simulator, we use a relatively silly way: for even-numbered logical blocks, the RAID chooses the even-numbered disk in the pair; the odd disk is used for odd-numbered logical blocks. This is done to make the results of each run easy to guess for you (instead of, for example, a random choice). 
We can also explore how writes behave (instead of just reads) with the -w .ag, which speci.es the “write fraction” of a workload, i.e., the fraction of requests that are writes. By default, it is setto zero, and thus the examples so far were 100% reads. Let’s see what happens to our mirrored RAID when some writes are introduced: 
prompt> ./raid2.py -R 20 -n 5 -L 1 -w 100 -c ... LOGICAL WRITE to addr:16 size:4096 
write [disk 0, offset 8] write [disk 1, offset 8] 
LOGICAL WRITE to addr:8 size:4096 write [disk 0, offset 4] write [disk 1, offset 4] 
LOGICAL WRITE to addr:10 size:4096 write [disk 0, offset 5] write [disk 1, offset 5] 
LOGICAL WRITE to addr:15 size:4096 write [disk 2, offset 7] write [disk 3, offset 7] 
LOGICAL WRITE to addr:9 size:4096 write [disk 2, offset 4] write [disk 3, offset 4] 
With writes, instead of generating just a single low-level disk op­eration, the RAID must of course update both disks, and hence two writes are issued. Even more interesting things happen with RAID-4 and RAID-5, as you might guess; we’ll leave the exploration ofsuch things to you in the questions below. 
The remaining options are discovered via the help .ag. They are: 
Options:-h, --help show this help message and exit -s SEED, --seed=SEED the random seed -D NUMDISKS, --numDisks=NUMDISKS 
number of disks in RAID -C CHUNKSIZE, --chunkSize=CHUNKSIZE chunk size of the RAID -n NUMREQUESTS, --numRequests=NUMREQUESTS 
OPERATING SYSTEMS ARPACI-DUSSEAU 
number of requests to simulate -S SIZE, --reqSize=SIZEsize of requests
-W WORKLOAD, --workload=WORKLOAD 
either "rand" or "seq" workloads -w WRITEFRAC, --writeFrac=WRITEFRAC 
write fraction (100->all writes, 0->all reads)-R RANGE, --randRange=RANGErange of requests (when using "rand" workload)-L LEVEL, --level=LEVEL 
RAID level (0, 1, 4, 5)
-5 RAID5TYPE, --raid5=RAID5TYPE RAID-5 left-symmetric "LS" or left-asym "LA" 
-r, --reverse instead of showing logical ops, show physical 
-t, --timing use timing mode, instead of mapping mode 
-c, --compute compute answers for me 
The -C .ag allows you to set the chunk size of the RAID, instead of using the default size of one 4-KB block per chunk. The size of each request can be similarly adjusted with the -S .ag. The default workload accesses random blocks; use -W sequential to explore the behavior of sequential accesses. With RAID-5, two different lay­out schemes are available, left-symmetric and left-asymmetric; use-5 LS or -5 LA to try those out with RAID-5 (-L 5). 
Finally, in timing mode (-t), the simulator uses an incredibly sim­ple disk model to estimate how long a set of requests takes, instead of just focusing on mappnings. In this mode, a “random” request takes 10 milliseconds, whereas a “sequential” request takes0.1 mil­liseconds. The disk is assumed to have a tiny number of blocks per track (100), and a similarly small number of tracks (100). Youcan thus use the simulator to estimate RAID performance under some different workloads. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
Questions 
1. 
Use the simulator to perform some basic RAID mapping tests. Run with different levels (0, 1, 4, 5) and see if you can .gure out the mappings of a set of requests. For RAID-5, see if you can .gure out the difference between left-symmetric and left-asymmetric layouts. Use some different random seeds to gen­erate different problems than above. 

2. 
Do the same as the .rst problem, but this time vary the chunk size with -C.How doeschunk size change the mappings? 

3. 
Do the same as above, but use the -r .ag to reverse the nature of each problem. 

4. 
Now use the reverse .ag but increase the size of each request with the -S .ag. Try specifying sizes of 8k, 12k, and 16k, while varying the RAID level. What happens to the underlying I/O pattern when the size of the request increases? Make sure to try this with the sequential workload too (-W sequential); for what request sizes are RAID-4 and RAID-5 much more I/O ef.cient? 

5. 
Use the timing mode of the simulator (-t)to estimate the per­formance of 100 random reads to the RAID, while varying the RAID levels, using 4 disks. 

6. 
Do the same as above, but increase the number of disks. How does the performance of each RAID level scale as the number of disks increases? 

7. 
Do the same as above, but use all writes (-w 100)instead of reads. How does the performance of each RAID level scale now? Can you do a rough estimate of the time it will take to complete the workload of 100 random writes? 

8. 
Run the timing mode one last time, but this time with a sequen­tial workload (-W sequential). How does the performance vary with RAID level, and when doing reads versus writes? How about when varying the size of each request? What size should you write to a RAID when using RAID-4 or RAID-5? 


OPERATING SYSTEMS ARPACI-DUSSEAU 
38 




The Abstraction: Files and Directories 
Thus far we have seen the development of two key operating system abstractions: the process, which is a virtualization of the CPU, and the address space, which is a virtualization of memory. In tandem, these two abstractions allow a program to run as if it is in its own pri­vate, isolated world; as if it has its own processor (or processors); as if it has its own memory. This illusion makes programming the sys­tem much easier and thus is prevalent today not only on desktops and servers but increasingly on all programmable platforms includ­ing mobile phones and the like. 
In this section, we add one more critical piece to the virtualiza­tion puzzle: persistent storage.A persistent-storage device, such as aclassic hard disk drive or a more modern solid-state storage de-vice,stores information permanently (or at least,for a long time). Unlike memory, whose contents are lost when there is a power loss, apersistent-storage device keeps such dataintact. Thus, the OS must take extra care with such a device: this is where users keep data that they really care about. 
CRUX:HOW TO MANAGE A PERSISTENT DEVICE 
How should the OS manage a persistent device? What are the APIs? What are the important aspects of the implementation? 
483 
Thus, in the next few chapters, we will explore critical techniques for managing persistent data, focusing on methods to improveper­formance and reliability. We begin, however, with an overview of the API: the exact interfaces you’ll expect to see when interacting with a modern UNIX .le system. 
38.1 Files and Directories 
Two key abstractions have developed over time in the virtualiza­tion of storage. The .rst is the .le.A .le is simply a linear array of bytes, each of which you can read or write. Each .le has some kind of low-level name,usually a number of some kind;often, the user is not aware of this name (as we will see). For historical reasons, the low-level name of a .le is often referred to as its inode number.We’ll be learning a lot more about inodes in future chapters; for now, just assume that each .le has an inode number associated with it. 
In most systems, the OS does not know much about the struc­ture of the .le (e.g., whether it is a picture, or a text .le, or Ccode); rather, the responsibility of the .le system is simply to store such data persistently on disk and make sure that when you request the data again, you get what you put there in the .rst place. Doing so is not as simple as it might seem! 
The second abstraction is that of a directory.A directory, like a .le, also has a low-level name (i.e., an inode number), but itscontents are quite speci.c: it contains a list of (user-readable name,low-level name) pairs. For example, let’s say there is a .le with the low-level name “10”, and it is referred to by the user-readable name of “foo”. The directory “foo” resides in thus would have an entry (“foo”, “10”) that maps the user-readable name to the low-level name. Each entry in a directory refers to either .les or other directories. By placing di­rectories within other directories, users are able to build an arbitrary directory tree (or directory hierarchy), under which all .les and di­rectories are stored. 
The directory hierarchy starts at a root directory (in UNIX-based systems, the root directory is simply referred to as /)and uses some kind of separator to name subsequent sub-directories until the de­sired .le or directory is named. For example, if a user createda di­rectory foo in the root directory /,andthen created a .le bar.txt in the directory foo,we couldrefer to the .le by its absolute path-
OPERATING SYSTEMS ARPACI-DUSSEAU 

Figure 38.1: An Example Directory Tree 
name,which in this case would be /foo/bar.txt.See Figure 38.1 for a more complex directory tree; valid directories in the example are /, /foo, /bar, /bar/bar, /bar/foo and valid .les are /foo/bar.txt and /bar/foo/bar.txt.Directories and .les can have the same name as long as they are in different spots in the tree (e.g., there are two different .les named bar.txt in the .gure). 
You may also notice that the .le name in this example often has two parts: bar and txt,separatedby a period. The .rst part isan arbitrary name, whereas the second part of the .le name is usually used to indicate the type of the .le, e.g., whether it is C code (e.g., .c), or an image (e.g., .jpg), or a music .le (e.g., .mp3). However, this is usually just a convention:there is usually no enforcement that the data contained in a .le named main.c is indeed C source code. 
Thus, we can see one great thing provided by the .le system: a convenient way to name all the .les we are interested in. Names are important in systems as the .rst step to accessing any resource is being able to name it. In UNIX systems, the .le system thus provides auni.ed way to access .les on disk, USB stick, CD-ROM, many other devices, and in fact many other things, all located under the single directory tree. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
DESIGN TIP:THINK CAREFULLY ABOUT NAMING 
Naming is an important aspect of computer systems [SK09]. In UNIX 
systems, virtually everything that you can think of is named through 
the .le system. Beyond just .les, devices, pipes, and even processes 
[K84] can be found in what looks like a plain old .le system. This 
uniformity of naming eases your conceptual model of the system, 
and makes the system simpler and more modular. Thus, whenever 
creating a system or interface, think carefully about what names you 
are using. 

38.2 The File System Interface 
Let’s now discuss the .le system interface in more detail. We’ll start with the basics of creating, accessing, and deleting .les. You may think this straightforward, but along the way we’ll discover the mysterious call that is used to remove .les, known as unlink(). Hopefully, by the end of this chapter, this mystery won’t be somys­terious to you! 

38.3 Creating Files 
We’ll start with the most basic of operations: creating a .le.This can be accomplished with the open system call; by calling open() and passing it the O 
CREAT .ag, a program can create a new .le. Here is some example code to create a .le called “foo” in the current working directory. 
int fd = open("foo", O_CREAT | O_WRONLY | O_TRUNC); 
The routine open() takes a number of different .ags. In this ex­ample, the program creates the .le (O 
CREAT), can only write to that .le while opened in this manner (O 
WRONLY), and, if the .le already exists, .rst truncate it to a size of zero bytes thus removing any exist­ing content (O 
TRUNC). 
OPERATING SYSTEMS ARPACI-DUSSEAU 
ASIDE:THE CREAT() SYSTEM CALL The older way of creating a .le is to call creat(),as follows: 
int fd = creat("foo"); 
You can think of creat() as open() with the following .ags: O 
CREAT | O 
WRONLY | O 
TRUNC.Because open() can create a .le, the usage of creat() has somewhat fallen out of favor (indeed, it could just be implemented as a library call to open()); however, it does hold a special place in UNIX lore. Speci.cally, when Ken Thompson was asked what he would do differently if he were re­designing UNIX,he replied: “I’d spell creat with an e.” 
One important aspect of open() is what it returns: a .le descrip­tor.A .le descriptor is just an integer, private per process, andis used in UNIX systems to access .les; thus, once a .le is opened, you use the .le descriptor to read or write the .le, assuming you have permission to do so. In this way, a .le descriptor is a capability,i.e., an opaque handle that gives you the power to perform certain oper­ations. Another way to think of a .le descriptor is as a pointerto an object of type .le; once you have such an object, you can call other “methods” to access the .le, like read() and write().We’ll see just how a .le descriptor is used below. 

38.4 Reading and Writing Files 
Once we have some .les, of course we might like to read or write them. Let’s start by reading an existing .le. If we were typingat acommand line, we mightjust use the program cat to dump the contents of the .le to the screen. 
prompt> echo hello > foo 
prompt> cat foo 
hello 
prompt> 

In this code snippet, we redirect the output of the program echo to the .le foo,which then contains the word “hello” in it. We then use cat to see the contents of the .le. But how does the cat program access the .le foo? 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
To .nd this out, we’ll use an incredibly useful tool to trace the system calls made by a program. On Linux, the tool is called strace; other systems have similar tools (see dtruss on Mac OS X, or truss on some older UNIX variants). What strace does is trace every system call made by a program while it runs, and dump the trace to the screen for you to see. 
Here is an example of using strace to .gure out what cat is doing (some calls removed for readability): 
prompt> strace cat foo 
... 
open("foo", O_RDONLY|O_LARGEFILE) = 3 
read(3, "hello\n", 4096) = 6 
write(1, "hello\n", 6) = 6 
hello 
read(3, "", 4096) = 0 
close(3) =0 
... 
prompt> 
CODING TIP:USE STRACE 
The strace tool provides an awesome way to see what programs are up to. By running it, you can trace which system calls a program makes, see the arguments and return codes, and generally get avery good idea of what is going on. 
The tool also takes some arguments which can be quite useful. For example, -f follows any fork’d children too; -t reports the time of day at each call; -e trace=open,close,read,write only traces calls to those system calls and ignores all others. There are many more powerful .ags – read the man pages and .nd out how to har­ness this wonderful tool. 
The .rst thing that cat does is open the .le for reading. A couple of things we should note about this; .rst, that the .le is only opened for reading (not writing), as indicated by the O 
RDONLY .ag; second, that the 64-bit offset be used (O 
LARGEFILE); third, that the call to open() succeeds and returns a .le descriptor, which in this case has the value of 3. 
Why does the .rst call to open() return 3, not 0 or perhaps 1 
as you might expect? As it turns out, each running process already 
has three .les open, standard input (which the process can read to 
OPERATING SYSTEMS ARPACI-DUSSEAU 
receive input), standard output (which the process can writetoin or­der to dump information to the screen), and standard error (which the process can write error messages to). These are represented by .le descriptors 0, 1, and 2, respectively. Thus, when you .rstopen another .le (as cat does above), it will almost certainly be .le de­scriptor 3. 
After the open succeeds, cat uses the read() system call to re­peatedly read some bytes from a .le. The .rst argument to read() is the .le descriptor, thus telling the .le system which .le toread; a process can of course have multiple .les open at once, and thusthe descriptor enables the operating system to know which .le a partic­ular read refers to. The second argument points to a buffer where the result of the read() will be placed; in the system-call trace above, strace shows the results of the read in this spot (“hello”). The third argument is the size of the buffer, which in this case is 4 KB. The call to read() returns successfully as well, here returning the number of bytes it read (6, which includes 5 for the letters in the word “hello” and one for an end-of-line marker. 
At this point, you see another interesting result of the strace: a single call to the write() system call, to the .le descriptor 1. As we mentioned above, this descriptor is known as the standard output, and thus is used to write the word “hello” to the screen as the pro­gram cat is meant to do. But does it call write() directly? Proba­bly not. Rather, what cat does is call the library routine printf(); internally, printf() .gures out all the formatting details you pass it, and eventually calls write on the standard output to printthe re­sults to the screen. 
The cat program then tries to read more from the .le, but since there are no bytes left in the .le, the read() returns 0 and the pro­gram knows that this means it has read the entire .le. Thus, the program calls close() to indicate that it is done with the .le “foo”, passing in the corresponding .le descriptor. The .le is thus closed, and the reading of it thus complete. 
Writing a .le is accomplished via a similar set of steps. First, a .le is opened for writing, then the write() system call is called, perhaps repeatedly for larger .les, and then close().Use strace to trace writes to a .le, perhaps of a program you wrote yourself, or by tracing the dd utility: dd if=foo of=bar. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

38.5 Reading And Writing, But Not Sequentially 
Thus far, we’ve discussed how to read and write .les, but all ac­
cess has been sequential;that is, we have either read a .le from the 
beginning to the end, or written a .le out from beginning to end. 
Sometimes, however, it is useful to be able to read or write to a speci.c offset within a .le; for example, if you build an indexover a text document, and use it to look up a speci.c word, you may end up reading from some random offsets within the document. To do so, we will use the lseek() system call. Here is the function prototype: 
off_t lseek(int fildes, off_t offset, int whence); 
The .rst argument is familiar (a .le descriptor). The second ar­gument is the offset,which positions the .le offset to a particular location within the .le. The third argument, called whence for his­torical reasons, determines exactly how the seek is performed. From the man page: 
If whence is SEEK_SET, the offset is set to offset bytes. 
If whence is SEEK_CUR, the offset is set to its current 
location plus offset bytes.
If whence is SEEK_END, the offset is set to the size of 
the file plus offset bytes. 

As you can tell from this description, for each .le a process opens, the OS tracks a “current” offset, which determines where the next read or write will begin reading from or writing to within the .le. Thus, part of the abstraction of an open .le is that it has a current offset, which is updated in one of two ways. The .rst is when a read or write of N bytes takes place, N is added to the current offset; thus each read or write implicitly updates the offset. The second is explicitly with lseek,which changes the offset asspeci.edabove. 
Note that this call lseek() has nothing to do with the seek op­eration of a disk, which moves the disk arm. The call to lseek() simply changes the value of a variable within the kernel; whenthe I/O is performed, depending on where the disk head is, the diskmay or may not perform an actual seek to ful.ll the request. 

38.6 Getting Information About Files 
OPERATING SYSTEMS ARPACI-DUSSEAU 
CODING TIP: LSEEK() DOES NOT PERFORM ADISK SEEK The poorly-named system call lseek() confuses many a student trying to understand disks and how the .le systems atop them work. Do not confuse the two! The lseek() call simply changes a variable in OS memory that tracks, for a particular process, at which offset to which its next read or write will start. A disk seek occurs whena read or write issued to the disk is not on the same track as the last read or write, and thus necessitates a head movement. Making this even more confusing is the fact that calling lseek() to read or write from/to random parts of a .le, and then reading/writing to those random parts, will indeed lead to more disk seeks. Thus, calling lseek() can certainly lead to a seek in an upcoming read or write, but absolutely does not cause any disk I/O to occur itself. 
Beyond .le access, we expect the .le system to keep a fair amount of information about each .le it is storing. We generally callsuch data about .les metadata.To see the metadata for a certain .le, we can use stat() or fstat() system call – read their man pages for details on how to call them. These calls take a pathname (or .lede­scriptor) to a .le and .ll in a stat structure as seen here: 
struct stat {dev_t st_dev; /* ID of device containing file */ino_t st_ino; /* inode number */mode_t st_mode; /* protection */nlink_t st_nlink; /* number of hard links */uid_t st_uid; /* user ID of owner */gid_t st_gid; /* group ID of owner */dev_t st_rdev; /* device ID (if special file) */off_t st_size; /* total size, in bytes */blksize_t st_blksize; /* blocksize for filesystem I/O */blkcnt_t st_blocks; /* number of blocks allocated */time_t st_atime; /* time of last access */time_t st_mtime; /* time of last modification */time_t st_ctime; /* time of last status change */
}; 
You can see that there is a lot of information kept about each .le, including its size (in bytes), its low-level name (i.e., inode number), some ownership information, and some information about whenthe 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
.le was accessed or modi.ed, among other things. To see this infor­mation, you can use the command line tool stat: 
prompt> echo hello > file 
prompt> stat file File: ‘file’ Size: 6 Blocks: 8 IO Block: 4096 regular file 
Device: 811h/2065d Inode: 67158084 Links: 1 Access: (0640/-rw-r-----) Uid: (30686/ remzi) Gid: (30686/remzi)Access: 2011-05-03 15:50:20.157594748 -0500 Modify: 2011-05-03 15:50:20.157594748 -0500Change: 2011-05-03 15:50:20.157594748 -0500 
As it turns out, each .le system usually keeps this type of infor­mation in a structure called an inode1.We’ll be learning a lot more about inodes when we talk about .le system implementation. For now, you should just think of an inode as a persistent data structure kept by the .le system that has information like we see above inside of it. 

38.7 Removing Files 
At this point, we know how to create .les and access them, either sequentially or not. But how do you delete .les? If you’ve used UNIX,you probably think you know: just run the program rm.But what system call does rm use to remove a .le? 
Let’s use our old friend strace again to .nd out. Here we re­move that pesky .le “foo”: 
prompt> strace rm foo 
... 
unlink("foo") = 0 
... 
We’ve removed a bunch of unrelated cruft from the traced out­put, leaving just a single call to the mysteriously-named system callunlink().As you can see, unlink() just takes the name of the .le to be removed, and returns zero upon success. But this leads usto a great puzzle: why is this system call named “unlink”? Why not just 
1
Some .le systems call these structures similar, but slightlydifferent, names, such as dnodes; the basic idea is similar however. 
OPERATING SYSTEMS ARPACI-DUSSEAU DESIGN TIP:BE WARY OF POWERFUL COMMANDS 
The program rm provides us with a great example of powerful com­
mands, and how sometimes too much power can be a bad thing. For 
example, to remove a bunch of .les at once, you can type something 
like: 
prompt> rm * 
where the * will match all .les in the current directory. But some­times you want to also delete the directories too, and in fact all of their contents. You can do this by telling rm to recursively descend into each directory, and remove its contents too: 
prompt> rm -rf * 
Where you get into trouble with this small string of characters is when you issue the command, accidentally, from the root directory of a .le system, thus removing every .le and directory from it.Oops! Thus, remember the double-edged sword of powerful commands; while they give you the ability to do a lot of work with a small num­ber of keystrokes, they also can quickly and readily do a greatdeal of harm. 
“remove” or “delete”. To understand the answer to this puzzle, we must .rst understand more than just .les, but also directories. 

38.8 Making Directories 
Beyond .les, a set of directory-related system calls enable you to make, read, and delete directories. Note you can never write to a directory directly; because the format of the directory is considered .le system metadata, you can only update a directory indirectly by, for example, creating .les, directories, or other object types within it. In this way, the .le system makes sure that the contents of the directory always are as expected. 
To create a directory, a single system call, mkdir(),isavailable. The eponymous mkdir program can be used to create such a direc­tory. Let’s take a look at what happens when we run the mkdir program to make a simple directory called foo: 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
prompt> strace mkdir foo 
... 
mkdir("foo", 0777) = 0 
... 
prompt> 

When such a directory is created, it is considered “empty”, al­though it does have a bare minimum of contents. Speci.cally, an empty directory has two entries: one entry that refers to itself, and one entry that refers to its parent. The former is referred to as the “.” (dot) directory, and the latter as “..” (dot-dot). You cansee these directories by passing a .ag (-a)to the program ls: 
prompt> ls -a 
./ ../
prompt> ls -al
total 8 
drwxr-x---2 remzi remzi 6 Apr 30 16:17 ./
drwxr-x---26 remzi remzi 4096 Apr 30 16:17 ../ 


38.9 Reading Directories 
Now that we’ve created a directory, we might wish to read one too. Indeed, that is exactly what the program ls does. Let’s write our own little tool like ls and see how it is done. 
Instead of just opening a directory as if it were a .le, we instead use a new set of calls. Below is an example program that prints the contents of a directory. The program uses three calls, opendir(), readdir(),and closedir(),to get the job done, and you can see how simple the interface is, which reads one directory entry at a time. 
int main(int argc, char *argv[]) { DIR *dp = opendir(".");assert(dp != NULL); struct dirent *d; while ((d = readdir(dp)) != NULL) {
printf("%d %s\n", (int) d->d_ino, d->d_name); }closedir(dp); return 0; 
} 
The declaration below shows the information available within each directory entry in the struct dirent data structure: 
OPERATING SYSTEMS ARPACI-DUSSEAU 
struct dirent {char d_name[256]; /* filename */ino_t d_ino; /* inode number */off_t d_off; /* offset to the next dirent */unsigned short d_reclen; /* length of this record */unsigned char d_type; /* type of file */
}; 
Because directories are light on information (basically, just map­ping the name to the inode number, along with a few other details), a program may want to call stat() on each .le to get more informa­tion on each, such as its length or other detailed information. Indeed, this is exactly what ls does when you pass it the -l .ag; try strace on ls with and without that .ag to see for yourself. 

38.10 Deleting Directories 
Finally, you can delete a directory with a call to rmdir() (which is used by the program of the same name, rmdir). Unlike .le dele­tion, however, removing directories is more dangerous, as you could potentially delete a large amount of data with a single command. Thus, rmdir() has the requirement that the directory be empty (i.e., only has “.” and “..” entries) before it is deleted. If you try to delete anon-empty directory, the call to rmdir() simply will fail. 

38.11 Hard Links 
We now come back to the mystery of why removing a .le is per­formed via unlink(),by understanding a new way to make an en­try in the .le system tree, through a system call known as link(). The link() system call takes two arguments, an old pathname and anew one; when you “link”anew .le name to an old one, you essen­tially create another way to refer to the same .le. The command-line program ln is used to do this, as we see in this example: 
prompt> echo hello > file 
prompt> cat file 
hello 
prompt> ln file file2 
prompt> cat file2 
hello 

ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
Here we created a .le with the word “hello” in it, and called the .le file2.We then create a hard link to that .le using the ln pro­gram. After this, we can examine the .le by either opening file or file2. 
The way link works is that it simply creates another name in the directory you are creating the link to, and refers it to the same inode number (i.e., low-level name) of the original .le. The .le is not copied in any way; rather, you now just have two human names (file and file2)that both refer to the same .le. We can even see this in the directory itself, by printing out the inode numberof each .le: 
prompt> ls -i file file2
67158084 file 
67158084 file2 
prompt> 

By passing the -i .ag to ls,it prints out the inode number of each .le (as well as the .le name). And thus you can see what link really has done: just make a new reference to the same exact inode number (67158084 in this example). 
By now you might be starting to see why unlink() is called unlink().When you create a .le, you are really doing two things. First, you are making a structure (the inode) that will track virtually all relevant information about the .le, including its size, where its blocks are on disk, and so forth. Second, you are linking ahuman­readable name to that .le, and putting that link into a directory. 
After creating a hard link to a .le, to the .le system, there is no dif­ference between the original .le name (file)and the newly created .le name (file2); indeed, they are both just links to the underlying metadata about the .le, which is found in inode number 67158084. 
Thus, to remove a .le from the .le system, we call unlink(). 
In the example above, we could for example remove the .le named
file,and still access the .le without dif.culty: 
prompt> rm file 
removed ‘file’ 
prompt> cat file2 
hello 

2
Note how creative the authors are. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
The reason this works is because when the .le system unlinks .le, it checks a reference count within the inode number. This refer­ence count (sometimes called the link count)allows the .le system to track how many different .le names have been linked to this par­ticular inode. When unlink() is called, it removes the link from the human-readable name being deleted to the given inode number, and decrements the reference count; only when the reference count reaches zero does the .le system also free the inode, any related data blocks on disk, and thus really “delete” the .le. 
You can see the reference count of a .le using stat() of course. Let’s see what it is when we create and delete hard links to a .le. In this example, we’ll create three links to the same .le, and then delete them. Watch the link count! 
prompt> echo hello > file 
prompt> stat file 
... Inode: 67158084 Links: 1 ... 
prompt> ln file file2 
prompt> stat file 
... Inode: 67158084 Links: 2 ... 
prompt> stat file2 
... Inode: 67158084 Links: 2 ... 
prompt> ln file2 file3 
prompt> stat file 
... Inode: 67158084 Links: 3 ... 
prompt> rm file 
prompt> stat file2 
... Inode: 67158084 Links: 2 ... 
prompt> rm file2 
prompt> stat file3 
... Inode: 67158084 Links: 1 ... 
prompt> rm file3 


38.12 Symbolic Links 
There is one other type of link that is really useful, and it is called a symbolic link or sometimes a soft link.As it turns out, hard links are somewhat limited: you can’t create one to a directory (forfear that you will create a cycle in the directory tree); you can’t hard link to .les in other disk partitions (because inode numbers are only unique within a particular .le system, not across .le systems); etc. Thus, a new type of link called the symbolic link was created. 
To create such a link, you can use the same program ln,but with 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
the -s .ag. Here is an example: 
prompt> echo hello > file 
prompt> ln -s file file2 
prompt> cat file2 
hello 

As you can see, creating a soft link looks much the same, and the 
original .le can now be accessed through the .le name file as well 
as the symbolic link name file2. 
However, beyond this surface similarity, symbolic links areactu­ally quite different from hard links. The .rst difference is that a sym­bolic link is actually a .le itself, of a different type. We’vealready talked about regular .les and directories; symbolic links are a third type the .le system knows about. A stat on the symlink reveals this fact: 
prompt> stat file 
... regular file ... 

prompt> stat file2 
... symbolic link ... 

Running ls also reveals this fact. If you look closely at the .rst character of the long-form of the output from ls,you can see that the .rst character in the left-most column is a -for regular .les, a d for directories, and an l for soft links. You can also see the size of the symbolic link (4 bytes in this case), as well as what the link points to (the .le named file). 
prompt> ls -aldrwxr-x---2 remzi remzi 29 May 3 19:10 ./ drwxr-x---27 remzi remzi 4096 May 3 15:14 ../ -rw-r-----1 remzi remzi 6 May 3 19:10 file lrwxrwxrwx 1 remzi remzi 4 May 3 19:10 file2 -> file 
The reason that file2 is 4 bytes is because the way a symbolic link is formed is by holding the pathname of the linked-to .le as the data of the link .le. Because we’ve linked to a .le named file,our link .le file2 is small (4 bytes). If we link to a longer pathname, our link .le would be bigger. For example: 
prompt> echo hello > alongerfilename
prompt> ln -s alongerfilename file3 

OPERATING SYSTEMS ARPACI-DUSSEAU 
prompt> ls -al alongerfilename file3 -rw-r-----1 remzi remzi 6 May 3 19:17 alongerfilename lrwxrwxrwx 1 remzi remzi 15 May 3 19:17 file3 -> alongerfilename 
Finally, because of the way symbolic links are created, they leave 
the possibility for what is known as a dangling reference.For exam­
ple: 
prompt> echo hello > file 
prompt> ln -s file file2 
prompt> cat file2 
hello 
prompt> rm file 
prompt> cat file2 
cat: file2: No such file or directory 

As you can see in this example, quite unlike hard links, removing 
the original .le named file causes the link to point to a pathname 
that no longer exists. 

38.13 Making and Mounting a File System 
We’ve now toured the basic interfaces to access .les, directories, and certain types of special types of links. But there is one more topic we should discuss: how to assemble a full directory treefrom many underlying .le systems. This task is accomplished via .rst making .le systems, and then mounting them to make their contents accessible. 
To make a .le system, most .le systems provide a tool, usually referred to as mkfs (pronounced “make fs”), that performs exactly this task. The idea is as follows: give the tool, as input, a device (such as a disk partition, e.g., /dev/sda1)a .le system type (e.g., ext3), and it simply writes an empty .le system, starting witha root directory, onto that disk partition. And mkfs said, let therebea .le system! 
However, once such a .le system is created, it needs to be made accessible within the uniform .le-system tree. This task is achieved via the mount program (which makes the underlying system call mount() to do the real work). What mount does, quite simply is take an existing directory as a target mount point and essentially paste a new .le system onto the directory tree at that point. 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
An example seems useful. Imagine we have an unmounted ext3 .le system, stored in device partition /dev/sda1,that hasthe fol­lowing contents: a root directory which contains two sub-directories, a and b,each of which in turn holds a single .le named foo.Let’s say we wish to mount this .le system at the mount point /home/users. We would type something like this: 
prompt> mount -t ext3 /dev/sda1 /home/users prompt> 
If successful, the mount would thus make this new .le system available. However, note how the new .le system is now accessed. To look at the contents of the root directory, we would use ls like this: 
prompt> ls /home/users/
ab 

As you can see, the pathname /home/users/ now refers to the root of the newly-mounted directory. Similarly, we could access .les a and b with the pathnames /home/users/a and /home/users/b. Finally, the .les named foo could be accessed via /home/users/a/foo and /home/users/b/foo.And thus the beauty of mount: instead of having a number of separate .le systems, mount uni.es all .le systems into one tree, making naming uniform and convenient. 
To see what is mounted on your system, and at which points, sim­ply run the mount program. You’ll see something like this: 
/dev/sda1 on / type ext3 (rw) 
proc on /proc type proc (rw) 
sysfs on /sys type sysfs (rw) 
devpts on /dev/pts type devpts (rw,gid=5,mode=620) 
/dev/sda8 on /scratch type ext3 (rw) 
/dev/sdb1 on /scratch.1 type xfs (rw) 
/dev/sda6 on /tmp type ext3 (rw) 
/dev/sda3 on /var type ext3 (rw) 
/dev/sda7 on /var/vice/cache type ext3 (rw) 
/dev/sda2 on /usr type ext3 (rw) 
tmpfs on /dev/shm type tmpfs (rw) 
AFS on /afs type afs (rw) 

This crazy mix shows that a whole number of different .le sys­tems, including ext3 (a standard disk-based .le system), theproc .le 
OPERATING SYSTEMS ARPACI-DUSSEAU 
system (a .le system for accessing information about currentpro­cesses), tmpfs (a .le system just for temporary .les), and AFS(a dis­tributed .le system) are all glued together onto this one machine’s .le-system tree. 

38.14 Summary 
The .le system interface in UNIX systems (and indeed, in any sys­tem) is seemingly quite rudimentary, but there is a lot to understand if you wish to master it. Nothing is better, of course, than simply us­ing it a lot. So please do so! Of course, read more; as always, Stevens [S92] is the place to begin. 
We’ve toured the basic interfaces, and hopefully understooda lit­tle bit about how they work. Even more interesting is how to imple­ment a .le system that meets the needs of the API, a topic we will delve into in great detail next. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

References 
[K84] “Processes as Files” 
Tom J. Killian 
USENIX, June 1984 

The paper that introduced the /proc .le system, where each process can be treated as a .le within a pseudo .le system. A clever idea that you can stillsee in modern UNIX systems. 
[SK09] “Principles of Computer System Design” Jerome H. Saltzer and M. Frans Kaashoek Morgan-Kaufmann, 2009 
This tour de force of systems is a must-read for anybody interested in the .eld. It’s how they teach systems at MIT. Read it once, and then read it a few more times to let it all soak in. 
[S92] “Advanced Programming in the UNIX Environment” 
W. Richard Stevens and Stephen A. Rago 
Addison-Wesley, 1992 

We have probably referenced this book a few hundred thousand times. It is that useful to you, if you care to become an awesome systems programmer. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

Homework 
In this homework, we’ll just familiarize ourselves with how the APIs described in the chapter work. To do so, you’ll just writea few different programs, mostly based on various UNIX utilities. 
Questions 
1. 	
Stat: Write your own version of the command line program 
stat,which simply callsthe stat() system call on a given 
.le or directory. Print out .le size, number of blocks allocated, 
reference (link) count, and so forth. What is the link count of 
adirectory, as the number of entries in the directory changes? 
Useful interfaces: stat() 


2. 	
List Files: Write a program that lists .les in the given direc­tory. When called without any arguments, the program should 
just print the .le names. When invoked with the -l .ag, the 
program should print out information about each .le, such 
as the owner, group, permissions, and other information ob­tained from the stat() system call. The program should take 
one additional argument, which is the directory to read, e.g.,
myls -l directory.If no directory is given, the program 
should just use the current working directory. Useful inter­faces: stat(), opendir(), readdir(), getcwd(). 


3. 	
Tail: Write a program that prints out the last few lines of a 
.le. The program should be ef.cient, in that it seeks to near 
the end of the .le, reads in a block of data, and then goes back­wards until it .nds the requested number of lines; at this point, 
it should print out those lines from beginning to the end of 
the .le. To invoke the program, one should type: mytail -n 
file,where n is the number of lines at the end of the .le to 
print. Useful interfaces: stat(), lseek(), open(), read(), 
close(). 


4. 	
Recursive Search: Write a program that prints out the names 
of each .le and directory in the .le system tree, starting at a 
given point in the tree. For example, when run without ar­guments, the program should start with the current working 
directory and print its contents, as well as the contents of any 



THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
sub-directories, etc., until the entire tree, root at the CWD, is printed. If given a single argument (of a directory name), use that as the root of the tree instead. Re.ne your recursive search with more fun options, similar to the powerful find command line tool. Useful interfaces: you .gure it out. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
39 




File System Implementation 
In this note, we introduce a simple .le system implementation, known as vsfs (the Very Simple File System). This .le system is a simpli.ed version of a typical UNIX .le system and thus serves to introduce some of the basic on-disk structures, access methods, and various policies that you will .nd in many .le systems today. 
The .le system is pure software; unlike our development of CPU and memory virtualization, we will not be adding hardware features to make some aspect of the .le system work better (though we will want to pay attention to device characteristics to make sure the .le system works well). Because of the great .exibility we have inbuild­ing a .le system, many different ones have been built, literally from AFS (the Andrew File System) [H+88] to ZFS (Sun’s Zettabyte File System) [B07]. All of these .le systems have different data structures and do some things better or worse than their peers. Thus, the way we will be learning about .le systems is through case studies:.rst, a simple .le system (vsfs) in this chapter to introduce most concepts, and then a series of studies of real .le systems to understand how they can differ in practice. 
505 
THE CRUX:HOW TO IMPLEMENT A SIMPLE FILE SYSTEM 
How can we build a simple .le system? What structures are 
needed on the disk? What do they need to track? How are they 
accessed? 
39.1 The Way To Think 
To think about .le systems, we usually suggest thinking about 
two different aspects of them; if you understand both of theseas­
pects, you probably understand how the .le system basically works. 
The .rst is the data structures of the .le system. In other words, what types of on-disk structures are utilized by the .le system to organize its data and metadata? The .rst .le systems we’ll see(in­cluding vsfs below) employ simple structures, like arrays ofblocks or other objects, whereas more sophisticated .le systems, like SGI’s XFS, use more complicated tree-based structures [S+96]. 
The second aspect of a .le system is its access methods.How does it map the calls made by a process, such as open(), read(), write(),etc., onto itsstructures? Which structures are readduring the execution of a particular system call? Which are written?How ef.ciently are all of these steps performed? 
If you understand the data structures and access methods of a .le system, you have developed a good mental model of how it truly works, a key part of the systems mindset. Try to work on developing your mental model as we delve into our .rst implementation. 

39.2 Overall Organization 
We’ll now develop the overall on-disk organization of the data structures of the vsfs .le system. The .rst thing we’ll need todois divide the disk into blocks of some size; simple .le systems use just one block size, and that’s exactly what we’ll do here. Let’s choose a commonly-used size of 4 KB. 
Thus, our view of the disk partition where we’re building our .le system is simple: a series of blocks, each of size 4 KB. The blocks are addressed from 0 to N - 1,in a partition of size N 4-KB blocks. Assume we have a really small disk, with just 64 blocks: 
OPERATING SYSTEMS ARPACI-DUSSEAU 
ASIDE:MENTAL MODELS As we’ve discussed before, mental models are what you are really trying to develop when learning about systems. For .le systems, your mental model should eventually include answers to questions like: what on-disk structures store the .le system’s data andmeta­data? What happens when a process opens a .le? Which on-disk structures are accessed during a read or write? By working on and improving your mental model, you develop an abstract under­standing of what is going on, instead of just trying to understand the speci.cs of some .le-system code (though that is also useful, of course!). 
BBBBBBBB BBBBBBBB BBBBBBBB BBBBBBBB BBBBBBBB BBBBBBBB BBBBBBBB BBBBBBBB 0 78 1516 2324 3132 3940 4748 5556 63 
Let’s now think about what we need to store in these blocks to build a .le system. Of course, the .rst thing that comes to mindis user data. In fact, most of the space in any .le system is (and should be) user data. Let’s call the region of the disk we use for user data the data region,and, again for simplicity, reserve a .xedportion of the disk for these blocks, say the last 56 of 64 blocks on the disk: 
<---------------------The Data Region ----------------------> ???????? DDDDDDDD DDDDDDDD DDDDDDDD DDDDDDDD DDDDDDDD DDDDDDDD DDDDDDDD 0 78 1516 2324 3132 3940 4748 5556 63 
As we learned about (a little) last chapter, the .le system hasto track information about each .le. This information is a key piece of metadata,and tracks things like which data blocks (in the data region) comprise a .le, the size of the .le, its owner and access rights, access and modify times, and other similar kinds of information. To store this information, .le system usually have a structure called an inode (we’ll read more about inodes below). 
To accommodate inodes, we’ll need to reserve some space on the disk for them as well. Let’s call this portion of the disk the inode table,which simply holds an array of on-disk inodes. Thus,our on-disk image now looks like this picture, assuming that we use 5 of our 64 blocks for inodes (denoted by I’s in the diagram): 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
<---------------------The Data Region ----------------------> 
???IIIII DDDDDDDD DDDDDDDD DDDDDDDD DDDDDDDD DDDDDDDD DDDDDDDD DDDDDDDD 
0 78 1516 2324 3132 3940 4748 5556 63 
We should note here that inodes are typically not that big, forex­ample 128 bytes in size. Thus, a 4-KB block can hold 32 inodes, and our .le system above can thus contain 160 inodes. In our simple.le system, built on a tiny 64-block partition, this number represents the maximum number of .les we can have in our .le system; however, do note that the same .le system, built on a larger disk, could simply allocate a larger inode table and thus accommodate more .les. 
Our .le system thus far has data blocks (D), and inodes (I), but afew things are still missing. One primary componentthat is still needed, as you might have guessed, is some way to track whether inodes or data blocks are free or allocated. Such allocation structures are thus a requisite element in any .le system. 
Many allocation-tracking methods are possible, of course. For ex­ample, we could use a free list that points to the .rst free block, which then points to the next free block, and so forth. We instead choose a simple and popular structure known as a bitmap,one for the data region (the data bitmap), and one for the inode table (the inode bitmap). A bitmap is a simple structure: each bit is used to indicate whether the corresponding object/block is free (0)or in-use (1). And thus our new on-disk layout, with an inode bitmap (i) and adata bitmap (d): 
<---------------------The Data Region ----------------------> 
?idIIIII DDDDDDDD DDDDDDDD DDDDDDDD DDDDDDDD DDDDDDDD DDDDDDDD DDDDDDDD 
0 78 1516 2324 3132 3940 4748 5556 63 
You may notice that it is a bit of overkill to use an entire 4-KB block for these bitmaps; such a bitmap can track whether 32K objects are allocated, and yet we only have 160 inodes and 56 data blocks. However, we just use an entire 4-KB block for each of these bitmaps for simplicity. 
The careful reader1 may have noticed there is one block left in the design of the on-disk structure of our very simple .le system.We re­serve this for the superblock,denotedby an S in the diagram below. The superblock contains information about this particular .le sys­tem, including, for example, how many inodes and data blocks are 
1
Or rather, the reader who is not completely asleep. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
in the .le system (160 and 56, respectively in this instance),where the inode table begins (block 3), and so forth. It will likely alsoinclude amagic number of some kind to identify the .le system type (in this case, vsfs). 
<---------------------The Data Region ----------------------> 
SidIIIII DDDDDDDD DDDDDDDD DDDDDDDD DDDDDDDD DDDDDDDD DDDDDDDD DDDDDDDD 
0 78 1516 2324 3132 3940 4748 5556 63 
Thus, when mounting a .le system, the operating system will read the superblock .rst, to initialize various parameters,and then attach the volume to the .le-system tree. When .les within thevol­ume are accessed, the system will thus know exactly where to look for the needed on-disk structures. 

39.3 File Organization: The Inode 
One of the most important on-disk structures of a .le system isthe inode;virtually all .le systems have a structure similar to this. The name inode is short for index node,the historical name given to it by UNIX inventor Ken Thompson [RT74], used because these nodes were originally arranged in an array, and the array indexed into when accessing a particular inode. 
Each inode is implicitly referred to by a number (called the inum­ber), which we’ve earlier called the low-level name of the .le. In vsfs (and other simple .le systems), given an i-number, you should directly be able to calculate where on the disk the corresponding in-ode is located. For example, take the inode table of vsfs as above: 20-KB in size (5 4-KB blocks) and thus consisting of 160 inodes(again assuming each inode is 128 bytes); further assume that the inode re­gion starts at 12KB (i.e, the superblock starts at 0KB, the inode bitmap is at address 4KB, the data bitmap at 8KB, and thus the inode table comes right after). In vsfs, we thus have the following layoutfor the beginning of the .le system partition: 
0 4KB 8KB 12KB 16KB 20KB 24KB 28KB 32KB Super | iBmap | dBmap | iBlk0 | iBlk1 | iBlk2 | iBlk3 | iBlk4 | ... 0-31 32-63 64-95 96-127 128-159 <-----------The inode Table -----------> 
To read inode number 64, the .le system would .rst calculate the offset into the inode region (64 · sizeof(inode) or 8192,add it to the 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
start address of the inode table on disk (inodeStartAddr = 12KB), and thus arrive upon the correct byte address of the desired block of inodes: 20KB.Recall that disks are not byte addressable, but rather consist of a large number of addressable sectors, usually of size 512 bytes. Thus, to fetch the block of inodes that contains inode 64, the .le system would issue a read to sector 20×1024 ,or 40, to fetch the 
512 
desired inode block. More generally, the sector address iaddr of the inode block can be calculated as follows: 
blk = (inumber * sizeof(inode_t)) / blockSize; 
sector = ((blk * blockSize) + inodeStartAddr) / sectorSize; 

Inside each inode is virtually all of the information you needabout a.le: its type (e.g., regular .le, directory, etc.), its size,the number of blocks allocated to it, protection information (such as who owns the .le, as well as who can access it), some time information, including when the .le was created, modi.ed, or last accessed, as well as informa­tion about where its data blocks reside on disk (e.g., pointers of some kind). We refer to all such information about a .le as metadata;in fact, any information inside the .le system that isn’t pure user data is often referred to as such. An example inode from ext2 [P09] is shown below in Figure 39.1. 
One of the most important decisions in the design of the inode is how it refers to where data blocks are. One simple approach would be to have one or more direct pointers (disk addresses) inside the inode; each pointer refers to one disk block that belongs to the .le. Such an approach is limited: for example, if you want to have a .le that is really big (e.g., bigger than the size of a block multiplied by the number of direct pointers), you are out of luck. 
The Multi-Level Index 
To support bigger .les, .le system designers have had to introduce different structures within inodes. One common idea is to have a special pointer known as an indirect pointer.Instead of pointing to a block that contains user data, it points to a block that contains more pointers, each of which point to user data. Thus, an inodemay have some .xed number of direct pointers (say 12), and then a single indirect pointer. If a .le grows large enough, an indirect block is allocated (from the data-block region of the disk), and the inode’s 
OPERATING SYSTEMS ARPACI-DUSSEAU 
Size Name What is this inode .eld for? 
2  mode  can this .le be read/written/executed?  
2  uid  who owns this .le?  
4  size  how many bytes are in this .le?  
4  time  what time was this .le last accessed?  
4  ctime  what time was this .le created?  
4  mtime  what time was this .le last modi.ed?  
4  dtime  what time was this inode deleted?  
2  gid  which group does this .le belong to?  
2  links count  how many hard links are there to this .le?  
4  blocks  how many blocks have been allocated to this .le?  
4  .ags  how should ext2 use this inode?  
4  osd1  an OS-dependent .eld  
60  block  a set of disk pointers (15 total)  
4  generation  .le version (used by NFS)  
4  .le acl  a new permissions model beyond mode bits  
4  dir acl  called access control lists  
4  faddr  an unsupported .eld  
12  i osd2  another OS-dependent .eld  
Table 39.1: The ext2 inode  

slot for an indirect pointer is set to point to it. Assuming that a block is 4KB and that each disk pointer is 4 bytes, that adds another 1024 pointers and thus the .le can grow to be (12 + 1024) · 4K or 4144KB in size. 
Not surprisingly, in such an approach, you might want to support even larger .les. To do so, just add another pointer to the inode: the double indirect pointer.This pointer refers to a block that contains pointers to indirect blocks, each of which contain pointers to data blocks. A double indirect block thus adds the possibility to grow .les with an additional 1024 · 1024 or 1-million 4KB blocks, in other words supporting .les that are over 4GB in size. You may want even more, though, and I bet you know where this is headed: the triple indirect pointer. 
Overall, this imbalanced tree is referred to as the multi-level in­dex approach to pointing to .le blocks. Many .le systems use such an approach, including commonly-used .le systems such as Linux ext2 [P09] and ext3 as well as the original UNIX .le system. Other .le systems (e.g., SGI XFS) use extents instead of simple pointers; see the aside for details. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
ASIDE:EXTENT-BASED APPROACHES 
Adifferent approach is to use extents instead of pointers. An extent 
is simply a disk pointer plus a length (in blocks); thus, instead of re­
quiring a pointer for every block of a .le, all one needs is a pointer 
and a length to specify the on-disk location of a .le. Just a single ex­
tent is limiting, as one may have trouble .nding a contiguous chunk 
of on-disk free space when allocating a .le. Thus, extent-based .le 
systems often allow for more than one extent, thus giving morefree­
dom to the .le system during .le allocation. 
In comparing the two approaches, pointer-based approaches are the most .exible but use a large amount of metadata per .le (particularly for large .les). Extent-based approaches are less .exible but more compact; in particular, they work well when there is enough free space on the disk and .les can be laid out contiguously (which is the goal for virtually any .le allocation policy anyhow). 
You might be wondering: why use an imbalanced tree like this? Why not a different approach? Well, as it turns out, many researchers have studied .le systems and how they are used, and virtually ev­ery time they .nd certain “truths” that hold across the decades. One such .nding is that most .les are small.This imbalanced design re­.ects such a reality; if most .les are indeed small, it makes sense to optimize for this case. Thus, with a small number of direct pointers (12 is a typical number), an inode can directly point to 48-KB of data, only needing one (or more) indirect blocks for larger .les. See [A+07] for a recent study; Table 39.2 summarizes the results. 
Of course, in the space of inode design, many other possibilities 
exist; after all, the inode is just a data structure, and any data struc­
ture that stores the relevant information is suf.cient. 
Most .les are small 
Roughly 2K is the most common size 
Average .le size is growing 
Almost 200K is the average 
Most bytes are stored in large .les 
Afew big .les use most of the space 
File systems contains lots of .les 
Almost 100K on average 
File systems are roughly half full 
Even as disks grow, .le systems remain almost 50% full 
Directories are typically small 
Many have just a few entries; most have 20 or fewer 
Table 39.2: File System Measurement Summary 
OPERATING SYSTEMS ARPACI-DUSSEAU 

ASIDE:LINKED-BASED APPROACHES 
Another simpler approach in designing inodes is to use a linked list.Thus, inside an inode, instead of having multiple pointers,you just need one, to point to the .rst block of the .le. To handle larger .les, add another pointer at the end of that data block, and so on, and thus you can support large .les. 
Of course, as you might have guessed, linked .le allocation per­forms very poorly for some workloads; think about reading thelast block of a .le, for example, or just doing random access. Thus,to make linked allocation work better, some systems will keep anin­memory table of link information, instead of storing the nextpoint­ers with the data blocks themselves. The table is indexed by the ad­dress of a data block D;the content of an entry is simply D’s next pointer, i.e., the address of the next block in a .le which follows D. Anull-value could be there too (indicating an end-of-.le), or some other marker to indicate that a particular block is free. Having such a table of next pointers makes it so that a linked allocation scheme can effectively do random .le accesses, simply by .rst scanning through the (in memory) table to .nd the desired block, and then accessing (on disk) it directly. 
Does such a table sound familiar? Turns out that what we have described is the basic structure of what is known as the .le allo­cation table,or FAT .le system. Yes, this classic old Windows .le system, before NTFS [C94], is based on a simple linked-based alloca­tion scheme. There are other differences from a standard UNIX .le system too; for example, there are no inodes per se, but ratherdirec­tory entries which store metadata about a .le and refer directly to the .rst block of said .le, which makes creating hard links impossible. See Brouwer [B02] for more of the inelegant details. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

39.4 Directory Organization 
In vsfs (as in many .le systems), directories have a simple organi­zation; a directory basically just contains a list of (entry name, inode number) pairs. Thus, for each .le or directory in a given directory, there is a string and a number in the data block(s) of the directory. For each string, there may also be a length (assuming variable-sized .le names). 
For example, assume a directory dir (inode number 5) has three .les in it (foo, bar,and foobar), and their inode numbers are 12, 13, and 24 respectively. The on-disk data block for dir might look like this: 
inum | reclen | strlen | name 
54 2. 
2 4 3.. 12 4 4foo 13 4 4bar 24 8 7 foobar 
In this example, each entry has an inode number, record length (the total bytes for the name plus any left over space), stringlength (the actual length of the name), and .nally the name of the entry. Note that each directory has two extra entries, . “dot” and .. “dot­dot”; the dot directory is just the current directory (in thisexample,dir), whereas dot-dot is the parent directory (in this example, the root). 
Deleting a .le (e.g., calling unlink())can leave an empty space in the middle of the directory, and hence there should be some way to mark that as well (e.g., with a reserved inode number such as zero). Such a delete is one reason the record length is used: a new entry may reuse an old, bigger entry and thus have a fair amount of space within. 
You might be wondering where exactly directories are stored.Most 
.le systems just treat directories as a special type of .le. Thus, a di­
rectory has an inode, somewhere in the inode table (with the type 
.eld of the inode marked as “directory” instead of “regular .le”). 
The directory has data blocks pointed to by the inode (and perhaps, 
indirect blocks); these data blocks live in the data block region of our 
simple .le system. Our on-disk structure thus remains unchanged. 
We should also note again that this simple linear list of directory entries is not the only way to store such information. As before, any 
OPERATING SYSTEMS ARPACI-DUSSEAU 
data structure is possible. For example, XFS [S+96] stores directories in binary tree form, making .le create operations (which haveto en­sure that a .le name has not been used before creating it) faster than systems with simple lists that must be scanned in their entirety. 

39.5 Free Space Management 
A.le system must track which inodes and data blocks are free, and which are not, so that when a new .le or directory is allocated, it can readily .nd space for it. Thus, free space management is an important aspect of any .le system. In vsfs, we have two simple bitmaps for this task. 
For example, when we create a .le, we will have to allocate an in-ode for that .le. The .le system will thus search through the bitmap for an inode that is free, and allocate it to the .le; the .le system will have to mark the inode as used (with a 1) and eventually update the on-disk bitmap with the correct information. A similar set ofactivi­ties take place when a data block is allocated. 
Some other considerations might also come into play when allo­cating data blocks for a new .le. For example, some Linux .le sys­tems, such as ext2 and ext3, will look for a sequence of blocks (say 8) that are free when a new .le is created and needs data blocks;by .nding such a sequence of free blocks, and then allocating them to the newly-created .le, the .le system guarantees that a portion of the .le will be on the disk and contiguous, thus improving performance. Such a pre-allocation policy is thus a commonly-used heuristic when allocating space for data blocks. 

39.6 Access Paths: Reading and Writing 
Now that we have some idea of how .les and directories are stored on disk, we should be able to follow the .ow of operation during the activity of reading or writing a .le. Understanding what happens on this access path is thus the second key in developing an understanding of how a .le system works; pay attention! 
For the following examples, let us assume that the .le system has 
been mounted and thus that the superblock is already in memory. 
Everything else (i.e., inodes, directories) is still on the disk. 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
ASIDE:FREE SPACE MANAGEMENT 
There are many ways to manage free space; bitmaps are just one way. 
Some early .le systems used free lists,where a single pointer in the 
super block was kept to point to the .rst free block; inside that block 
the next free pointer was kept, thus forming a list through thefree 
blocks of the system. When a block was needed, the head block was 
used and the list updated accordingly. 
Modern .le systems use more sophisticated data structures. For ex­ample, SGI’s XFS [S+96] uses some form of a binary tree to com­pactly represent which chunks of the disk are free. As with anydata structure, different time-space trade-offs are possible. 
Reading A File From Disk 
In this simple example, let us .rst assume that you want to simply open a .le (e.g., /foo/bar.txt,readit, and then close it. For this simple example, let’s assume the .le is just 4KB in size (i.e.,1 block). 
When you issue an open("/foo/bar.txt", O RDONLY) call, the .le system .rst needs to .nd the inode for the .le bar.txt and make sure it is OK that you are opening it (i.e., that you have the correct permissions). To do so, the .le system must be able to .nd the inode. Unfortunately, all the FS has right now is the full pathname. Thus, it must traverse the pathname and in doing so locate the inode of the desired .le. 
All traversals begin at the root of the .le system, in the root di­rectory which is simply called /.Thus, the .rst thing the FS will read from disk is the inode of the root directory. But where is this inode? To .nd an inode, we must know its i-number. Usually, we .nd the i-number of a .le or directory in its parent directory;the root has no parent (by de.nition). Thus, the root inode numbermust be “well known”; the FS must know what it is when the .le system is mounted. In most UNIX .le systems, the root inode number is 2. Thus, to begin the process, the FS reads in the block that contains inode number 2 (the .rst inode block). 
Once the inode is read in, the FS can look inside of it to .nd point­
ers to data blocks, which contain the contents of the root directory. 
The FS will thus use these on-disk pointers to read through thedi-
OPERATING SYSTEMS ARPACI-DUSSEAU 

rectory, in this case looking for an entry for foo.By reading in one or more directory data blocks, it will .nd the entry for foo; oncefound, the FS will also have found the inode number of foo (say it is 44) which it will need next. 
The next step is to recursively traverse the pathname until the desired inode is found. In this example, the FS would next read the block containing the inode of foo and then read in its directory data, .nally .nding the inode number of bar.txt.The .nal step of open(),then, is to read its inode into memory;the FS can then do a.nal permissions check, allocate a.le descriptor for this process in the per-process open-.le table, and return it to the user. 
open 
bar.txt read 4k read 4k read 4k
block 
12  bar.txt data[2]  
11  bar.txt data[1]  
10  bar.txt data[0]  
9  foo data  
8  root data  
3  bar.txt inode  
3  foo inode  
3  root inode  
2  d-bitmap  
1  i-bitmap  
0  

R 
R 
R 
R 
RR 
R 
R 
WR WR 
R 
W 
1 2 3 4 5 6 7 8 9 10111213141516171819202122232425262728 
I/O Timeline 
Figure 39.1: File Read Timeline Once open, the program can then issue a read() system call to read from the .le. The .rst read (at offset 0 unless lseek() has been called) will thus read in the .rst block of the .le, consulting the inode to .nd the location of such a block; it may also updatethe inode with a new last-accessed time. The read will further update the in-memory open .le table for this .le descriptor, updating the .le offset such that the next read will read the second block ofthe .le, and so forth. At some point, the .le will be closed. There is much less work to be done here; clearly, the .le descriptor should be deallocated, but for now, that is all the FS really needs to do. No disk I/Os take place. Adepiction of this entire process is found in Figure 39.1. As you can see from the graph, the open causes numerous reads to take place 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
in order to .nally locate the inode of the .le. Afterwards, reading each block requires the .le system to .rst consult the inode, then read the block, and then update the inode’s last-accessed-time .eld with a write. 
You might also notice that the amount of I/O generated by the open is proportional to the length of the pathname. For each addi­tional directory in the path, we have to read its inode as well as its data. Making this worse would be the presence of large directories; here, we only have to read one block to get the contents of a direc­tory, whereas with a large directory, we might have to read many data blocks to .nd what we are looking for. 

Writing to Disk 
Writing to a .le is a similar process. First, the .le must be opened (as above). Then, the application can issue write() calls to update the .le with new contents. Finally, the .le is closed. 
Unlike reading, writing to the .le may also allocate ablock (un­less the block is being overwritten, for example). When writing out anew .le, each write notonly has to write datato disk buthas to .rst decide which block to allocate to the .le and thus update other structures of the disk accordingly (e.g., the data bitmap). Thus, each write to a .le logically generates three I/Os: one to read the data bitmap, which is then updated to mark the newly-allocated block as used, one to write the bitmap (to re.ect its new state to disk),and one to the actual block itself. 
The amount of write traf.c is even worse when one considers a simple and common operation such as .le creation. To create a .le, the .le system must not only allocate an inode, but also allocate space within the directory containing the new .le. The total amountof I/O traf.c to do so is quite high: one read to the inode bitmap (to .nd a free inode), one write to the inode bitmap (to mark it allocated), one write to the new inode itself (to initialize it), one to the data of the directory (to link the high-level name of the .le to its inode number), and one read and write to the directory inode to update it. If the directory needs to grow to accommodate the new entry, additional I/Os (to the data bitmap to allocate the block and to the new block to record the entry) will be needed too. All of that work just tocreate a.le! 
OPERATING SYSTEMS ARPACI-DUSSEAU 
Let’s look at a speci.c example, where the .le /foo/bar.txt is created, and three blocks are written to it. Figure 39.2 showswhat happens during the open() (which creates the .le), as well as dur­ing each of three 4-KB calls to write(). 
create /foo/bar.txt write 4k write 4k write 4k
block 
12 11 10 9 8 
3 3 3 2 1 0 
bar.txt data[2] bar.txt data[1] bar.txt data[0] foo data root data 
bar.txt inode foo inode root inode d-bitmap i-bitmap 
W 
W 
W 
R 
R 
R 
R 
W WWR 
R  W  R  
W  

RW  

R W  
R W  

RW RW 
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 I/O Timeline  
Figure 39.2: File Creation Timeline  
In the .gure, reads and writes to the disk are grouped under which system call caused them to occur, and the rough orderingthey might take place in goes from left to right along the x-axis. The y-axis shows which block is being accessed (read or written) at a particular point in time. You can see how much work it is to create the .le: 10 I/Os in this case, to walk the pathname and then .nally create the .le. You can also see that each allocating write costs 5 I/Os: apair to read and update the inode, another pair to read and update the data bitmap, and then .nally the write of the data itself. How can a .le system accomplish any of this with reasonable ef.ciency?  
The Crux: HOW TO REDUCE FILE SYSTEM I/O COSTS Even the simplest of operations like opening, reading, or writing a .le incurs a huge number of I/O operations, scattered over thedisk. What can a .le system do to reduce the obvious high costs of doing so many I/Os?  
ARPACI-DUSSEAU  THREE EASY PIECES (V0.5)  



39.7 Caching and Buffering 
As the examples above show, reading and writing .les can be ex­pensive, incurring many I/Os to the (slow) disk. To remedy what would clearly be a huge performance problem, most .le systemsag­gressively use system memory (DRAM) to cache important blocks. 
Imagine the open example above: without caching, every .le open would require at least two reads for every level in the directory hi­erarchy (one to read the inode of the directory in question, and at least one to read its data). With a long pathname (e.g., /1/2/3/ ... /100/.le.txt), the .le system would literally perform hundreds of reads just to open the .le! 
Early .le systems thus introduced a .x-sized cache to hold pop­ular blocks. As in our discussion of virtual memory, strategies such as LRU and different variants would decide which blocks to keep in cache. This .x-sized cache would usually be allocated at boottime to be roughly 10% of the total memory of the system. Modern systems integrate virtual memory pages and .le system pages into a uni.ed page cache [S00]. In this way, memory can be allocated much more .exibly across virtual memory and .le system, depending on which needs more memory at a given time. 
Now imagine the .le open example with caching. The .rst open may generate a lot of I/O traf.c to read in directory inode and data, but subsequent .le opens of that same .le (or .les in the same direc­tory) will mostly hit in the cache and thus no I/O is needed. 
Let us also consider the effect of caching on writes. Whereas read I/O can be avoided altogether with a suf.ciently large cache,write traf.c has to go to disk in order to become persistent. Thus, a cache does not serve as the same kind of .lter on write traf.c that it does for reads. That said, write buffering (as it is sometimes called) certainly has a number of performance bene.ts. First, by delaying writes, the .le system can batch some updates into a smaller set of I/Os; for example, if an inode bitmap is updated when one .le is created and then updated moments later as another .le is created, the .le system saves an I/O by delaying the write after the .rst update. Second, by buffering a number of writes in memory, the system can then schedule the subsequent I/Os and thus increase performance. Fi­nally, some writes are avoided altogether by delaying them; for ex­ample, if an application creates a .le and then deletes it, delaying the writes to re.ect the .le creation to disk avoids them entirely. In this 
OPERATING SYSTEMS ARPACI-DUSSEAU 
case, laziness (in writing blocks to disk) is a virtue. 
For the reasons above, most modern .le systems buffer writes in memory for anywhere between 5 and 30 seconds, which represents another trade-off. If the system crashes before the updates have been propagated to disk, the updates are lost; for this reason, more para­noid applications will force writes to disk explicitly (e.g., by call­ing fsync()). However, by buffering writes, performance can be greatly increased, by batching writes together, schedulingthem, and avoiding some altogether. 

39.8 Summary 
We have seen the basic machinery required in building a .le sys­tem. There needs to be some information about each .le (metadata), usually stored in an inode. Directories are just a speci.c type of .le that store name-to-i-number mappings. And other structuresare needed too, e.g., bitmaps to track which inodes or data blocksare free or allocated. 
The terri.c aspect of .le system design is its freedom; the .lesys­tems we explore in the coming chapters each take advantage of this freedom to optimize some aspect of the .le system. There are also clearly many policy decisions we have left unexplored. For example, when a new .le is created, where should it be placed on disk? This policy and others will also be the subject of future notes. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

References 
[A+07] Nitin Agrawal, William J. Bolosky, John R. Douceur, Jacob R. Lorch AFive-Year Study of File-System Metadata FAST ’07, pages 31–45, February 2007, San Jose, CA 
An excellent recent analysis of how .le systems are actually used. Use the bibliography within to follow the trail of .le-system analysis papers back to the early 1980s. 
[B07] “ZFS: The Last Word in File Systems” Jeff Bonwick and Bill Moore Available: http://opensolaris.org/os/community/zfs/docs/zfs last.pdf 
One of the most recent important .le systems, full of featuresand awesomeness. Weshould have achapter on it, and perhaps soon will. 
[B02] “The FAT File System” Andries Brouwer September, 2002 Available: http://www.win.tue.nl/ aeb/linux/fs/fat/fat.html 
Anice clean description of FAT. The .le system kind, not the bacon kind. Though you have to admit, bacon fat probably tastes better. 
[C94] “Inside the Windows NT File System” 
Helen Custer 
Microsoft Press, 1994 

Ashort book about NTFS; there are probably ones with more technical details elsewhere. 
[H+88] “Scale and Performance in a Distributed File System” John H. Howard, Michael L. Kazar, Sherri G. Menees, David A. Nichols, M. Satya­narayanan, Robert N. Sidebotham, Michael J. West. ACM Transactions on Computing Systems (ACM TOCS), page 51-81, Volume 6, Number 1, February 1988 
Aclassic distributed .le system; we’ll be learning more about it later, don’t worry. 
[P09] “The Second Extended File System: Internal Layout” Dave Poirier, 2009 Available: http://www.nongnu.org/ext2-doc/ext2.html 
Some details on ext2, a very simple Linux .le system based on FFS, the Berkeley Fast File System. We’ll be reading about it in the next chapter. 
[RT74] “The UNIX Time-Sharing System” 
M. Ritchie and K. Thompson CACM, Volume 17:7, pages 365-375, 1974 The original paper about UNIX.Read it to see the underpinnings of much of modern operating systems. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

[S00] “UBC: An Ef.cient Uni.ed I/O and Memory Caching Subsystem for NetBSD” Chuck Silvers FREENIX, 2000 
Anice paper about NetBSD’s integration of .le-system buffercaching and thevirtual-memory page cache. Many other systems do the same type of thing. 
[S+96] “Scalability in the XFS File System” 
Adan Sweeney, Doug Doucette, Wei Hu, Curtis Anderson, 
Mike Nishimoto, Geoff Peck 
USENIX ’96, January 1996, San Diego, CA 

The .rst attempt to make scalability of operations, including things like having millions of .les in a directory, a central focus. A great example of pushing an idea to the extreme. The key idea behind this .le system: everything is a tree. We should have a chapter on this .le system too. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

Homework 
Use this tool, vsfs.py,to study how .le system state changes as various operations take place. The .le system begins in an empty state, with just a root directory. As the simulation takes place, various operations are performed, thus slowly changing the on-disk state of the .le system. 
The possible operations are: 
• 
mkdir() -creates a new directory 

• 
creat() -creates a new (empty) .le 

• open(), write(), close() -appends a block to a .le 

• 
link() -creates a hard link to a .le 

• 
unlink() -unlinks a .le (removing it if linkcnt==0) 


To understand how this homework functions, you must .rst un­derstand how the on-disk state of this .le system is represented. The state of the .le system is shown by printing the contents of four dif­ferent data structures: 
inode bitmap -indicates which inodes are allocated inodes -table of inodes and their contents data bitmap -indicates which data blocks are allocated data -indicates contents of data blocks 
The bitmaps should be fairly straightforward to understand,with 
a“1” indicating that the corresponding inode or datablock isallo­
cated, and a “0” indicating said inode or data block is free. 
The inodes each have three .elds: the .rst .eld indicates the type of .le (e.g., f for a regular .le, d for a directory); the second indi­cates which data block belongs to a .le (here, .les can only be empty, which would have the address of the data block set to -1, or one block in size, which would have a non-negative address); the third shows the reference count for the .le or directory. For example, thefollow­ing inode is a regular .le, which is empty (address .eld set to -1), and has just one link in the .le system: 
[f a:-1 r:1] 
OPERATING SYSTEMS ARPACI-DUSSEAU 
If the same .le had a block allocated to it (say block 10), it would be shown as follows: 
[f a:10 r:1] 
If someone then created a hard link to this inode, it would then become: 
[f a:10 r:2] 
Finally, data blocks can either retain user data or directorydata. If .lled with directory data, each entry within the block is of the form (name, inumber), where “name” is the name of the .le or directory, and “inumber” is the inode number of the .le. Thus, an empty root directory looks like this, assuming the root inode is 0: 
[(.,0) (..,0)] 
If we add a single .le “f” to the root directory, which has been allocated inode number 1, the root directory contents would then become: 
[(.,0) (..,0) (f,1)] 
If a data block contains user data, it is shown as just a single char­acter within the block, e.g., [h]. If it is empty and unallocated, just a pair of empty brackets ([]) are shown. 
An entire .le system is thus depicted as follows: 
inode bitmap 11110000
inodes [d a:0 r:6] [f a:1 r:1] [f a:-1 r:1] [d a:2 r:2] [] [] [] [] 
data bitmap 11100000 
data [(.,0) (..,0) (y,1) (z,2) (f,3)] [u] [(.,3) (..,0)] [] [][] [] [] 

This .le system has eight inodes and eight data blocks. The root directory contains three entries (other than “.” and “..”), to “y”, “z”, and “f”. By looking up inode 1, we can see that “y” is a regular .le (type f), with a single data block allocated to it (address 1).In that data block 1 are the contents of the .le “y”: namely, “u”. We can also see that “z” is an empty regular .le (address .eld set to -1), and that “f” (inode number 3) is a directory, also empty. You can also see from the bitmaps that the .rst four inode bitmap entries are marked as allocated, as well as the .rst three data bitmap entries. 
The simulator can be run with the following .ags: 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
prompt> vsfs.py -h
Usage: vsfs.py [options] 

Options:-h, --help show this help message and exit -s SEED, --seed=SEED the random seed -i NUMINODES, --numInodes=NUMINODES 
number of inodes in file system-d NUMDATA, --numData=NUMDATA number of data blocks in file system -n NUMREQUESTS, --numRequests=NUMREQUESTS
number of requests to simulate -r, --reverse instead of printing state, print ops -p, --printFinal print the final set of files/dirs -c, --compute compute answers for me 
Atypical usage would simply specify a random seed (to generate adifferent problem), and the number of requests to simulate.In this default mode, the simulator prints out the state of the .le system at each step, and asks you which operation must have taken place to take the .le system from one state to another. For example: 
prompt> vsfs.py -n 6 -s 16 
... 
Initial state 

inode bitmap 10000000 
inodes [d a:0 r:2] [] [] [] [] [] [] [] 
data bitmap 10000000 
data [(.,0) (..,0)] [] [] [] [] [] [] [] 

Which operation took place? 

inode bitmap 11000000 
inodes [d a:0 r:3] [f a:-1 r:1] [] [] [] [] [] [] 
data bitmap 10000000 
data [(.,0) (..,0) (y,1)] [] [] [] [] [] [] [] 

Which operation took place? 

inode bitmap 11000000 
inodes [d a:0 r:3] [f a:1 r:1] [] [] [] [] [] [] 
data bitmap 11000000 
data [(.,0) (..,0) (y,1)] [u] [] [] [] [] [] [] 

Which operation took place? 

inode bitmap 11000000 

OPERATING SYSTEMS ARPACI-DUSSEAU 
inodes [d a:0 r:4] [f a:1 r:2] [] [] [] [] [] [] 
data bitmap 11000000 
data [(.,0) (..,0) (y,1) (m,1)] [u] [] [] [] [] [] [] 

Which operation took place? 

inode bitmap 11000000 
inodes [d a:0 r:4] [f a:1 r:1] [] [] [] [] [] [] 
data bitmap 11000000 
data [(.,0) (..,0) (y,1)] [u] [] [] [] [] [] [] 

Which operation took place? 

inode bitmap 11100000 
inodes [d a:0 r:5] [f a:1 r:1] [f a:-1 r:1] [] [] [] [] [] 
data bitmap 11000000 
data [(.,0) (..,0) (y,1) (z,2)] [u] [] [] [] [] [] [] 

Which operation took place? 

inode bitmap 11110000 
inodes [d a:0 r:6] [f a:1 r:1] [f a:-1 r:1] [d a:2 r:2] [] [] [] [] 
data bitmap 11100000 
data [(.,0) (..,0) (y,1) (z,2) (f,3)] [u] [(.,3) (..,0)] [] [][] [] [] 

When run in this mode, the simulator just shows a series of states, and asks what operations caused these transitions to occur. Running with the -c .ag shows us the answers. Speci.cally, .le “/y” was created, a single block appended to it, a hard link from “/m” to“/y” created, “/m” removed via a call to unlink, the .le “/z” created, and the directory “/f” created: 
prompt> vsfs.py -n 6 -s 16 -c 
... 
Initial state 

inode bitmap 10000000 
inodes [d a:0 r:2] [] [] [] [] [] [] [] 
data bitmap 10000000 
data [(.,0) (..,0)] [] [] [] [] [] [] [] 

creat("/y"); 

inode bitmap 11000000 
inodes [d a:0 r:3] [f a:-1 r:1] [] [] [] [] [] [] 
data bitmap 10000000 
data [(.,0) (..,0) (y,1)] [] [] [] [] [] [] [] 

THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
fd=open("/y", O_WRONLY|O_APPEND); write(fd, buf, BLOCKSIZE); close(fd); 

inode bitmap 11000000 
inodes [d a:0 r:3] [f a:1 r:1] [] [] [] [] [] [] 
data bitmap 11000000 
data [(.,0) (..,0) (y,1)] [u] [] [] [] [] [] [] 

link("/y", "/m"); 

inode bitmap 11000000 
inodes [d a:0 r:4] [f a:1 r:2] [] [] [] [] [] [] 
data bitmap 11000000 
data [(.,0) (..,0) (y,1) (m,1)] [u] [] [] [] [] [] [] 

unlink("/m") 

inode bitmap 11000000 
inodes [d a:0 r:4] [f a:1 r:1] [] [] [] [] [] [] 
data bitmap 11000000 
data [(.,0) (..,0) (y,1)] [u] [] [] [] [] [] [] 

creat("/z"); 

inode bitmap 11100000 
inodes [d a:0 r:5] [f a:1 r:1] [f a:-1 r:1] [] [] [] [] [] 
data bitmap 11000000 
data [(.,0) (..,0) (y,1) (z,2)] [u] [] [] [] [] [] [] 

mkdir("/f"); 

inode bitmap 11110000 
inodes [d a:0 r:6] [f a:1 r:1] [f a:-1 r:1] [d a:2 r:2] [] [] [] [] 
data bitmap 11100000 
data [(.,0) (..,0) (y,1) (z,2) (f,3)] [u] [(.,3) (..,0)] [] [][] [][ 

You can also run the simulator in “reverse” mode (with the “-r” .ag), printing the operations instead of the states to see if you can predict the state changes from the given operations: 
prompt> ./vsfs.py -n 6 -s 16 -r 
Initial state 

inode bitmap 10000000 
inodes [d a:0 r:2] [] [] [] [] [] [] [] 
data bitmap 10000000 
data [(.,0) (..,0)] [] [] [] [] [] [] [] 

creat("/y"); 
OPERATING SYSTEMS ARPACI-DUSSEAU 
State of file system (inode bitmap, inodes, data bitmap, data)? fd=open("/y", O_WRONLY|O_APPEND); write(fd, buf, BLOCKSIZE); close(fd); 
State of file system (inode bitmap, inodes, data bitmap, data)? link("/y", "/m"); 
State of file system (inode bitmap, inodes, data bitmap, data)? unlink("/m") 
State of file system (inode bitmap, inodes, data bitmap, data)? creat("/z"); 
State of file system (inode bitmap, inodes, data bitmap, data)? mkdir("/f"); 
State of file system (inode bitmap, inodes, data bitmap, data)? 
Afew other .ags control various aspects of the simulation, in­cluding the number of inodes (“-i”), the number of data blocks(“­d”), and whether to print the .nal list of all directories and .les in the .le system (“-p”). 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
Questions 
1. 
Run the simulator with some different random seeds (say 17, 18, 19, 20), and see if you can .gure out which operations must have taken place between each state change. 

2. 
Now do the same, using different random seeds (say 21, 22, 23, 24), except run with the -r .ag, thus making you guess the state change while being shown the operation. What can you conclude about the inode and data-block allocation algorithms, in terms of which blocks they prefer to allocate? 

3. 
Now reduce the number of data blocks in the .le system, to very low numbers (say two), and run the simulator for a hun­dred or so requests. What types of .les end up in the .le sys­tem in this highly-constrained layout? What types of opera­tions would fail? 

4. 
Now do the same, but with inodes. With very few inodes, what types of operations can succeed? Which will usually fail? What is the .nal state of the .le system likely to be? 


OPERATING SYSTEMS ARPACI-DUSSEAU 
40 




Locality and The Fast File System 
When the UNIX operating system was .rst introduced, the UNIX wizard himself Ken Thompson wrote the .rst .le system. We will call that the “old UNIX .le system”, and it was really simple. Basi­cally, its data structures looked like this on the disk: 
Super block | Inodes | Data blocks 
The super block contained information about the entire .le sys­tem: how big the volume is, how many inodes there are, a pointerto the head of a free list of blocks, and so forth. The inode regionof the disk contained all the inodes for the .le system. Finally, most of the disk was taken up by data blocks. 
The good thing about the old .le system was that it was simple, and supported the basic abstractions the .le system was trying to de­liver: .les and the directory hierarchy. This easy-to-use system was areal step forward from the clumsy, record-based storage systems of the past, and the directory hierarchy a true advance over simpler, one-level hierarchies provided by earlier systems. 
40.1 The Problem: Poor Performance 
The problem: performance was terrible. As measured by Kirk 
McKusick and his colleagues at Berkeley [MJLF84], performance started 
off bad and got worse over time, to the point where the .le system 
was delivering only 2% of overall disk bandwidth! 
The main issue was that the old UNIX .le system treated the disk like it was a random-access memory; data was spread all over the 
531 place without regard to the fact that the medium holding the data was a disk, and thus had real and expensive positioning costs.For example, the data blocks of a .le were often very far away from its inode, thus inducing an expensive seek whenever one .rst readthe inode and then the data blocks of a .le (a pretty common operation). 
Worse, the .le system would end up getting quite fragmented,as the free space was not carefully managed. The free list would end up pointing to a bunch of blocks spread across the disk, and as .les got allocated, they would simply take the next free block. The result was that a logically contiguous .le would be accessed by going back and forth across the disk, thus reducing performance dramatically. 
For example, imagine the following data block region, which con­tains four .les (A, B, C, and D), each of size 2 blocks: 
A1 A2B1 B2 C1 C2 D1 D2 
Now, if B and D are deleted, we will have: 
A1 A2 free free C1 C2 free free 
As you can see, the free space is fragmented into two chunks of 
two blocks, instead of one nice contiguous chunk of four. Let’s say 
we now wish to allocate a .le E, of size four blocks: 
A1 A2E1 E2 C1 C2 E3 E4 
You can see what happens: E gets spread across the disk, and as aresult, when accessing E, you don’tget peak (sequential) perfor­mance from the disk. Rather, you .rst read E1 and E2, then seek, then read E3 and E4. This fragmentation problem happened all the time in the old UNIX .le system, and it hurt performance. (A side note: this problem is exactly what disk defragmentation tools help with; they will reorganize on-disk data to place .les contiguously and make free space one or a few contiguous regions, moving data around and then rewriting inodes and such to re.ect the changes) 
One other problem: the original block size was too small (512 bytes). Thus, transferring data from the disk was inherentlyinef.­cient. Smaller blocks were good because they minimized internal fragmentation (waste within the block), but bad for transfer as each block might require a positioning overhead to reach it. We cansum­marize the problem as follows: 
OPERATING SYSTEMS ARPACI-DUSSEAU 
THE CRUX: HOW TO ORGANIZE ON-DISK DATA TO IMPROVE PERFORMANCE How can we organize .le system data structures so as to improve performance? What types of allocation policies do we need on top of those data structures? Put simply: How do we make the .le system “disk aware”?  
40.2  FFS: Disk Awareness Is The Solution  
Agroup at Berkeley decided to build a better, faster .le system, which they cleverly called the Fast File System (FFS).The idea was to design the .le system structures and allocation policies to be “disk aware” and thus improve performance, which is exactly what they did. FFS thus ushered in a new era of .le system research; by keep­ing the same interface to the .le system (the same APIs, including open(), read(), write(), close(),andother .le system calls) but changing the internal implementation,the authors paved the path for new .le system construction, work that continues today. Virtu­ally all modern .le systems adhere to the existing interface (and thus preserve compatibility with applications) while changing their inter­nals for performance, reliability, or other reasons.  
40.3  Organizing Structure: The Cylinder Group  
The .rst step was to change the on-disk structures. FFS divides the disk into a bunch of groups known as cylinder groups (some modern .le systems like Linux ext2 and ext3 just call them block groups). We can thus imagine a disk with eight cylinder groups as follows:  
Group0 | Group1 | Group2 | Group3 | Group4 | Group5 | Group6 | Group7  
These groups are the central mechanism that FFS uses to improve performance; by placing two .les within the same group, FFS can ensure that accessing one after the other will not result in long seeks across the disk. Thus, FFS needs to have the ability to allocate .les and directories within each of these groups.  
ARPACI-DUSSEAU  THREE EASY PIECES (V0.5)  

Each group looks like this: 
Super Block | Inode Bitmap | Data Bitmap | Inodes | Data blocks 
We now describe the components of a cylinder group. A copy of the super block is found in each group for reliability reasons (e.g., if one gets corrupted or scratched, you can still mount and access the .le system by using one of the others). 
The inode bitmap and data bitmap track whether each inode or data block is free, respectively. Bitmaps are an excellent way to man­age free space in a .le system because it is easy to .nd a large chunk of free space and allocate it to a .le, perhaps avoiding some ofthe fragmentation problems of the free list in the old UNIX .le system. 
Finally, the inode and data block regions are just like in the previ­
ous .le system. Most of each cylinder group, thus, is comprised of 
data blocks. 
ASIDE:FFS FILE CREATION 
As an example, think about what data structures must be updated 
when a .le is created; assume, for this example, that the user creates 
anew .le /foo/bar.txt and that the .le is one block long (4KB). 
The .le is new, and thus needs a new inode; thus, both the inode 
bitmap and the newly-allocated inode will be written to disk.The .le 
also has data in it and thus it too must be allocated; the data bitmap 
and a data block will thus (eventually) be written to disk. Hence, at 
least four writes to the current cylinder group will take place (recall 
that these writes may be buffered in memory for a while before the 
write takes place). But this is not all! In particular, when creating 
anew .le, we must also place the .le in the .le-system hierarchy; 
thus, the directory must be updated. Speci.cally, the parentdirec­
tory foo must be updated to add the entry for bar.txt;this update 
may .t in an existing data block of foo or require a new block to be 
allocated (with associated data bitmap). The inode of foo must also 
be updated, both to re.ect the new length of the directory as well 
as to update time .elds (such as last-modi.ed-time). Overall, it is a 
lot of work just to create a new .le! Perhaps next time you do so, 
you should be more thankful, or at least surprised that it all works 
so well. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

40.4 Policies: How To Allocate Files and Directories 
With this group structure in place, FFS now has to decide how to place .les and directories and associated metadata on diskto im­prove performance. The basic mantra is simple: keep related stuff to­gether (and its corollary, keep unrelated stuff far apart). 
Thus, to obey the mantra, FFS has to decide what is “related” and place it within the same block group; conversely, unrelated items should be placed into different block groups. To achieve thisend, FFS makes use of a few simple placement heuristics. 
The .rst is the placement of directories. FFS employs a simple approach: .nd the cylinder group with a low number of allocated directories (because we want to balance directories across groups) and a high number of free inodes (because we want to subsequently be able to allocate a bunch of .les), and put the directory dataand inode in that group. Of course, other heuristics could be usedhere (e.g., taking into account the number of free data blocks). 
For .les, FFS does two things. First, it makes sure (in the general 
case) to allocate the data blocks of a .le in the same group as its in-
ode, thus preventing long seeks between inode and data (as in the 
old .le system). Second, it places all .les that are in the samedirec­
tory in the cylinder group of the directory they are in. Thus, if a user 
creates four .les, /dir1/1.txt, /dir1/2.txt, /dir1/3.txt,and 
/dir99/4.txt,FFS would try to place the .rst three near one an­
other (same group) and the fourth far away (in some other group). 
It should be noted that these heuristics are not based on extensive studies of .le-system traf.c or anything particularly nuanced; rather, they are based on good old-fashioned common sense (isn’t thatwhat CS stands for after all?). Files in a directory are often accessed to­gether (imagine compiling a bunch of .les and then linking them into a single executable). Because they are, FFS will often improve performance, making sure that seeks between related .les areshort. 

40.5 The Large-File Exception 
There is one important exception to the general policy of .le place­ment, and it happens for large .les. Without a different rule,this large .le would entirely .ll its .rst block group (and maybe others). Filling a block group in this manner is undesirable for the follow-
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
OPERATING SYSTEMS 
ing reason: it prevents subsequent “related” .les from beingplaced within this block group, and thus may hurt subsequent .le-access locality. 
Thus, for large .les, FFS does the following. After some number of blocks are allocated into the .rst block group (e.g., 12 blocks), FFS places the next “large” chunk of the .le in another block group(per­haps chosen for its low utilization). Then, the next chunk of the .le is placed in yet another different block group, and so on. Thus, in­stead of a single large .le (with chunks 0 through 7) .lling up ablock group like this: 
Group0 | Group1 | Group2 | Group3 | Group4 | Group5 | Group6 | Group70123457 
Instead we get a .le F spread across the disk in chunks: 
Group0 | Group1 | Group2 | Group3 | Group4 | Group5 | Group6 | Group767 01 23 45 
The astute reader will note that this might hurt performance,par­ticularly in the relatively common case of sequential .le access (e.g., when a user or application reads blocks 0 through 7 in order). And you are right! It will. But we can help this a little bit, by choosing our chunk size carefully. 
Speci.cally, if the chunk size is large enough, we will still spend most of our time transferring data from disk and just a relatively little time seeking between chunks of the block. This process of reducing an overhead by doing more work per overhead paid is called amor­tization and is a common technique in computer systems. 
Let’s do an example: assume that the average seek time for a disk is 10 ms. Assume further that the disk transfers data at 40 MB/s. If our goal was to spend half our time seeking between chunks and half our time transferring data (and thus achieve 50% of peak disk performance), we would thus need to spend 10 ms transferring data for every 10 ms seek. So the question becomes: how big does a chunk have to be in order to spend 10 ms in transfer? Easy, just use ourold Basically, what this equation says is this: if you transfer data at 40 
friend, math:  
40 MB s  ·  1 s 1000 ms  · 10 ms =409 KB  (40.1)  
ARPACI-DUSSEAU  

MB/s, you need to transfer only 409 KB1 every time you seek in or­
der to spend half your time seeking and half your time transferring. 
Similarly, you can compute the size of the chunk you would needto 
achieve 90% of peak bandwidth (turns out it is about 3.69 MB), or 
even 99% of peak bandwidth (40.6 MB!). As you can see, the closer 
you want to get to peak, the bigger these chunks get. Much bigger in 
fact! 
FFS did not use this type of calculation in order to spread large 
.les across groups, however. Instead, it took a simple approach, 
based on the structure of the inode itself. The .rst twelve direct 
blocks were placed in the same group as the inode; each subsequent 
indirect block, and all the blocks it pointed to, was placed ina differ­
ent group. With a block size of 4-KB, and 32-bit disk addresses, this 
strategy implies that every 1024 blocks of the .le (4 MB) were placed 
in separate groups, the lone exception being the .rst 48-KB ofthe .le 
as pointed to by direct pointers. 
We should note that the trend in disk drives is that transfer rate 
improves fairly rapidly, as disk manufacturers are good at cramming 
more bits into the same surface, but the mechanical aspects ofdrives 
related to seeks (disk arm speed and the rate of rotation) improve 
rather slowly [P98]. The implication is that over time, mechanical 
costs become relatively more expensive, and thus, to amortize said 
costs, you have to transfer more data between seeks. 

40.6 A Few Other Things About FFS 
FFS introduced a couple of other small innovations too. In par­
ticular, the designers were also extremely worried about accommo­
dating small .les; as it turned out, many .les were 2 KB or so in size 
back then, and using 4-KB blocks, while good for transferringdata, 
was not so good for space ef.ciency. This internal fragmentation 
could thus lead to roughly half the disk being wasted for a typical 
.le system. 
The solution the FFS designers hit upon was simple and solved 
the problem. They decided to introduce sub-blocks,which were 512­
1
Remember that dividing a MB by 1000 does not get you a KB exactly, which is why you get 409 KB and not 400 KB. 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
byte little blocks that the .le system could allocate to .les.Thus, if you created a small .le (say 1 KB in size), it would occupy two sub-blocks and thus not waste an entire 4-KB block. As the .le grew, the .le system will continue allocating 512-byte blocks to ituntil it acquires a full 4-KB of data. At that point, FFS will .nd a 4-KB block, copy the sub-blocks into it, and free the sub-blocks for future use. 
You might note that this is inef.cient, requiring a lot of extra work for the .le system (in particular, a lot of extra I/O to performthe copy). And you’d be right again! Thus, FFS generally avoided this pessimal behavior by modifying the libc library; the library would buffer writes and then issue them in 4-KB chunks to the .le system, thus avoiding the sub-block specialization entirely in mostcases. 
Asecond neat thing that FFS introduced was a disk layout that was optimized for performance. In those times (before SCSI and other more modern device interfaces), disks were much less sophisti­cated and required the host CPU to control their operation in amore hands-on way. A problem arose in FFS when a .le was placed on consecutive sectors of the disk, as on the left in Figure 40.1. 

Figure 40.1: FFS: Standard vs. Parameterized Placement 
In particular, the problem arose during sequential reads. FFS would 
.rst issue a read to block 0; by the time the read was complete, and 
FFS issued a read to block 1, it was too late: block 1 had rotatedunder 
the head and now the read to block 1 would incur a full rotation. 
FFS solved this problem with a different layout, as you can see on the right in Figure 40.1. By skipping over every other block(in the example), FFS has enough time to request the next block before it went past the disk head. In fact, FFS was smart enough to .gure out for a particular disk how many blocks it should skip in doing lay­out in order to avoid the extra rotations; this technique was called parameterization,as FFS would .gure out the speci.c performance 
OPERATING SYSTEMS ARPACI-DUSSEAU 
parameters of the disk and use those to decide on the exact staggered layout scheme. 
You might be thinking: this scheme isn’t so great after all. Infact, you will only get 50% of peak bandwidth with this type of layout, be­cause you have to go around each track twice just to read each block once. Fortunately, modern disks are much smarter: they internally read the entire track in and buffer it in an internal disk cache(of­ten called a track buffer for this very reason). Then, on subsequent reads to the track, the disk will just return the desired data from its cache. File systems thus no longer have to worry about these incred­ibly low-level details. Abstraction and higher-level interfaces can be agood thing, when designed properly. 
Some other usability improvements were added as well. FFS was one of the .rst .le systems to allow for long .le names,thus enabling more expressive names in the .le system instead of a the traditional .xed-size approach (e.g., 8 characters). Further, a new concept was introduced called a symbolic link.As discussed in a previous chap­ter, hard links are limited in that they both could not point todirec­tories (for fear of introducing loops in the .le system hierarchy) and that they can only point to .les within the same volume (i.e., the in-ode number must still be meaningful). Symbolic links allow the user to create an “alias” to any other .le or directory on a system and thus are much more .exible. FFS also introduced an atomic rename() operation for renaming .les. Usability improvements, beyond the basic technology, also likely gained FFS a strong user base inthe com­munity. 
DESIGN TIP:MAKE THE SYSTEM USABLE 
Probably the most basic lesson from FFS is that not only did it in­troduce the conceptually good idea of disk-aware layout, butit also added a number of features that simply made the system more us­able. Long .le names, symbolic links, and a rename operation that worked atomically all improved the utility of a system; whilehard to write a paper about (imagine trying to read a 14-pager about“The Symbolic Link: Hard Link’s Long Lost Cousin”), such small features made FFS more useful and thus likely increased its chances foradop­tion. Making a system usable is often as or more important thanits deep technical innovations. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

40.7 Summary 
The introduction of FFS was a watershed moment in .le system history, as it made clear that the problem of .le management was one of the most interesting issues within an operating system, and showed how one might begin to deal with that most important of devices, the hard disk. Since that time, hundreds of new .le systems have developed, but still today many .le systems take cues from FFS (e.g., Linux ext2 and ext3 are obvious intellectual descendants). Cer­tainly all modern systems account for the main lesson of FFS: treat the disk like it’s a disk. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

References 
[MJLF84] “A Fast File System for UNIX” 
Marshall K. McKusick, William N. Joy, Sam J. Lef.er, Robert S.Fabry 
ACM Transactions on Computing Systems. 
August, 1984. Volume 2, Number 3. 
pages 181-197. 

McKusick was recently honored with the IEEE Reynold B. Johnson award for his contributions to .le systems, much of which was based on his work building FFS. In his acceptance speech, he discussed the original FFS software: only 1200 lines of code!Modern versions are a little more complex, e.g., the BSD FFS descendant now is in the 50-thousand lines-of-code range. 
[P98] “Hardware Technology Trends and Database Opportunities” 
David A. Patterson 
Keynote Lecture at the ACM SIGMOD Conference (SIGMOD ’98) 
June, 1998 

Agreat and simple overviewof disk technology trends and how they change over time. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
41 



Crash Consistency: FSCK and Journaling 
As we’ve seen thus far, the .le system manages a set of data struc­tures to implement the expected abstractions: .les, directories, and all of the other metadata needed to support the basic abstraction that we expect from a .le system. Unlike most data structures (for exam­ple, those found in memory of a running program), .le system data structures must persist,i.e., they must survive over the long haul, stored on devices that retain data despite power loss (such ashard disks or .ash-based SSDs). 
One major challenge faced by a .le system is how to update per­sistent data structures despite the presence of a power loss or system crash.Speci.cally, what happens if, right in the middle of updating on-disk structures, someone trips over the power cord and thema­chine loses power? Or the OS encounters a bug and crashes? Because of power losses and crashes, updating a persistent data structure can be quite tricky, and leads to a new problem in .le system implemen­tation, known as the crash-consistency problem. 
The problem is quite simple. Imagine you have to update two on-disk structures, A and B,in order to complete a particular operation. Because the disk only services a single request at a time, one of these requests will reach the disk .rst (either A or B). If the system crashes or loses power after one write completes, the on-disk structure will be left in an inconsistent state. And thus, we have a problem that all .le systems need to solve: 
543 
THE CRUX:HOW TO UPDATE THE DISK DESPITE CRASHES 
The system may crash or lose power between any two writes, and thus the on-disk state may only partially get updated. After the crash, the system boots and wishes to mount the .le system again (in order to access .les and such). Given that crashes can occur at arbitrary points in time, how do we ensure the .le system keepsthe on-disk image in a reasonable state? 
In this chapter, we’ll describe this problem in more detail, and look at some methods .le systems have used to overcome it. We’ll begin by examining the approach taken by older .le systems, known as fsck or the .le system checker.We’ll then turn our attention to another approach, known as journaling (also known by database people as write-ahead logging), a technique which adds a little bit of overhead to each write but recovers more quickly from crashes or power losses. We will discuss the basic machinery of journaling, in­cluding a few different .avors of journaling that Linux ext3 [T98] (a modern journaling .le system) implements. 
41.1 A Detailed Example 
To start things off, let’s look at an example. Let’s say we are trying to append a block to an existing .le. For simplicity, let’s assume we are using a simpli.ed version Linux ext2 [T98], which is an intellec­tual descendent of the FFS .le system [MJLF84]. 
Before we do this write, the .le is on disk in the form of an inode, one (or more) existing data blocks, and some bitmaps that markthe inode and data blocks as in-use. This might look something like this (on a tiny .le system): 
i-node | data | inodes | data blocks bitmap | bitmap | |010000 | 000010| --Iv1 --------| --------D1 -­
Inside the .rst version of the inode (Iv1), we see: 
owner : remzi 
permissions : read-only
size :1 

OPERATING SYSTEMS ARPACI-DUSSEAU 
pointerpointerpointerpointer  : 4 : null : null : null  
In this simpli.ed inode, the size of the .le is 1 (it has one block allocated), the .rst direct pointer points to block 4 (the .rst data block of the .le, D1), and all three other direct pointers are set tonull (indicating that they are not used). Of course, real inodes have many more .elds; see previous chapters for more information. Inside the data bitmap (B1), we have a bit indicating that data block 4 is in use. And .nally, of course, we see that disk block 4 holds the contents of the .rst block of the .le (D1). When we append to the .le, we are adding a new data block to it, and thus must update three on-disk structures: the inode (which now must contain a pointer to the new block as well as an updated size count to re.ect the new size of the .le), the new data blockD2, and a new version of the data bitmap to indicate that the new data block has been allocated. Thus, in the memory of the system, we have three blocks which we must write to disk. The updated inode (inode version 2, or Iv2 for short) now looks like this:  
owner : remzi permissions : read-onlysize : 2 pointer : 4 pointer : 5 pointer : null pointer : null  
The updated data bitmap (B2) now looks like this:  
00 0 0 11  
Finally, there is the data block (D2), which is just .lled withwhatever it is users put into .les. Stolen music perhaps? What we would like is for the .nal on-disk image of the .le sys­tem to look like this:  
i-node | data | inodes | data blocks bitmap | bitmap | |010000 | 000011 | -­Iv2 -­-­-­-­| -­-­-­-­D1 D2  
ARPACI-DUSSEAU  THREE EASY PIECES (V0.5)  

To achieve this transition, the .le system must perform threesep­arate writes to the disk, one each for the inode (Iv2), bitmap (B2), and data block (D2). Note that these writes usually don’t happen imme­diately when the user issues a write() system call; rather, the dirty inode, bitmap, and new data will sit in main memory (in the page cache or buffer cache)for some time .rst; then, when the .le sys­tem .nally decides to write them to disk (after say 5 seconds or30 seconds), the .le system will issue the requisite write requests to the disk. Unfortunately, a crash may occur and thus interfere with these updates to the disk. In particular, if a crash happens after one or two of these writes have taken place, but not all three, the .lesystem could be left in a funny state. 
Crash Scenarios 
To understand the problem better, let’s look at some example crash scenarios. Imagine only a single write succeeds; there are thus three possible outcomes, which we list here: 
• 	
Just the data block (D2) is written to disk. In this case, the data is on disk, but there is no inode that points to it and no bitmap that even says the block is allocated. Thus, it is as if the write never occurred. This case is not a problem at all, from the 

perspective of .le-system crash consistency1. 

• 	
Just the updated inode (Iv2) is written to disk. In this case, the inode points to the disk address (5) where D2 was about to be written, but D2 has not yet been written there. Thus, if we trust that pointer, we will read garbage data from the disk (the old contents of disk address 5). 


Further, we have a new problem, which we call a .le-system inconsistency.The on-disk bitmap is telling us that data block 5has notbeen allocated, butthe inode is saying that it has. This disagreement in the .le system data structures is an inconsis­tency in the data structures of the .le system; to use the .le system, we must somehow resolve this problem (more on that below). 
1
However, it might be a problem for the user, who just lost some data! 
OPERATING SYSTEMS ARPACI-DUSSEAU 
• Just the updated bitmap (B2) is written to disk. In this case, the bitmap indicates that block 5 is allocated, but there is noin­ode that points to it. Thus the .le system is inconsistent again; if left unresolved, this write would result in a space leak,as block 5 would never be used by the .le system.  
There are also three more crash scenarios in this attempt to write three blocks to disk. In these cases, two writes succeed and the last one fails:  
• The inode (Iv2) and bitmap (B2) are written to disk, but not data (D2). In this case, the .le system metadata is completely consistent: the inode has a pointer to block 5, the bitmap in­dicates that 5 is in use, and thus everything looks OK from the perspective of the .le system’s metadata. But there is one prob­lem: 5 has garbage in it again.  
• The inode (Iv2) and the data block (D2) are written, but not the bitmap (B2). In this case, we have the inode pointing to the correct data on disk, but again have an inconsistency between the inode and the old version of the bitmap (B1). Thus, we once again need to resolve the problem before using the .le system.  
• The bitmap (B2) and data block (D2) are written, but not the inode (Iv2). In this case, we again have an inconsistency be­tween the inode and the data bitmap. However, even though the block was written and the bitmap indicates its usage, we have no idea which .le it belongs to, as no inode points to the .le.  
The Crash Consistency Problem  
Hopefully, from these crash scenarios, you can see the many prob­lems that can occur to our on-disk .le system image because of crashes: we can have inconsistency in .le system data structures; we can have space leaks; we can return garbage data to a user; and so forth.What we’d like to do ideally is move the .le system from one consistent state (e.g., before the .le got appended to) to another atomically (e.g., after the inode, bitmap, and new data block have been writ­ten to disk). Unfortunately, we can’t do this easily because the disk  
ARPACI-DUSSEAU  THREE EASY PIECES (V0.5)  

only commits one write at a time, and crashes or power loss may oc­cur between these updates. We call this general problem the crash-consistency problem (we could also call it the consistent-update problem). 

41.2 Solution #1: The File System Checker 
Early .le systems took a simple approach to the .le system up­
date problem. Basically, they decided to let inconsistencies happen 
and then .x them later (when rebooting). A classic example of this 
lazy approach is found in a tool that does this: fsck2. fsck is a UNIX tool for .nding such inconsistencies and repairing them [M86]; sim­ilar tools to check and repair a disk partition exist on different sys­tems. Note that such an approach can’t .x all problems; consider, for example, the case above where the .le system looks consistentbut the inode points to garbage data. The only real goal is to make sure the .le system metadata is internally consistent. 
The tool fsck operates in a number of phases, as summarized in McKusick and Kowalski’s paper [MK96]. It is run before the .le system is mounted and made available (fsck assumes that no other .le-system activity is on-going while it runs); once .nished, the on-disk .le system should be consistent and thus can be made accessible to users. 
Here is a basic summary of what fsck does: 
• 	
Superblock: fsck .rst checks if the superblock looks reason­able, mostly doing sanity checks such as making sure the .le system size is greater than the number of blocks allocated. Usu­ally the goal of these sanity checks is to .nd a suspect (corrupt) superblock; in this case, the system (or administrator) may de­cide to use an alternate copy of the superblock. 

• 	
Free blocks: Next, fsck scans the inodes, indirect blocks, dou­ble indirect blocks, etc., to build an understanding of which blocks are currently allocated within the .le system. It uses this knowledge to produce a correct version of the allocation bitmaps; thus, if there is any inconsistency between bitmaps 


2
Pronounced either “eff-ess-see-kay”, “eff-ess-check”, or, if you don’t like the tool, “eff-suck”. Yes, serious professional people use this term. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
and inodes, it is resolved by trusting the information within 
the inodes. The same type of check is performed for all the in-
odes, making sure that all inodes that look like they are in use 
are marked as such in the inode bitmaps. 

• 	
Inode state: Each inode is checked for corruption or other prob­lems. For example, fsck makes sure that each allocated inode has a valid type .eld (e.g., regular .le, directory, symboliclink, etc.). If there are problems with the inode .elds that are not eas­ily .xed, the inode is considered suspect and cleared by fsck; the inode bitmap is correspondingly updated. 

• 	
Inode links: fsck also veri.es the link count of each allo­cated inode. As you may recall, the link count indicates the number of different directories that contain a reference (i.e., alink) to this particular .le. To verify the link count, fsck scans through the entire directory tree, starting at the rootdi­rectory, and builds its own link counts for every .le and di­rectory in the .le system. If there is a mismatch between the newly-calculated count and that found within an inode, cor­rective action must be taken, usually by .xing the count within the inode. If an allocated inode is discovered but no directory refers to it, it is moved to the lost+found directory. 

• 	
Duplicates: fsck also checks for duplicate pointers, i.e., cases where two different inodes refer to the same block. If one inode is obviously bad, it may be cleared. Alternately, the pointed-to block could be copied, thus giving each inode its own copy as desired. 

• 	
Bad blocks: Acheck for bad block pointers is also performed while scanning through the list of all pointers. A pointer is considered “bad” if it obviously points to something outsideits valid range, e.g., it has an address that refers to a block greater than the partition size. In this case, fsck can’t do anything too intelligent; it just removes (clears) the pointer from the inode or indirect block. 

• 	
Directory checks: fsck does not understand the contents of user .les; however, directories hold speci.cally formattedin­formation created by the .le system itself. Thus, fsck per­forms additional integrity checks on the contents of each direc-


THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
tory, making sure that “.” and “..” are the .rst entries, that each inode referred to in a directory entry is allocated, and ensuring that no directory is linked to more than once in the entire hier­archy. 
As you can see, building a working fsck requires intricate knowl­edge of the .le system; making sure such a piece of code works cor­rectly in all cases can be challenging [G+08]. However, fsck (and sim­ilar approaches) have a bigger and perhaps more fundamental prob­lem: they are too slow.With a very large disk volume, scanning the entire disk to .nd all the allocated blocks and read the entiredirec­tory tree may take many minutes or even hours. Thus, performance of fsck,as disks grew in capacity and RAIDs grew in popularity, became prohibitive. 
Worse, the basic premise of fsck seems irrational. Consider our example above with just a few blocks being written to the disk;what awaste to scan the entire disk justto see if one of those three writes didn’t complete! It is kind of like dropping your keys on the .oor in your bedroom, and then commencing a search-the-entire-house-for­keys recovery algorithm, starting in the basement and working your way through every room. It works, but it certainly seems wasteful. Thus, as disks (and multi-disk RAID systems) grew in size, people started to look for other solutions. 

41.3 Solution #2: Journaling (or Write-Ahead Logging) 
Probably the most popular solution to the consistent update prob­lem is to steal an idea from the world of database management sys­tems. That idea, known as write-ahead logging,was invented to address exactly this type of problem. In .le systems, we usually call write-ahead logging journaling for historical reasons. The .rst .le system to do this was Cedar [H87], though many modern .le sys­tems use the idea, including Linux ext3, reiserfs, IBM’s JFS,and Win­dows NTFS. 
The basic idea is as follows. When updating the disk, before over­writing the structures in place, .rst write down a little note(some­where else on the disk, in a well-known location) describing what you are about to do. Writing this note is the “write ahead” part, and we write it to a structure that we organize as a “log”; hence, write-
OPERATING SYSTEMS ARPACI-DUSSEAU 
ahead logging. By writing the note to disk, you are guaranteeing that if a crash takes places during the update (overwrite) of the structuresyou are updating, you can go back and look at the note you made and try again; thus, you will know exactly what to .x (and how to .x it) after a crash, instead of having to scan the entire disk. By design, journaling thus adds a bit of work during updates to greatly reduce the amount of work required during recovery. We’ll now describe how Linux ext3,a popular journaling .le sys­tem, incorporates journaling into the .le system. Most of theon-disk structures are identical to Linux ext2,e.g., the disk is divided into block groups, and each block group has an inode and data bitmap as well as inodes and data blocks. The new key structure is the jour­nal itself, which occupies some small amount of space within the partition or on another device. Thus, an ext2 .le system (without journaling) looks like this:  
Superblock | Group0 | Group1 | ... | GroupN  
Assuming the journal is placed within the same .le system im­age 3,an ext3 .le system with a journal looks like this:  
Superblock | Journal | Group0 | Group1 | ... | GroupN  
The real difference is just the presence of the journal, and ofcourse, how it is used.  
Data Journaling  
Let’s look at a simple example to understand how data journaling works. Data journaling is available as a mode with the Linux ext3 .le system, from which much of this discussion is based. Say we have our canonical update again, where we wish to write the inode (Iv2), bitmap (B2), and data block (D2) to disk again. Before writing them to their .nal disk locations, we are now .rst going to write them to the log (a.k.a. journal). This is what this will look like in the log:  
TxBegin | Iv2 | B2 | D2 | TxEnd  
3The journal can also be placed on a separate device.  
ARPACI-DUSSEAU  THREE EASY PIECES (V0.5)  

You can see we have written .ve blocks here. The transaction be­gin (TxBegin) tells us about this update, including information about the pending update to the .le system (e.g., the .nal addressesof the blocks Iv2, B2, and D2), as well as some kind of transaction iden­ti.er (TID). The middle three blocks just contain the exact contents of the blocks themselves; this is known as physical logging as we are putting the exact physical contents of the update in the journal (an alternate idea, logical logging,puts a more compact logical rep­resentation of the update in the journal, e.g., “this update wishes to append data block D2 to .le X”, which is a little more complex but can save space in the log and perhaps improve performance). The .nal block is a marker of the end of this transaction, and will also contain the TID. 
Once this transaction is safely on disk, we are ready to overwrite the old structures in the .le system; this process is called checkpoint­ing.Thus, to checkpoint the .le system (i.e., bring it up to date with the pending update in the journal), we issue the writes Iv2, B2, and D2 to their disk locations as seen above; if these writes complete suc­cessfully, we have successfully checkpointed the the .le system and are basically done. Thus, our initial sequence of operations: 
1. 	
Journal write: Write the transaction (containing TxBegin, Iv2, B2, D2, and TxEnd) to the log; wait for these writes to complete 

2. 	
Checkpoint: Write the update (e.g., Iv2, B2, D2) to the .le sys­tem proper 


Things get a little trickier when a crash occurs during the writes to the journal. Here, we are trying to write the set of blocks in the trans­action (TxBegin|Iv2|B2|D2|TxEnd)to disk. One simple way to do this would be to issue each one at a time, waiting for each to complete, and then issuing the next. However, this is slow. Ideally, we’d like to issue all .ve block writes at once, as this would turn .ve writes into a single sequential write and thus be faster. However, this is unsafe, for the following reason: given such a big write, the disk internally may perform scheduling and complete small piecesof the big write in any order. Thus, the disk internally may (1) writeTxBe­gin, Iv2, B2, and TxEnd and only later (2) write D2. Unfortunately, if the disk loses power between (1) and (2), this is what ends upon disk: 
TxBegin (TID=1) | Iv2 | B2 | ??? | TxEnd (TID=1) 
OPERATING SYSTEMS ARPACI-DUSSEAU 
ASIDE:FORCING WRITES TO DISK To enforce ordering between two disk writes, modern .le systems have to take a few extra precautions. In olden times, forcing ordering between two writes, A and B,was easy: just issue the write of A to the disk, wait for the disk to interrupt the OS when the writeis complete, and then issue the write of B. 
Things got slightly more complex due to the increased use of write caches within disks. With write buffering enabled (sometimes called immediate reporting), a disk will inform the OS the write is com­plete when it simply has been placed in the disk’s memory cache, and has not yet reached disk. If the OS then issues a subsequent write, it is not guaranteed to reach the disk after previous writes; thus ordering between writes is not preserved. One solution is to disable write buffering. However, more modern systems take extra precautions and issue explicit write barriers;such a barrier, when it completes, guarantees that all writes issued before the barrier will reach disk before any writes issued after the barrier. 
All of this machinery requires a great deal of trust in the correct operation of the disk. Unfortunately, recent research showsthat some disk manufacturers, in an effort to deliver “higher performing” disks, explicitly ignore write-barrier requests, thus making the disks seemingly run faster but at the risk of incorrect operation [R+11]. As Kahan famously said, the fast almost always beats out the slow, even if the fast is wrong. 
Why is this a problem? Well, the transaction looks like a valid transaction (it has a begin and an end with matching sequence num­bers). Further, the .le system can’t look at that fourth blockand know it is wrong; after all, it is arbitrary user data. Thus, ifthe sys­tem now reboots and runs recovery, it will replay this transaction, and ignorantly copy the contents of the garbage block ’???’ tothe location where D2 is supposed to live. This is bad for arbitrary user data in a .le; it is much worse if it happens to a critical piece of .le system, such as the superblock, which could render the .le system unmountable. 
To avoid this problem, a journaling .le system issues the transac­tional write in two steps. First, it writes all blocks except the TxEnd 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
ASIDE:OPTIMIZING LOG WRITES 
You may have noticed a particular inef.ciency of writing to the log. 
Namely, the .le system .rst has to write out the transaction-begin 
block and contents of the transaction; only after these writes com­
plete can the .le system send the transaction-end block to disk. The 
performance impact is clear, if you think about how a disk works: 
usually an extra rotation is incurred (think about why). 
One of our former graduate students, Vijayan Prabhakaran, had a simple idea to .x this problem [P+05]. When writing a transaction to the journal, include a checksum of the contents of the journal in the begin and end blocks. Doing so enables the .le system to write the entire transaction at once, without incurring a wait; if,during recovery, the .le system sees a mismatch in the computed checksum versus the stored checksum in the transaction, it can conclude that a crash occurred during the write of the transaction and thus discard the .le-system update. Thus, with a small tweak in the write protocol and recovery system, a .le system can achieve faster common-case performance; on top of that, the system is slightly more reliable, as any reads from the journal are now protected by a checksum. 
This simple .x was attractive enough to gain the notice of Linux .le system developers, who then incorporated it into the next generation Linux .le system, called (you guessed it!) Linux ext4.It now ships on millions of machines worldwide, including the Android handheld platform. Thus, every time you write to disk on many Linux-based systems, a little code developed at Wisconsin makes your system a little faster and more reliable. 
block to the journal, issuing these writes all at once. When these writes complete, the journal will look something like this: 
TxBegin (TID=1) | Iv2 | B2 | D2 | 
When those writes complete, the .le system issues the write of the TxEnd block, thus leaving the journal in this .nal, safe state: 
TxBegin (TID=1) | Iv2 | B2 | D2 | TxEnd (TID=1) 
What you really need to understand here is the atomicity guar­antee provided by the disk. It turns out that the disk guarantees 
OPERATING SYSTEMS ARPACI-DUSSEAU 
that any 512-byte write will either happen or not (and never behalf-written); thus, to make sure the write of TxEnd is atomic, one should make it a single 512-byte block. Thus, our current protocol toupdate the .le system, with each of its three phases labeled: 
1. 	
Journal write: Write the contents of the transaction (containing TxBegin, and in our example, Iv2, B2, D2) to the log; wait for writes to complete 

2. 	
Journal commit: Write the transaction commit block (contain­ing TxEnd) to the log; wait for write to complete; transactionis now committed 

3. 	
Checkpoint: Write the contents of the update (e.g., Iv2, B2, D2) to the .le system proper 


Recovery 
Of course, a crash may happen at any time during this sequence of updates. If the crash happens before the transaction is written safely to the log (i.e., before Step 2 above completes), then our job is easy: that pending update is simply skipped. If the crash happens after the transaction has committed to the log, but before thecheck­point is complete, the .le system can recover the update as follows. When the system boots, the .le system recovery process will scan the log and look for transactions that have committed to the disk;these transactions are thus replayed,with the .le system again attempting to write out the blocks in the transaction to their .nal on-disk loca­tions. This form of logging is one of the simplest forms there is, and is called redo logging.By recovering the committed transactions in the journal, the .le system ensures that the on-disk structures are consis­tent, and thus can proceed by mounting the .le system and readying itself for new requests. 
Note that it is thus OK for a crash to happen at any point during checkpointing, even after some of the updates to the .nal locations of the blocks have completed. In the worst case, some of these updates are simply performed again during recovery. Because recovery is a rare operation (only taking place after an unexpected systemcrash), 
afew redundantwrites are nothing to worry about4. 
4
Unless you worry about everything, in which case we can’t helpyou. Stop worry­ing so much, it is unhealthy! But now you’re probably worried about over-worrying. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

Batching Log Updates 
You might have noticed that the basic protocol could add a lot of ex­tra disk traf.c. For example, imagine we create two .les in a row, called file1 and file2,in the same directory. To create one .le, one has to update a number of on-disk structures, minimally includ­ing: the inode bitmap (to allocated a new inode), the newly-created inode of the .le, the data block of the parent directory containing the new directory entry, as well as the parent directory inode (which now has a new modi.cation time). With journaling, we logically commit all of this information to the journal for each of our two .le creations; because the .les are in the same directory, and let’s assume even have inodes within the same inode block, this means that if we’re not care­ful, we’ll end up writing these same blocks over and over. 
To remedy this problem, ext3 does not commit each update to disk one at a time. Rather, ext3 has a single on-going global transac­tion which buffers such updates to the .le system. In our example above, when the two .les are created, the .le system just marksthe in-memory inode bitmap, inodes of the .les, directory data, and di­rectory inode as dirty, and adds them to the list of blocks thatform the current transaction. When it is .nally time to write theseblocks to disk (say, after a timeout of 5 seconds), this single global transaction is committed containing all of the updates described above. Thus, by buffering updates, ext3 avoids excessive write traf.c to disk in most cases. 

Making The Log Finite 
We thus have arrived at a basic protocol for updating .le-system on-disk structures. The .le system buffers updates in memory forsome time; when it is .nally time to write to disk, the .le system .rst care­fully writes out the details of the transaction to the journal(a.k.a. write-ahead log); after the transaction is complete, the .lesystem checkpoints those blocks to their .nal locations on disk. 
However, the log is of a .nite size. If we keep adding transactions 
to it (as in the .gure below), it will soon .ll. And what do you think 
happens then? 
Tx1 |Tx2 | Tx3| Tx4 | Tx5 | ... 
OPERATING SYSTEMS ARPACI-DUSSEAU 
Two problems arise when the log becomes full. The .rst is sim­pler: the larger the log, the longer recovery will take, as therecovery process must replay all the transactions within the log in order to re­cover. The second is more of an issue: when the log is full (or nearly full), no further transactions can be committed to the disk, thus mak­ing the .le system “less than useful” (i.e., useless). To address this problem, journaling .le systems treat the logas a circular data structure, re-using it over and over. To do so, the .le system must take action some time after a checkpoint. Speci.cally, once a transaction has been checkpointed, the .le system should free it, allowing the log space to be reused. There are many ways to achieve this end; for example, you could simply set a counter to the value T in the a journal superblock (located at a .xed location in the log) to indicate that transaction T is the current starting point in the log. Thus, the journal looks something like this:  
Superblock | Tx1 | Tx2 | Tx3 | Tx4 | Tx5 | ...  
In the superblock, the journaling system records enough informa­tion to know which transactions have not yet been checkpointed, and thus reduces recovery time as well as enables re-use of the login a circular fashion. And thus we add another step to our basic protocol:  
1. Journal write: Write the contents of the transaction (containing TxBegin and the contents of the update) to the log; wait for writes to complete 2. Journal commit: Write the transaction commit block (contain­ing TxEnd) to the log; wait for write to complete; transactionis now committed 3. Checkpoint: Write the contents of the update to the .le system proper 4. Free: Some time later, mark the transaction free in the journal  
Thus we have our .nal data journaling protocol. But there is still aproblem: we are writing each data block to the disk twice, which is a large cost to pay. Can you .gure out a way to retain .le-system consistency without writing all data twice?  
ARPACI-DUSSEAU  THREE EASY PIECES (V0.5)  


Metadata Journaling 
Although recovery is now fast (scanning the journal and replaying afew transactions as opposed to scanning the entire disk), normal operation of the .le system is slower. In particular, for eachwrite to disk, we are now also writing to the journal .rst, thus doubling write traf.c. Further, between writes to the journal and writes to the main .le system, there is a costly seek. 
Because of the high cost of writing every data block to disk twice, people have tried a few different things in order to speed up per­formance. For example, the mode of journaling we described above is often called data journaling (as in Linux ext3), as it journals all user data (in addition to the metadata of the .le system). A simpler (and more common) form of journaling is sometimes called ordered journaling (or just metadata journaling), and it is nearly the same, except that user data is not written to the journal. Thus, whenper­forming the same update as above, the following would be written to the journal: 
TxBegin | Iv2 | B2 | TxEnd 
The data block D2, previously written to the log, would instead be written to the .le system proper, thus avoiding the extra write. This modi.cation does raise an interesting question; when should we write D2 to disk? 
What if we write D2 to disk after the transaction (containing Iv2 and B2) completes? Unfortunately, this approach has a problem: it may end up with a consistent .le system but one that has Iv2 point­ing to garbage data. Speci.cally, if the .le system is writingIv2, B2, and D2 to disk and only manages to complete the .rst two writes before crashing, D2 will not be on the disk. The .le system willthen try to recover (but notice that D2 is not in the log). Thus, it will re­play the writes to Iv2 and B2, and produce a consistent .le system (from the perspective of .le-system metadata). However, Iv2will be pointing to garbage data. 
Thus, to ensure this does not arise, ext3 (in ordered mode) writes dirty data blocks (of regular .les) to the disk .rst,before any related metadata is written to disk. Speci.cally, the ordered mode protocol is as follows: 
OPERATING SYSTEMS ARPACI-DUSSEAU 
1. 	
Data write: Write the data (e.g., D2) to its .nal location; wait for this write to complete. 

2. 	
Journal metadata write: Write the contents of the transaction (containing TxBegin, and in our example, Iv2, B2, but not D2) to the log; wait for writes to complete 

3. 	
Journal commit: Write the transaction commit block (contain­ing TxEnd) to the log; wait for write to complete; transactionis now committed 

4. 	
Checkpoint metadata: Write the contents of the metadata up­date (e.g., Iv2 and B2) to the .le system proper 

5. 
Free: Some time later, mark the transaction free in the journal 


By forcing the data write .rst, ext3 guarantees that a pointerwill never point to garbage. Indeed, this rule of “write the pointed to object before the object with the pointer to it” is at the core of con­sistency, and is exploited even further by other crash consistency schemes [GP94] (see below for details). 
In most systems, metadata journaling (akin to ordered journal­ing of ext3) is more popular than full data journaling. For example, Windows NTFS and SGI’s XFS both use non-ordered metadata jour­naling. Linux ext3 gives you the option of choosing either data, or­dered, or unordered modes (in unordered mode, data can be written at any time, thus potentially letting inodes point to garbage). All of these modes keep metadata consistent; they vary in their semantics for user data. 

Tricky Case: Block Reuse 
There are some interesting corner cases that make journalingmore tricky, and thus are worth discussing. A number of them revolve around block reuse; as Stephen Tweedie (one of the main forcesbe­hind ext3) said, “What’s the hideous part of the entire system? ... It’s deleting .les. Everything to do with delete is hairy. Everything to do with delete... you have nightmares around what happens if blocks get deleted and then reallocated.” [T00] 
The particular example Tweedie gives is as follows. Suppose you are just using some form of metadata journaling (and thus datablocks for .les are not journaled). Let’s say you have a directory called foo.The user adds an entry to foo (say by creating a .le), and thus the contents of foo (because directories are considered meta-
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
data) are written to the log; say this was in data block 1000. Then the user deletes everything in the directory as well as the directory itself, thus freeing up block 1000 for reuse. Finally, the user creates a new .le (say foobar), which ends up reusing the same block (1000) that used to belong to foo.The inode of foobar is committed to disk, as is its data; note, however, because metadata journaling is inuse, the newly-written data in block 1000 in the .le foobar is not journaled. 
Now assume a crash occurs and all of this information is still in the log. During replay, the recovery process simply replays every­thing in the log, including the write of directory data in block 1000, thus overwriting the user data in the block with old directorycon­tents! Clearly this is not a correct recovery action. 
There are a number of solutions to this problem. One could, for example, never reuse blocks until the delete of said blocks ischeck­pointed out of the journal. What Linux ext3 does instead is to add anew type of record to the journal, known as a revoke record. In the case above, deleting the directory would cause a revoke record to be written to the journal. When replaying the journal, the system .rst scans for such revoke records; any such revoked data is never replayed, thus avoiding the problem mentioned above. 


41.4 Solution #3: Other Approaches 
We’ve thus far described two options in keeping .le system meta­data consistent: a lazy approach based on fsck,anda more active approach known as journaling. However, these are not the onlytwo approaches. One such approach, known as Soft Updates [GP94], was introduce by Ganger and Patt. This approach carefully orders all writes to the .le system to ensure that the on-disk structures are never left in an inconsistent state. For example, by writing apointed­to data block to disk before the inode that points to it, we can ensure that the inode never points to garbage; similar rules can be derived for all the structures of the .le system. Implementing Soft Updates can be a challenge, however; whereas the journaling layer described above can be implemented with relatively little knowledge ofthe ex­act .le system structures, Soft Updates requires intricate knowledge of each .le system data structure and thus adds a fair amount of complexity to the system. 
Another approach is known as copy-on-write (yes, COW), and is 
OPERATING SYSTEMS ARPACI-DUSSEAU 
used in a number of popular .le systems, including Sun’s ZFS [B07]. This technique never overwrites .les or directories in place; rather, it places new updates to previously unused locations on disk.Af­ter a number of updates are completed, COW .le systems .ip the root structure of the .le system to include pointers to the newly up­dated structures. Doing so makes keeping the .le system consistent straightforward. We’ll be learning more about this technique when we discuss the log-structured .le system (LFS) in a future chapter; LFS was an early variant of a COW .le system. 
One .nal approach we just developed here at Wisconsin. In this new technique, entitled backpointer-based consistency (or BBC), no ordering is enforced between writes. To achieve consistency, an addi­tional back pointer is added to every block in the system; for exam­ple, each data block has a reference to the inode to which it belongs. When accessing a .le, the .le system can determine if the .le iscon­sistent by checking if the forward pointer (e.g., the addressin the inode or direct block) points to a block that refers back to it.If so, everything must have safely reached disk and thus the .le is consis­tent; if not, the .le is inconsistent, and an error is returned. By adding back pointers to the .le system, a new form of lazy crash consistency can be attained; read the paper for more details [C+12]. 

41.5 Summary 
We have introduced the problem of crash consistency, and dis­cussed various approaches to attacking this problem. The older ap­proach of building a .le system checker works but is likely to slow to recover on modern systems. Thus, many .le systems now use jour­naling. Journaling reduces recovery time from O(size-of-the-disk­volume) to O(size-of-the-log), thus speeding recovery substantially after a crash and restart. For this reason, many modern .le sys­tems use journaling. We have also seen that journaling can come in many different forms; the most commonly used is ordered meta­data journaling, which reduces the amount of traf.c to the journal while still preserving reasonable consistency guarantees for both .le system metadata as well as user data. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

References 
[B07] “ZFS: The Last Word in File Systems” Jeff Bonwick and Bill Moore Available: http://opensolaris.org/os/community/zfs/docs/zfs last.pdf 
ZFS uses copy-on-write and journaling, actually, as in some cases, logging writes to disk will perform better. 
[C+12] “Consistency Without Ordering” Vijay Chidambaram, Tushar Sharma, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau FAST ’12, San Jose, California 
Arecent paper of ours about a newform of crash consistency based on back pointers. Read it for the exciting details! 
[GP94] “Metadata Update Performance in File Systems” Gregory R. Ganger and Yale N. Patt OSDI ’94 
Aclever paper about using careful ordering of writes as the main way to achieve consistency. Implemented later in BSD-based systems. 
[G+08] “SQCK: A Declarative File System Checker” Haryadi S. Gunawi, Abhishek Rajimwale, Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau OSDI ’08, San Diego, California 
Our own paper on a new and better way to build a .le system checker using SQL queries. We also show some problems with the existing checker, .nding numerous bugs and odd behaviors, a direct result of the complexity of fsck. 
[H87] “Reimplementing the Cedar File System Using Logging and Group Commit”, Robert Hagmann SOSP ’87 
The .rst work that applied write-ahead logging (journaling)to a .le system. 
[MK96] “Fsck -The UNIX File System Check Program” Marshall Kirk McKusick and T. J. Kowalski Revised in 1996 
Describes the .rst comprehensive .le-system checking tool,the eponymous fsck.Written by some of the same people who brought you FFS. 
[MJLF84] “A Fast File System for UNIX” 
Marshall K. McKusick, William N. Joy, Sam J. Lef.er, Robert S.Fabry 
ACM Transactions on Computing Systems. 
August 1984, Volume 2:3 

You already know enough about FFS, right? But yeah, it is OK to reference papers like this more than once in a book, because you should know about them. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

[R+11] “Coerced Cache Eviction and Discreet-Mode Journaling: 
Dealing with Misbehaving Disks” 
Abhishek Rajimwale, Vijay Chidambaram, Deepak Ramamurthi, 
Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau 
DSN ’11, Hong Kong, China, June 2011 

Our own paper on the problem of disks that buffer writes in a memory cache instead of forcing them to disk, even when explicitly told not to do that! Our solution to overcome this problem: if you want A to be written to disk before B,.rst write A,then send a lot of “dummy” writes to disk, hopefully causing A to be forced to disk to make room for them in the cache. A neat if impractical solution. 
[T98] “Journaling the Linux ext2fs File System” Stephen C. Tweedie The Fourth Annual Linux Expo, May 1998 
Tweedie did much of the heavy lifting in adding journaling to the Linux ext2 .le system; the result, not surprisingly, is called ext3. Some nice design decisions include the strong focus on backwards compatibility, e.g., you can just add a journaling.le to an existingext2 .le system and then mount it as an ext3 .le system. 
[T00] “EXT3, Journaling Filesystem” Stephen Tweedie Talk at the Ottawa Linux Symposium, July 2000 olstrans.sourceforge.net/release/OLS2000-ext3/OLS2000-ext3.html 
Atranscript of a talk given by Tweedie on ext3. 
[T01] “The Linux ext2 File System” 
Theodore Ts’o, June, 2001. 
Available: http://e2fsprogs.sourceforge.net/ext2.html 

Asimple Linux .le system based on the ideas found in FFS. For a while it was quite heavily used; now it is really just in the kernel as an example of a simple .le system. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
42 



Log-structured File Systems 
In the early 90’s, a group at Berkeley led by Professor John Ouster­hout and graduate student Mendel Rosenblum developed a new .le system known as the log-structured .le system [RO91]. Their moti­vation to do so was based on the following observations: 
• 	
Memory sizes were growing:As memory got bigger, more data could be cached in memory. As more data is cached, disk traf.c would increasingly consist of writes, as reads would be serviced in the cache. Thus, .le system performance would largely be determined by its performance for writes. 

• 	
There was a large and growing gap between random I/O per­formance and sequential I/O performance:Transfer bandwidth increases roughly 50%-100% every year; seek and rotational delay costs decrease much more slowly, maybe at 5%-10% per year [P98]. Thus, if one is able to use disks in a sequential man­ner, one gets a huge performance advantage, which grows over time. 

• 	
Existing .le systems perform poorly on many common work­loads:For example, FFS [MJLF84] would perform a large num­ber of writes to create a new .le of size one block: one for a new inode, one to update the inode bitmap, one to the direc­tory data block that the .le is in, one to the directory inode to update it, one to the new data block that is apart of the new .le, and one to the data bitmap to mark the data block as allocated. Thus, although FFS would place all of these blocks within the 


565 
same block group, FFS would incur many short seeks and sub­sequent rotational delays and thus performance would fall far short of peak sequential bandwidth. 
• 	File systems were not RAID-aware:For example, RAID-4 and RAID-5 have the small-write problem where a logical write to asingle block causes 4physical I/Os to take place. Existing .le systems do not try to avoid this worst-case RAID writing behavior. 
An ideal .le system would thus focus on write performance, and try to make use of the sequential bandwidth of the disk. Further, it would perform well on common workloads that not only write out data but also update on-disk metadata structures frequently. Finally, it would work well on RAIDs as well as single disks. 
The new type of .le system Rosenblum and Ousterhout intro­
duced was called LFS,short for the Log-structured File System.When 
writing to disk, LFS .rst buffers all updates (including metadata!) in 
an in-memory segment;when the segment is full, it is written to disk 
in one long, sequential transfer to an unused part of the disk (i.e., LFS 
never overwrites existing data, but rather always writes segments to 
afree part of the disk). Because segments are large, the disk is used 
quite ef.ciently, and thus performance of the .le system approaches 
the peak performance of the disk. 
THE CRUX: HOW TO MAKE ALL WRITES SEQUENTIAL WRITES? 
How can a .le system turns all writes into sequential writes? For reads, this task is impossible, as the desired block to be readmay be anywhere on disk. For writes, however, the .le system always has a choice, and it is exactly this choice we hope to exploit. 
42.1 Writing To The Log: Some Details 
Let’s try to understand this a little bit better through an example. 
Imagine we are appending a new block to a .le; assume that the .le 
already exists but currently has no blocks allocated to it (itis zero 
OPERATING SYSTEMS ARPACI-DUSSEAU 
sized). To do so, LFS of course places the data block D in this in-memory segment: 
|D| 
However, we also must update the inode to now point to the block. Because LFS wants to make all writes sequential, it also must include the inode I in the update to disk. Thus, the segment (still in memory) now looks like this: 
|D|I| 
Note further that I is also updated to point to D (and also note that the pointer within I is a disk address, and thus when placing I in the segment, LFS must have an idea of where this segment will be written to disk). Assume this type of activity continues and the segment .nally .lls up and is written to disk. So far, so good. We have now written out I and D to disk, and the write to disk was ef.cient. Unfortunately, we have our .rst real problem: how can we .nd the inode I? 

42.2 How Can We Find Those Pesky Inodes? 
To understand how we .nd an inode in LFS, let us .rst make sure we understand how to .nd an inode in a typical UNIX .le system. In a typical .le system such as FFS, or even the old UNIX .le system, .nding inodes is really easy. They are organized in an array and placed on disk at a .xed location (or locations). For example,the old UNIX FS keeps all inodes at a .xed portion of the disk. Thus, given an inode number and the start address, to .nd a particular inode, you can calculate its exact disk address simply by multiplying the inode number by the size of an inode, and adding that to the start address of the on-disk array. Here is what this looks like on disk: 
Super Block | Inodes | Data blocks 
If we expand this a bit, and assume a single block for the super block, and that we have ten blocks for inodes, we get: 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
Super Block | Inodes | Data blocks b0 |b1b2b3b4b5b6b7b8b9b10|b11 ... 
Imagine we know that each inode block of size 512 bytes, and that each inode is of size 128 bytes, and let us also assume that inodes are numbered from 0 to 39. We thus get this picture, with 4 inodes (i.e., 512/128) in each block: 
Super Block | Inodes | Data blocks 
b0 |b1b2b3b4b5b6b7b8b9b10|b11 ... 
| 0 4 812162024283236 |
| 1 5 913172125293337 |
| 2 61014182226303438 |
| 3 71115192327313539 | 

Thus, to .nd inode 14, we .rst divide 14 by 4 and get 3 (we use integer division); thus inode 14 is in the “third” block of inodes (0 is the “zeroth” block of inodes, 1 is the “.rst”, and so on). Because the inode array starts at sector address 1, we add 3 to 1 and get sector 4(b4); now we know which block to read to fetch inode 14. Then we do 14 mod 4 to get which inode within the block (b4) to read: 2 (again starting at 0). It is just a simple calculation. 
Finding an inode given an inode number in FFS is only slightly more complicated; FFS splits up the array into chunks and places a group of inodes within each cylinder group. Thus, one must know how big each chunk of inodes is and the start addresses of each.After that, the calculations are similar and also easy. 
In LFS, life is more dif.cult. Why? Well, we’ve managed to scatter the inodes all throughout the disk! Worse, we never overwritein place, and thus the latest version of an inode (i.e., the one wewant) keeps moving. 

42.3 Solution Through Indirection: The Inode Map 
To remedy this, the designers of LFS introduced a level of indirec­tion between inode numbers and the inodes through a data structure called the inode map (imap).The imap is a structure that takes an inode number as input and produces the disk address of the most recent version of the inode. Thus, you can imagine it would often be implemented as a simple array, with 4 bytes (a disk pointer)per entry. Any time an inode is written to disk, the imap is updatedwith its new location. 
OPERATING SYSTEMS ARPACI-DUSSEAU DESIGN TIP:USE ALEVEL OF INDIRECTION 
People often say that the solution to all problems in Computer Science is simply a level of indirection.This is clearly not true; it is just the solution to most problems. You certainly can think of ev­ery virtualization we have studied, e.g., virtual memory, assimply a level of indirection. And certainly the inode map in LFS is a virtual­ization of inode numbers. Hopefully you can see the great power of indirection in these examples, allowing us to freely move structures around (such as pages in the VM example, or inodes in LFS) without having to change every reference to them. Of course, indirection can have a downside too: extra overhead. So next time you have a prob­lem, try solving it with indirection. But make sure to think about the overheads of doing so .rst. 

42.4 Even More Problems: Where To Put The Inode Map? 
The imap, unfortunately, needs to be kept persistent (i.e., written to disk); doing so allows LFS to keep track of the locations of inodes across crashes, and thus operate as desired. Thus, a question: where should the imap live? 
It could live on a .xed part of the disk, of course. Unfortunately, as it gets updated frequently, this would then require the segment writes to be followed by writes to the imap, and hence performance would suffer (i.e., there would be more disk seeks, between each seg­ment and the .xed location of the imap). 
Thus, LFS places pieces of the inode map into the current segment 
as well. Thus, when writing a data block to disk (as above), we might 
actually see: 
|D|I |imap(I) | 
where imap(I) is the piece of the inode map that tells us where 
inode I is on disk. Note that imap(I) will also include the mapping 
information for some other inodes that are near inode I in the imap. 
The clever reader might have noticed a problem here. How do we .nd the inode map, now that pieces of it are also now spread 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
across the disk? LFS .nally keeps a .xed place on disk for this,and it is known as the checkpoint region (CR).The checkpoint region contains pointers to the latest pieces of the inode map, and thus the inode map pieces can be found. Note the checkpoint region is only updated periodically (say every 30 seconds or so), and thus perfor­mance is not ill-affected. Thus, the overall structure of theon-disk layout contains a checkpoint region and a bunch of segments, some of which are in use, and some of which are yet to be used, as we see here: 
|| | | | | (pointer to thenext segment inthe log) |v| v|v 
|CR |seg1 |FREE |seg2 |seg3| FREE| .... 
CR : contains pointers to pieces of the imap segment : contains pieces of the imap, inodes, data, etc. 
As you can also see from the picture, each segment also contains apointer to the nextsegment; the chain of these segments thusforms the log,and hence the overall name of the log-structured .le system. By starting at the checkpoint region, one should be able to .ndthe .rst segment that was written to disk and follow a chain of segments on the disk to see what updates have taken place. 

42.5 Reading A File From Disk: A Recap 
To make sure you understand what is going on, let us now read a.le from disk. Assume we have nothing in memory to begin. The .rst on-disk data structure we must read is the checkpoint region. The checkpoint region contains pointers (i.e., disk addresses) to the entire inode map, and thus LFS then reads in the entire inode map and caches it in memory. After this point, when given an inode num­ber of a .le, LFS simply looks up the inode-number to inode-disk­address mapping in the imap, and reads in the most recent version of the inode. To read a block from the .le, at this point, LFS pro­ceeds exactly as a typical UNIX .le system, by using direct pointers or indirect pointers or doubly-indirect pointers as need be.Thus, in the common case, LFS should perform the same number of I/Os as atypical .le system when reading a .le from disk; the entire imap is 
OPERATING SYSTEMS ARPACI-DUSSEAU 
cached and thus the only extra work LFS does during .le read is to look up the address of the inode in the imap. 

42.6 A New Problem: Garbage Collection 
You may have noticed another problem with LFS; it keeps writ­ing newer version of a .le, its inode, and in fact all data to newparts of the disk. This process, while keeping writes ef.cient, implies that LFS leaves older versions of a .le all over the disk, scatteredthrough­out a number of older segments. 
One could keep those older versions around and allow users to restore old .le versions (for example, when they accidentally over­write or delete a .le, it could be quite handy to do so); such a .le system is known as a versioning .le system because it keeps track of the different versions of a .le. However, LFS instead keepsonly the latest live version of a .le; thus (in the background), LFSmust pe­riodically .nd these old dead versions of .le data, inodes, etc., and clean them; cleaning should thus make blocks on disk free again for use in a subsequent segment write. Note that the process of clean­ing is a form of garbage collection,a similar method that arises in languages that automatically free unused memory for programs. 
The basic LFS cleaning process works as follows. Periodically, the LFS cleaner must read in a number of old (partially-used) seg­ments, determine which blocks are live within the segment, and then write out a new set of segments with just the live blocks withinthem. Speci.cally, we expect the cleaner to read in M existing segments, compact their contents into N new segments (where N < M), and then write the N segments to disk in new locations. The old M seg­ments are then freed and can be used by the .le system for subse­quent writes. It is such cleaning that leads to free space between used segments, as shown in the picture above. 
We are now left with two problems, however. The .rst is mecha­nism: how can LFS tell which blocks within a segment are live, and which are dead? The second is policy: how often should the cleaner run, and which segments should it pick to clean? 
42.7 How Can We Determine Which Blocks Are Live? 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
We address the mechanism .rst. Given a data block D within an on-disk segment S, LFS must be able to determine whether D is live. To do so, LFS adds a little extra information to each segment that describes each block. Speci.cally, LFS includes, for each data block D, its inode number (which .le it belongs to) and its offset (which block of the .le this is). This information is recordedin a little structure at the head of the segment known as the segment summary block. 
Given this information, it is straightforward to determine whether 
ablock is live or dead. For a block D located on disk ataddress 
A, look in the segment summary block and .nd its inode number I 
and offset T. Next, look in the imap to .nd where I lives and read 
Ifrom disk (perhaps it is already in memory, which is even better). 
Finally, using the offset T, look in the inode (or some indirect block) 
to see where the Tth block of this .le is on disk. If it points exactly to 
disk address A, LFS can conclude that this block is live. If it points 
anywhere else, LFS can conclude that D is not in use (i.e., it isdead) 
and thus know that this version is no longer needed. 
(a PICTURE here would be useful) 
There are some shortcuts LFS takes to make the process of deter­mining liveness more ef.cient. For example, when a .le is truncated or deleted, LFS increases its version number and records the new version number in the imap. By also recording the version num­ber in the on-disk segment, LFS can short circuit the longer check described above simply by comparing the on-disk version number with a version number maintained in the imap, and thus avoid extra reads. 

42.8 A Policy Question: Which Blocks To Clean, And When? 
On top of the mechanism described above, LFS must build a set of policies to determine both when to clean and which blocks are worth cleaning. Determining when to clean is easier; either periodically, during idle time, or when you have to because the disk is full. 
Determining which blocks to clean is more challenging, and has been the subject of many research papers. In the original LFS paper [RO91], the authors describe an approach which tries to segregate hot and cold blocks. A hot block is one in which the contents are being frequently over-written; thus, for such a block, the best policy 
OPERATING SYSTEMS ARPACI-DUSSEAU 
is to wait a long time before cleaning it, as more and more blocks are getting over-written (in new segments) and thus being freed for use. Acold block, in contrast, may have a few dead blocks but the rest of its contents are relatively stable. Thus, the authors conclude that one should clean cold segments sooner and hot segments later,and develop a heuristic that does exactly that. However, as with most policies, this is just one approach, and by de.nition is not “the best” approach; later approaches show how to do better [MR+97]. 

42.9 Crash Recovery 
Crash recovery begins with the checkpoint region; the latestcheck­
point region points to pieces of the imap, and those point to inodes 
representing a snapshot of .le system state. However, for perfor­
mance reasons, the checkpoint region is only .ushed to disk period­
ically; thus, a crash will leave a number of segments on the disk that 
are not pointed to by the latest checkpoint update. 
LFS tries to recover many of those segments through recovery, which takes place when you mount the .le system after a crash. LFS does this by a technique known as roll forward in the database com­munity. The basic idea is to start with the last checkpoint, .nd the end of the log, and then use that to read through the next segment and see if there are any valid updates within it. If so, update the .le system accordingly and thus recover data and metadata written since the last checkpoint. 
There are some more details here (tricky cases and such) whichI have not yet included. Sorry! 

42.10 Summary 
LFS introduces a new approach to updating the disk. Instead of over-writing .les in places, LFS always writes to an unused portion of the disk, and then later reclaims that old space through cleaning. This approach, which in database systems is called shadow paging [L77] and in .le-system-speak is sometimes called copy-on-write, enables highly ef.cient writing, as LFS can gather all updates into an in-memory segment and then write them out together sequentially. 
The downside to this approach is that it generates garbage; old copies of the data are scattered throughout the disk, and if one wants 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
to reclaim such space for subsequent usage, one must clean oldseg­ments periodically. Cleaning became the focus of much controversy in LFS, and concerns over cleaning costs [SS+95] perhaps limited LFS’s initial impact on the .eld. However, some modern commercial .le systems, including NetApp’s WAFL and Sun’s ZFS both adopta similar copy-on-write approach to writing to disk, and thus the in­tellectual legacy of LFS lives on in these modern .le systems. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

References 
[L77] “Physical Integrity in a Large Segmented Database” 
R. Lorie ACM Transactions on Databases, 1977, Volume 2:1, pages 91-104 The original idea of shadow paging is presented here. 
[MJLF84] “A Fast File System for UNIX” 
Marshall K. McKusick, William N. Joy, Sam J. Lef.er, Robert S.Fabry 
ACM TOCS, August, 1984, Volume 2, Number 3, pages 181-197 

[MR+97] “Improving the performance of log-structured .le systems with adaptive meth­ods” Jeanna Neefe Matthews, Drew Roselli, Adam M. Costello, Randolph Y. Wang, 
Thomas E. Anderson 
SOSP 1997, pages 238-251, October, Saint Malo, France 

[P98] “Hardware Technology Trends and Database Opportunities” 
David A. Patterson 
ACM SIGMOD ’98 Keynote Address, Presented June 3, 1998, Seattle, Washington 
Available: http://www.cs.berkeley.edu/ pattrsn/talks/keynote.html 

[RO91] “Design and Implementation of the Log-structured File System” 
Mendel Rosenblum and John Ousterhout, SOSP ’91 
More information is available in the dissertation: 
http://www.eecs.berkeley.edu/Pubs/TechRpts/1992/CSD-92-696.pdf 

[SS+95] “File system logging versus clustering: a performance comparison” 
Margo Seltzer, Keith A. Smith, Hari Balakrishnan, Jacqueline Chang, Sara McMains, 
Venkata Padmanabhan 
USENIX 1995 Technical Conference, New Orleans, Louisiana, 1995 

THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
43 



Modern File Systems (INCOMPLETE) 
Beyond the basic advances found in the .le systems we have stud­ied thus far, a number of other .le systems have interesting features worth studying. For example, how should a .le system protect its data? How should it provide scalable performance under increas­ingly demanding workloads? And how should a .le system be de­signed to run atop multiple disks instead of just one? And hence the crux of our study: 

CRUX:WHAT FEATURES SHOULD MODERN FILE SYSTEMS HAVE 
xxx. 
43.1 WAFL and Checksums 
The .rst .le system we study is NetApp’s Write-Anywhere File Layout (WAFL)[HL93]. 

43.2 XFS and B-Trees 
Another 
43.3 ZFS and RAID 
577 
References 
OPERATING SYSTEMS ARPACI-DUSSEAU 
44 



Summary Dialogue on Persistence 
Professor: xxx Student: xxx 
579 
45 


ADialogue on Distribution 
Professor: And thus we reach our .nal little piece in the world of oper­ating systems: distributed systems. Since we can’t cover much here, we’ll sneak in a little intro here in the section on persistence, andfocus mostlyon distributed .le systems. Hope that is OK! 
Student: Sounds OK. But what is a distributed system exactly, oh glorious and all-knowing professor? 
Professor: Well, I bet you know how this is going to go... 
Student: There’s a peach? 
Professor: Exactly! But this time, it’s far away from you, and may take some time to get the peach. And there are a lot of them! Even worse, some­times a peach becomes rotten. But you want to make sure that when any­body bites into a peach, they will get a mouthful of deliciousness. 
Student: This peach analogy is working less and less for me. 
Professor: Come on! It’s the last one, just go with it. 
Student: Fine. 
Professor: So anyhow, forget about the peaches. Building distributed sys­tems is hard, because things fail all the time. Messages get lost, machines go down, disks corrupt data. It’s like the whole world is workingagainst you! 
Student: But I use distributed systems all the time, right? 
581 
Professor: Yes! You do. And... ? 
Student: Well, it seems like they mostly work. After all, when I send a search request to google, it usually comes back in a snap, withsome great results! Same thing when I use facebook, or Amazon, and so forth. 
Professor: Yes, it is amazing. And that’s despite all of those failures tak­ing place! Those companies build a huge amount of machinery into their systems so as to ensure that even though some machines have failed, the entire system stays up and running. They use a lot of techniques to do this: replication, retry, and various other tricks people have developed over time to detect and recover from failures. 
Student: Sounds interesting. Time to learn something for real? 
Professor: It does seem so. Let’s get to work! But .rst things .rst ... (bites into peach he has been holding, which unfortunately is rotten) 
OPERATING SYSTEMS ARPACI-DUSSEAU 
46 



Distributed Systems 
We now delve into one .nal major theme of our text: distribution. In this part of the book, we will explore some of the underlyingtech­nology in one of the most important aspect of modern systems: how to build distributed systems. 
Distributed systems have changed the face of the world. When your web browser connects to a web server somewhere else on the planet, it is participating in what seems to be a simple form ofa client/server distributed system. When you contact a modern web service such as Google or facebook, you are not just interacting with asingle machine, however; behind the scenes, these complex ser­vices are built from a large collection (i.e., thousands) of machines, each of which cooperate to provide the particular service of the site. Thus, it should be clear what makes studying distributed systems interesting. Indeed, it is worthy of an entire class; here, wejust intro­duce a few of the major topics. 
Anumber of new challenges arise when building a distributed system. The major one we focus on is failure;machines,disks, net­works, and software all fail from time to time, as we do not (and likely, will never) know how to build “perfect” components and sys­tems. However, when we build a modern web service, we’d like itto appear to clients as if it never fails; how can we accomplish this task? 
Interestingly, while failure is a central challenge in constructing distributed systems, it also represents an opportunity. Yes, machines fail; but the mere fact that a machine fails does not imply the en­tire system must fail. By collecting together a set of machines, we can build a system that appears to rarely fail, despite the fact that 
583 
Crux: HOW TO BUILD SYSTEMS THAT WORK WHEN COMPONENTS FAIL The crux of the problem in building distributed systems is dealing with failure. How can we build a working system out of parts that don’t work correctly all the time? The basic question should remind you of some of the topics we discussed in RAID storage arrays; how­ever, the problems here tend to be more complex, as are the solutions. 
its components fail regularly. This reality is the central beauty and value of distributed systems, and why they underly virtuallyevery modern web service you use, including Google, facebook, Amazon, etc. 
Other important issues exist as well. System performance is of­ten critical; with a network connecting our distributed system to­gether, system designers must often think carefully about how to ac­complish their given tasks, trying to reduce the number of messages sent and further make communication as ef.cient (low latency, high bandwidth) as possible. 
Finally, security is also a necessary consideration. When connect­ing to a remote site, having some assurance that the remote party is who they say they are becomes a central problem. Further, ensuring that third parties cannot monitor or alter an on-going communica­tion between two others is also a challenge. 
In this introduction, we’ll cover the most basic new aspect that is new in a distributed system: communication.Namely, how should machines within a distributed system communicate with one an­other? We’ll start with the most basic primitives available,messages, and build a few higher-level primitives on top of them. As we said above, failure will be a central focus: how should communication layers handle failures? 
46.1 Communication Basics 
The central tenet of modern networking is that communicationis fundamentally unreliable. Whether in the wide-area Internet, or a local-area high-speed network such as In.niband, packets are regu­larly lost, corrupted, or otherwise do not reach their destination. 
OPERATING SYSTEMS ARPACI-DUSSEAU ASIDE:COMMUNICATION IS UNRELIABLE 
In virtually all circumstances, it is good to view communication as a fundamentally unreliable activity. Bit corruption, down or non­working links and machines, and lack of buffer space for incoming packets all lead to the same result: packets sometimes do not reach their destination. Thus, to build reliable services atop such unreliable networks, we must consider techniques that can cope with packet loss. 
There are a multitude of causes for packet loss or corruption.Some­
times, during transmission, some bits get .ipped due to electrical or 
other similar problems. Sometimes, an element in the system,such 
as a network link or packet router or even the remote host, are some­
how damaged or otherwise not working correctly; network cables 
do accidentally get severed, at least sometimes. 
More fundamental however is packet loss due to lack of buffering within a network switch, router, or endpoint. Speci.cally, even if we could guarantee that all links worked correctly, and that all the components in the system (switches, routers, end hosts) wereup and running as expected, loss is still possible, for the following reason. Imagine a packet arrives at a router; for the packet to be processed, it must be placed in memory somewhere within the router. If many such packets arrive at once, it is possible that the memory within the router cannot accommodate all of the packets. The only choicethe router has at that point is to drop one or more of the packets. This same behavior occurs at end hosts as well; when you send a large number of messages to a single machine, the machine’s resources can easily become overwhelmed, and thus packet loss again arises. 
Thus, packet loss is fundamental in networking. The question thus becomes: how should we deal with it? 

46.2 Unreliable Communication Layers 
One simple way is this: we don’t deal with it. Because some ap­plications know how to deal with packet loss, it is sometimes useful to let them communicate with a basic unreliable messaging layer, an example of the end-to-end argument one often hears about in the world of networking. One excellent example of such an unreliable 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
// client code
int main(int argc, char *argv[]) { int sd = UDP_Open(20000); struct sockaddr_in addr, addr2; int rc = UDP_FillSockAddr(&addr, "machine.cs.wisc.edu", 10000);char message[BUFFER_SIZE];sprintf(message, "hello world"); rc = UDP_Write(sd, &addr, message, BUFFER_SIZE);if (rc > 0){ 
int rc = UDP_Read(sd, &addr2, buffer, BUFFER_SIZE); } return 0; 
} 
// server code 
int main(int argc, char *argv[]) { 
int sd = UDP_Open(10000);
assert(sd > -1);
while (1) { 

struct sockaddr_in s; 
char buffer[BUFFER_SIZE];
int rc = UDP_Read(sd, &s, buffer, BUFFER_SIZE); 
if (rc > 0) {

char reply[BUFFER_SIZE];
sprintf(reply, "reply"); 
rc = UDP_Write(sd, &s, reply, BUFFER_SIZE); 

}
} 
return 0; 

} 
Figure 46.1: Example UDP/IP Client/Server Code 
layer is found in the UDP/IP networking stack available today on virtually all modern systems. To use UDP, a process uses the sock­ets API in order to create a communication endpoint;processes on other machines (or on the same machine) send UDP datagrams to the original process (a datagram is just a .xed-sized buffer up tosome maximum size). 
Figures 46.1 and 46.2 show a simple client and server built on top of UDP/IP. The client can send a message to the server, which then responds with a reply. With this small amount of code, you haveall you need to begin building distributed systems! 
UDP is a great example of an unreliable communication layer. If you use it, you will encounter situations where packets getlost 
OPERATING SYSTEMS ARPACI-DUSSEAU 
int UDP_Open(int port) {int sd; if ((sd = socket(AF_INET, SOCK_DGRAM, 0)) == -1) { return -1; } struct sockaddr_in myaddr;bzero(&myaddr, sizeof(myaddr));myaddr.sin_family = AF_INET; myaddr.sin_port = htons(port);myaddr.sin_addr.s_addr = INADDR_ANY; if (bind(sd, (struct sockaddr *)&myaddr, sizeof(myaddr)) == -1){ 
close(sd); 
return -1; 
} 
return sd; 

} 
int UDP_FillSockAddr(struct sockaddr_in *addr, char *hostName, int port) { bzero(addr, sizeof(struct sockaddr_in));addr->sin_family = AF_INET; // host byte order addr->sin_port = htons(port); // short, network byte order struct in_addr *inAddr; struct hostent *hostEntry;if ((hostEntry = gethostbyname(hostName)) == NULL) { return-1; } inAddr = (struct in_addr *)hostEntry->h_addr; addr->sin_addr = *inAddr; return 0; 
} 
int UDP_Write(int sd, struct sockaddr_in *addr, char *buffer, int n) { int addrLen = sizeof(struct sockaddr_in); return sendto(sd, buffer, n, 0, (struct sockaddr *)addr, addrLen); 
} 
int UDP_Read(int sd, struct sockaddr_in *addr, char *buffer, int n) { int len = sizeof(struct sockaddr_in); return recvfrom(sd, buffer, n, 0, (struct sockaddr *)addr, 
(socklen_t *)&len); return rc; } 
Figure 46.2: A Simple UDP Library 
(dropped) and thus do not reach their destination; the senderis never thus informed of the loss. However, that does not mean that UDP does not guard against any failures at all. For example, UDP includes a checksum to detect some forms of packet corruption. 
However, because many applications simply want to send data to 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
DESIGN TIP:USE CHECKSUMS 
Checksums are a commonly-used method to detect corruption 
quickly and effectively in modern systems. A simple checksumis 
addition: just sum up the bytes of a chunk of data; of course, many 
other more sophisticated checksums have been created, including 
basic cyclic redundancy codes (CRCs), the Fletcher checksum, and 
many others [MK09]. 
In networking, checksums are used as follows. Before sendinga mes­sage from one machine to another, compute a checksum over the bytes of the message. Then send both the message and the checksum to the destination. At the destination, the receiver computes a check­sum over the incoming message as well; if this computed checksum matches the sent checksum, the receiver can feel some assurance that the data likely did not get corrupted during transmission. 
Checksums can be evaluated along a number of different axes. Effec­tiveness is one primary consideration: does a change in the data lead to a change in the checksum? The stronger the checksum, the harder it is for changes in the data to go unnoticed. Performance is the other important criterion: how costly is the checksum to compute? Unfor­tunately, effectiveness and performance are often at odds, meaning that checksums that are of high quality are often expensive tocom­pute. Life, once again, isn’t perfect. 
adestination and notworry about packetloss, we need more. Specif­ically, we need to develop a reliable communication layer on top of an unreliable network. 

46.3 Reliable Communication Layers 
To build a reliable communication layer, we need some new mech­anisms and techniques to handle packet loss. Let us consider asim­ple example in which a client is sending a message to a server over an unreliable connection. The .rst question we must answer: howdoes the sender know that the receiver has actually received the message? 
The technique that we will use is known as an acknowledgment, or ack for short. The idea is simple: the sender sends a message to the receiver; the receiver then sends a short message back to acknowledge its receipt. Figure 46.3 depicts the process. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
sender  receiver  
(send msg) ------­(msg)  
------­ 
-------->  
(recv message;send ack)  
-------­ 
------­ 
<------­(ack)(receive ack)  

Figure 46.3: Message Plus Acknowledgment  
When the sender receives an acknowledgment of the message, it  
can then rest assured that the message did indeed receive the original  
message. However, what should the sender do if it does not receive  
an acknowledgment?  
To handle this case, we need an additional mechanism, known as  
a timeout.When the sender sends a message, the sender now sets a  
timer to go off after some period of time. If, in that time, no acknowl­ 
edgment has been received, the sender concludes that the message  
has been lost. In this case, the sender simply performs a retry of the  
send, sending the same message again with hopes that this time, it  
will get through. Note that for this to work, it is implied thatthe  
sender keeps a copy of the message around, in case it needs to per­ 
form the retry. The combination of the timeout and the retry have led  
some to call the approach timeout/retry;pretty clever crowd, those  
networking types, no? Figure 46.4 shows an example of the entire  
sequence.  
Unfortunately, timeout/retry in this form is not quite enough.  
Figure 46.5 shows an example of packet loss which could lead to  
trouble. In this example, it is not the original message that gets lost,  
but the acknowledgment. From the perspective of the sender, the  
situation seems the same: no ack was received, and thus a timeout  
and retry are in order. But from the perspective of the receiver, it  
is quite different: now the same message has been received twice!  
While there may be cases where this is OK, in general it is not; imag­ 
ine what would happen when you are downloading something from  
the Internet and some extra packets were repeated inside the down­ 
load. Thus, when we are aiming for a reliable message layer, wealso  
THREE  
ARPACI-DUSSEAU  EASY PIECES  
(V0.5)  

sender receiver 
(send msg;
keep copy; 
set timer) 

-------(msg) 
-------X (msg dropped) ... (waiting for ack...) ... (timer goes off; 
set timer; 
retry message) 
-------(msg) 

--------> (recv message;send ack) 
<-------(ack)
(receive ack;
discard copy) 

Figure 46.4: Message Plus Acknowledgment 
usually want to guarantee that each message is received exactly once by the receiver. 
To enable the receiver to detect duplicate message transmission, the sender has to identify each message in some unique way, andthe receiver needs some way to track whether it has already seen each message before. When the receiver sees a duplicate transmission, it simply acks the message, but (critically) does not pass the message up to the application that receives the data. Thus, the senderreceives the ack but the message is not received twice, thus preservingthe exactly-once semantics mentioned above. 
There are myriad ways one could go about implementing this du­plicate message detector; imagine the sender generating a unique 64­bit ID number for each message sent, and the receiver keeping ahis­tory of every ID it has ever seen. Such an approach could work, but is prohibitively costly, requiring an unbounded amount of space to track all IDs. Thus, a simple approach used by many message layers is known as a sequence counter. 
With a sequence counter, the sender and receiver agree upon a 
OPERATING SYSTEMS ARPACI-DUSSEAU 
sender receiver 
(send msg;keep copy; set timer) -------(msg) 
--------> (recv message;send ack) 
(ack dropped) X ------­
(ack) ... (waiting for ack...) ... (timer goes off; 
set timer; 
retry message) 
-------(msg) 

--------> (recv message; // AGAIN!send ack) 
<-------(ack)(receive ack;discard copy) 
Figure 46.5: Message Plus Acknowledgment 
start value (e.g., 1) for a counter that each side will maintain. When­ever the send side sends a message, it sends the current value of the counter along with the sent message; this counter value (say N)thus serves as an ID for the message. The sender then increments the value (to N +1). 
The receive side uses its counter value as the expected value for the ID of the incoming message from that sender. If the ID of a re­ceived message (N)matches the receiver’s counter (also N), it acks the message and passes it up to the application; in this case, the re­ceiver concludes that this is the .rst time this message has been re­ceived. The receiver then increments its counter (to N +1), and thus now expects the next message. 
If the ack is lost, the sender will timeout and re-send message 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
ASIDE:SETTING THE TIMEOUT VALUE 
As you can probably guess from the discussion, setting the timeout 
value correctly is an important aspect of using timeouts to retry mes­
sage sends. If the timeout is too small, the sender will re-send mes­
sages needlessly, thus wasting CPU time on the sender and network 
resources. If the timeout is too large, the sender waits too long to re-
send and thus perceived performance at the sender is reduced.The 
“right” value, from the perspective of a single client and server, is 
thus to wait just long enough to detect packet loss but no longer. 
However, there are often more than just a single client and server in adistributed system, as we will see in future chapters. In ascenario with many clients sending to a single server, packet loss at the server may be an indicator that the server is overloaded. If true, clients might retry in a different adaptive manner; for example, after the .rst timeout, a client might increase its timeout value to a higheramount, perhaps twice as high as the original value. Such an exponential back-off scheme, pioneered in the early Aloha network and adopted in the .rst Ethernet networks [A70], avoid situations where resources are being overloaded by an excess of re-sends. Robust systemsstrive to avoid overload of this nature. 
N.This time, the receiver’s counter is higher (N +1), and thus the receiver knows it has already received this message. Thus it acks it but does not pass it up to the application. And in this manner, sequence counters can be used to avoid duplicates. 
The most commonly used reliable communication layer is known as TCP/IP,or just TCP for short. TCP has a great deal more sophis­tication than we describe above, including machinery to handle con­gestion in the network [VJ90], multiple outstanding requests, and hundreds of other small tweaks and optimizations. Read more about it if you’re curious (or more appropriately, take a networking course). 
OPERATING SYSTEMS ARPACI-DUSSEAU 

46.4 Communication Abstractions 
Given a basic messaging layer, we now approach the next ques­
tion in this chapter: what abstraction of communication should we 
use when building a distributed system? 
The systems community developed a number of approaches over 
the years. One body of work took OS abstractions and extended 
them to operate in a distributed environment. For example, dis­
tributed shared memory (DSM)systems enable processes on differ­
ent machines to share a large, virtual address space [LH89]. This ab­
straction turns a distributed computation into something that looks 
like a multi-threaded application; the only difference is that these 
threads run on different machines instead of different processors within 
the same machine. 
The way most DSM systems work is through the virtual mem­ory system of the OS. When a page is accessed on one machine, two things can happen. In the .rst (best) case, the page is alreadylocal on the machine, and thus the data is fetched quickly. In the second case, the page is currently on some other machine. A page fault occurs, and the page fault handler sends a message to some other machine to fetch the page, install it in the page table of the requesting process, and continue execution. 
This approach is quite a bad idea for a number of reasons. The 
largest problem for DSM is how such approaches handle failure. Imag­
ine, for example, if a machine fails; what happens to the pageson that 
machine? What if the data structures of the distributed computation 
are spread across the entire address space? In this case, parts of these 
data structures would suddenly become unavailable. Dealingwith 
failure when parts of your address space go missing is hard; imag­
ine a linked list that where a next pointer points into a portion of the 
address space that is gone. Yikes! 
Afurther problem is performance. One usually assumes, when writing code, that access to memory is cheap. In DSM systems, some accesses are inexpensive, but others cause page faults and expensive fetches from remote machines. Thus, programmers of such DSM sys­tems had to be very careful to organize computations such thatal­most no communication occurred at all, defeating much of the point of such an approach. Though a lot of research was performed in this space, it resulted in little practical impact; nobody buildsreliable dis­tributed systems using DSM today. 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 

46.5 Remote Procedure Call (RPC) 
While OS abstractions turned out to be a poor choice for building distributed systems, programming language (PL) abstractions make much more sense. The most dominant abstraction is based on the idea of a remote procedure call,or RPC for short [BN84]1. 
Remote procedure call packages all have a simple goal: to make the process of executing code on a remote machine as simple and straightforward as calling a local function. Thus, to a client, a proce­dure call is made, and some time later, the results are returned. The server simply de.nes some routines that it wishes to export. The rest of the magic is handled by the RPC system, which in general has two pieces: a stub generator (sometimes called a protocol compiler), and the run-time library.We’ll now take a look at each of these pieces in more detail. 
Stub Generator 
The stub generator’s job is simple: to take some of the yuckiness of packing function arguments and results into messages and auto­mate it. Numerous bene.ts arise: one avoids, by design, the simple mistakes that occur in writing such code by hand; further, a stub compiler can perhaps optimize such code and thus improve perfor­mance. 
The input to such a compiler is simply the set of calls a server 
wishes to export to clients. Conceptually, it could be something as 
simple as this: 
interface {
int func1(int arg1);
int func2(int arg1, int arg2);

}; 
The stub generator takes an interface like this and generatesa few different pieces of code. For the client, a client stub is gener­ated, which contains each of the functions speci.ed in the interface; aclientprogram wishing to use this RPC service would link with this client stub and call into it in order to make RPCs. 
1
In modern programming languages, we might instead say remote method invoca­tion (RMI), but who likes these languages anyhow, with all of their fancy objects? 
OPERATING SYSTEMS ARPACI-DUSSEAU 
Internally, each of these functions in the client stub do all of the work needed to perform the remote procedure call. To the client, the code just appears as a function call (e.g., the client calls func1(x)); internally, the code in the client stub for func1() does this: 
• 	
Create a message buffer. Amessage buffer is usually just a contiguous array of bytes of some size. 

• 	
Pack the needed information into the message buffer. This information includes some kind of identi.er for the functionto be called, as well as all of the arguments that the function needs (e.g., in our example above, one integer for func1). The pro­cess of putting all of this information into a single contiguous buffer is sometimes referred to as the marshaling of arguments or the serialization of the message. 

• 	
Send the message to the destination RPC server. The com­munication with the RPC server, and all of the details required to make it operate correctly, are handled by the RPC run-time library, described further below. 

• 	
Wait for the reply. Because function calls are usually syn­chronous,the call will wait for its completion. 

• 	
Unpack return code and other arguments. If the function just returns a single return code, this process is straightforward; however, more complex functions might return more complex results (e.g., a list), and thus the stub might need to unpack those as well. This step is also known as unmarshaling or de-serialization. 

• 	
Return to the caller. Finally, just return from the client stub back into the client code. 


For the server, code is also generated. The steps taken on the server are as follows: 
• 	
Unpack the message. This step, called unmarshaling or de­serialization,takes the information out of the incoming mes­sage. The function identi.er and function arguments are all extracted. 

• 	
Call into the actual function. Finally! We have reached the point where the remote function is actually executed. The RPC runtime calls into the function speci.ed by the ID (using some­thing as simple as a switch statement), passing in the desired arguments. 


THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
• 	
Package the results. The return argument(s) are marshaled back into a single reply buffer. 

• 
Send the reply. The reply is .nally sent to the caller. 


There are a few other important issues to consider in a stub com­piler. The .rst is complex arguments, i.e., how does one package and send a complex data structure? For example, when one calls the write() system call, one passes in three arguments: an integer .le descriptor, a pointer to a buffer, and a size indicating how many bytes (starting at the pointer) are to be written. If an RPC package is passed a pointer, it needs to be able to .gure out how to interpret that pointer, and perform the correct action. Usually this isaccom­plished through either well-known types (e.g., a buffer t that is used to pass chunks of data given a size, which the RPC compiler understands), or by annotating the data structures with moreinfor­mation, enabling the compiler to fully understand which bytes need to be serialized. 
Another important issue is the organization of the server with re­gards to concurrency. A simple server just waits for requestsin a simple loop, and handles each request one at a time. However, as you might have guessed, this can be grossly inef.cient; if oneRPC call blocks (e.g., on I/O), server resources are wasted. Thus, most servers are constructed in some sort of concurrent fashion. Acom­mon organization is a thread pool.In this organization, a .nite set of threads are created when the server starts; when a message arrives, it is dispatched to one of these worker threads, which then does the work of the RPC call, eventually replying; during this time, amain thread keeps receiving other requests, and perhaps dispatching them to other workers. Such an organization enables concurrent execution within the server, thus increasing its utilization; the standard costs arise as well, mostly in programming complexity, as the RPC calls may now need to use locks and other synchronization primitives in order to ensure their correct operation. 

Run-Time Library 
The run-time library handles much of the heavy lifting in an RPC system; most performance and reliability issues are handledherein. We’ll now discuss some of the major challenges in building such a run-time layer. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

One of the .rst challenges we must overcome is how to locate aremote service. This problem, of naming,is a common one in distributed systems, and in some sense goes beyond the scope of our current discussion. The simplest of approaches build on exist­ing naming systems, e.g., hostnames and port numbers provided by current internet protocols. In such a system, the client mustknow the hostname or IP address of the machine running the desired RPC 
service, as well as the port number it is using2.The protocol suite must then provide a mechanism to route packets to a particularad­dress from any other machine in the system. For a good discussion of naming, read either the Grapevine paper or about DNS and name resolution on the Internet, or better yet just read the excellent chapter in Saltzer and Kaashoek’s book [SK09]. 
Once a client knows which server it should talk to for a particular remote service, the next question is which transport-level protocol should RPC be built upon. Speci.cally, should the RPC system use areliable protocol such as TCP/IP, or be builtupon an unreliable communication layer such as UDP/IP? 
Naively the choice would seem easy: clearly we would like for a request to be reliably delivered to the remote server, and clearly we would like to reliably receive a reply. Thus we should choose the reliable transport protocol such as TCP, right? 
Unfortunately, building RPC on top of a reliable communication layer can lead to a major inef.ciency in performance. Recall from the discussion above how reliable communication layers work: with acknowledgments plus timeout/retry. Thus, when the client sends an RPC request to the server, the server responds with an acknowl­edgment so that the caller knows the request was received. Similarly, when the server sends the reply to the client, the client acks it so that the server knows it was received. By building a request/response protocol (such as RPC) on top of a reliable communication layer, two “extra” messages are sent. 
For this reason, many RPC packages are built on top of unreliable communication layers, such as UDP. Doing so enables a more ef.­cient RPC layer, but does add the responsibility of providingreliabil­ity to the RPC system. The RPC layer achieves the desired levelof responsibility by using timeout/retry and acknowledgmentsmuch 
2
Aport number is just a way of identifying a particular communication activity taking place on a machine, allowing multiple communication channels at once. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
like we described above. By using some form of sequence num­bering, the communication layer can guarantee that each RPC takes place exactly once (in the case of no failure), or at most once (in the case where failure arises). 
ASIDE:THE END-TO-END ARGUMENT 
The end-to-end argument makes the case that the highest level in asystem, i.e., usually the application at“the end”, is ultimately the only locale within a layered system where certain functionality can truly be implemented. In their landmark paper, Saltzer et al.argue this through an excellent example: reliable .le transfer between two machines. If you want to transfer a .le from machine A to machine B,and make sure that the bytes that end up on B are exactly the same as those that began on A,you must have an “end-to-end” check of this; lower-level reliable machinery, e.g., in the network or disk, provides no such guarantee. 
The contrast is an approach which tries to solve the reliable-.le­transfer problem by adding reliability to lower layers of thesystem. For example, say we build a reliable communication protocol and use it to build our reliable .le transfer. The communication proto­col guarantees that every byte sent by a sender will be received in order by the receiver, say using timeout/retry, acknowledgments, and sequence numbers. Unfortunately, using such a protocol does not a reliable .le transfer make; imagine the bytes getting corrupted in sender memory before the communication even takes place, or something bad happening when the receiver writes the data to disk. In those cases, even though the bytes were delivered reliablyacross the network, our .le transfer was ultimately not reliable. Tobuild a reliable .le transfer, one must include end-to-end checks ofreliabil­ity, e.g., after the entire transfer is complete, read back the .le on the receiver disk, compute a checksum, and compare that checksumto that of the .le on the sender. 
The corollary to this maxim is that sometimes having lower layers provide extra functionality can indeed improve system performance or otherwise optimize a system. Thus, you should not rule out hav­ing such machinery at a lower-level in a system; rather, you should carefully consider the utility of such machinery, given its eventual usage in an overall system or application. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

Other Issues 
There are some other issues an RPC run-time must handle as well. For example, what happens when a remote call takes a long time to complete? Given our timeout machinery, a long-running remote call might appear as a failure to a client, thus triggering a retry, and thus the need for some care here. One solution is to use an explicit acknowledgment (from the receiver to sender) when the reply isn’t immediately generated; this lets the client know the server received the request. Then, after some time has passed, the client can peri­odically ask whether the server is still working on the request; if the server keeps saying “yes”, the client should be happy and continue to wait (after all, sometimes a procedure call can take a long time to .nish executing). 
The run-time must also handle procedure calls with large argu­ments, larger than what can .t into a single packet. Some lower-level network protocols provide such sender-side fragmentation (of larger packets into a set of smaller ones) and receiver-side reassem­bly (of smaller parts into one larger logical whole); if not, the RPC run-time may have to implement such functionality itself. See Birrell and Nelson’s excellent RPC paper for details [BN84]. 
One issue that many systems handle is that of byte ordering.As you may know, some machines store values in what is known as big endian ordering, whereas others use little endian ordering. Big en­dian stores bytes (say, of an integer) from most signi.cant toleast signi.cant bits, much like Arabic numerals; little endian does the opposite. Both are equally valid ways of storing numeric informa­tion; the question here is how to communicate between machines of different endianness. 
RPC packages often handle this by providing a well-de.ned en­dianness within their message formats. In Sun’s RPC package,the XDR (eXternal Data Representation)layer provides this functional­ity. If the machine sending or receiving a message matches theen­dianness of XDR, messages are just sent and received as expected. If, however, the machine communicating has a different endianness, each piece of information in the message must be converted. Thus, the difference in endianness can have a small performance cost. 
A.nal issue is whether to expose the asynchronous nature of communication to clients, thus enabling some performance optimiza­tions. Speci.cally, typical RPCs are made synchronously,i.e., when 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
aclient issues the procedure call, itmustwait for the procedure call to return before continuing. Because this wait can be long, and be­cause the client may have other work it could be doing, some RPC packages enable you to invoke an RPC asynchronously.When an asynchronous RPC is issued, the RPC package sends the requestand returns immediately; the client is then free to do other work,such as call other RPCs or indeed any computation it sees .t to perform. The client at some point will want to see the results of the asynchronous RPC; it thus calls back into the RPC layer, telling it to wait for one or all outstanding RPCs to complete, at which point return arguments can be accessed. 


46.6 Summary 
We have seen the introduction of a new topic, distributed systems, and its major issue: how to handle failure which is now a common­place event. As they say inside of Google, when you have just your desktop machine, failure is rare; when you’re in a data centerwith thousands of machines, failure is happening all the time. Thekey to any distributed system is how you deal with that failure. 
We have also seen that communication forms the heart of any dis­tributed system. A common abstraction of that communicationis found in remote procedure call (RPC), which enables clients to make remote calls on servers; the RPC package handles all of the gory de­tails, including timeout/retry and acknowledgment, in order to de­liver a service that closely mirrors a local procedure call. 
The best way to really understand an RPC package is of course to use one yourself. Sun’s RPC system, using the stub compiler rpcgen,is a common one,andis widely available on systems to­day, including Linux. Try it out, and see what all the fuss is about. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

References 
[A70] “The ALOHA System – Another Alternative for Computer Communications” Norman Abramson The 1970 Fall Joint Computer Conference 
The ALOHA network pioneered some basic concepts in networking, including exponential back-off and retransmit, which formed the basis for communicationinshared-bus Ethernet networks for years. 
[BN84] “Implementing Remote Procedure Calls” Andrew D. Birrell, Bruce Jay Nelson ACM TOCS, Volume 2:1, February 1984 
The foundational RPC system upon which all others build. Yes,another pioneering effort from our friends at Xerox PARC. 
[MK09] “The Effectiveness of Checksums for Embedded ControlNetworks” Theresa C. Maxino and Philip J. Koopman IEEE Transactions on Dependable and Secure Computing, 6:1, January ’09 
Anice overview of basic checksum machinery and some performance and robustness comparisons between them. 
[LH89] “Memory Coherence in Shared Virtual Memory Systems” Kai Li and Paul Hudak ACM TOCS, 7:4, November 1989 
The introduction of software-based shared memory via virtual memory. An intriguing idea for sure, but not a lasting or good one in the end. 
[SK09] “Principles of Computer System Design” Jerome H. Saltzer and M. Frans Kaashoek Morgan-Kaufmann, 2009 
An excellent book on systems, and a must for every bookshelf. One of the few terri.c discussions on naming we’ve seen. 
[SRC84] “End-To-End Arguments in System Design” Jerome H. Saltzer, David P. Reed, David D. Clark ACM TOCS, 2:4, November 1984 
Abeautiful discussion of layering, abstraction, and where functionality must ultimately reside in computer systems. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
47 



Sun’s Network File System (NFS) 
One of the .rst uses of distributed client/server computing was in the realm of distributed .le systems. In such an environment,there are a number of client machines and one server (or a few); the server stores the data on its disks, and clients request data throughwell­formed protocol messages. Figure 47.1 depicts the basic setup. 
As you can see from the (ugly) picture, the server has the disks; 
the clients communicate through the network to access their directo­
ries and .les on those disks. 
Why do we bother with this arrangement? (i.e., why don’t we just let clients use their local disks?) Well, primarily this setup allows for easy sharing of data across clients. Thus, if you access a .le on one machine (Client0) and then later use another (Client2), you will have the same view of the .le system. Your data is naturally shared across these different machines. A secondary bene.t is centralized admin­istration;for example, backing up .les can be done from the few server machines instead of from the multitude of clients. Another advantage could be security;having all servers in a locked machine room prevents certain types of problems from arising. 
47.1 A Basic Distributed File System 
We now will study the architecture of a simpli.ed distributed.le system. A simple client/server distributed .le system has more com­ponents than the .le systems we have studied so far. On the client side, there are client applications which access .les and directories through the client-side .le system.A client application issues sys­
603 
Client 0 
Client 1 

Server 
Client 2 

Client 3 
Figure 47.1: A Generic Client/Server System 
tem calls to the client-side .le system (such as open(), read(), write(), close(), mkdir(), etc.) in order to access .les which are stored on the server. Thus, to client applications, the .le system does notappear to be any different than a local (disk-based) .le system, exceptperhaps for performance; in this way, distributed .le systems provide trans­parent access to .les, an obvious goal; after all, who would want to use a .le system that required a different set of APIs or otherwise was a pain to use? 
The role of the client-side .le system is to execute the actions needed to service those system calls. For example, if the client is­sues a read() request, the client-side .le system may send a message to the server-side .le system (or, more commonly, the .le server)to read a particular block; the .le server will then read the block from disk (or its own in-memory cache), and send a message back to the client with the requested data. The client-side .le system will then copy the data into the user buffer supplied to the read() system call and thus the request will complete. Note that a subsequent read() of the same block on the client may be cached in client memory or on the client’s disk even; in the best such case, no network traf.c need be generated. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
Client Application 
Client-side File System File Server 

Disks 
Networking Layer Networking Layer 
Figure 47.2: Distributed File System Architecture 
From this simple overview, you should get a sense that there are two important pieces of software in a client/server distributed .le system: the client-side .le system and the .le server. Together their behavior determines the behavior of the distributed .le system. 

47.2 On To NFS 
One of the earliest and most successful systems was developed by Sun Microsystems, and is known as the Sun Network File Sys­tem (or NFS) [S86]. In de.ning NFS, Sun took an unusual approach: instead of building a proprietary and closed system, Sun instead de­veloped an open protocol which simply speci.ed the exact message formats that clients and servers would use to communicate. Different groups could develop their own NFS servers and thus compete inan NFS marketplace while preserving interoperability. It worked: today there are many companies that sell NFS servers (including Sun, Net-App [HLM94], EMC, IBM, and others), and the widespread success of NFS is likely attributed to this “open market” approach. 

47.3 Focus: Simple and Fast Server Crash Recovery 
In this note, we will discuss the classic NFS protocol (version 2, 
a.k.a. NFSv2), which was the standard for many years; small changes were made in moving to NFSv3, and larger-scale protocol changes were made in moving to NFSv4. However, NFSv2 is both wonderful and frustrating and thus serves as our focus. 
In NFSv2, one of the main goals of the design of the protocol was simple and fast server crash recovery.In a multiple-client, single-server environment, this goal makes a great deal of sense; any minutethat the server is down (or unavailable) makes all the client machines (and their users) unhappy and unproductive. Thus, as the server goes, so goes the entire system. 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 

47.4 Key To Fast Crash Recovery: Statelessness 
This simple goal is realized in NFSv2 by designing what we refer to as a stateless protocol. The server, by design, does not keep track of anything about what is happening at each client. For example, the server does not know which clients are caching which blocks, or which .les are currently open at each client, or the current .le pointer position for a .le, etc. Simply put, the server does not track anything about what clients are doing; rather, the protocol is designed to de­liver in each protocol request all the information that is needed in order to complete the request. If it doesn’t now, this stateless approach will make more sense as we discuss the protocol in more detail below. 
For an example of a stateful (not stateless) protocol, consider the open() system call. Given a pathname, open() returns a .le descriptor (an integer). This descriptor is used on subsequent read() orwrite() requests to access various .le blocks, as in this applicationcode (note that proper error checking of the system calls is omitted for space reasons): 
char buffer[MAX];
int fd = open("foo", O_RDONLY); // get descriptor "fd"
read(fd, buffer, MAX); // use fd to read MAX bytes from foo 
read(fd, buffer, MAX); // use fd to read MAX bytes from foo 
... 
read(fd, buffer, MAX); // use fd to read MAX bytes from foo 
close(fd); // close file 
Figure 47.3: Client Code: Reading from a File 
Now imagine that the client-side .le system opens the .le by sending a protocol message to the server saying “open the .le ’foo’ and give me back a descriptor”. The .le server then opens the .le locally on its side and sends the descriptor back to the client. On subsequent reads, the client application uses that descriptor to call the read() system call; the client-side .le system then passes the de­scriptor in a message to the .le server, saying “read some bytes from the .le that is referred to by the descriptor I am passing you here”. 
In this example, the .le descriptor is a piece of shared state be­tween the client and the server (Ousterhout calls this distributed state [O91]). Shared state, as we hinted above, complicates crash recovery. Imagine the server crashes after the .rst read completes, but before the client has issued the second one. After the server is 
OPERATING SYSTEMS ARPACI-DUSSEAU 
up and running again, the client then issues the second read. Un­fortunately, the server has no idea to which .le fd is referring; that information was ephemeral (i.e., in memory) and thus lost when the server crashed. To handle this situation, the client and server would have to engage in some kind of recovery protocol,where the client would make sure to keep enough information around in its memory to be able to tell the server what it needs to know (in this case,that .le descriptor fd refers to .le foo). 
It gets even worse when you consider the fact that a stateful server has to deal with client crashes. Imagine, for example, a client that opens a .le and then crashes. The open() uses up a .le descriptor on the server; how can the server know it is OK to close a given .le? In normal operation, a client would eventually call close() and thus inform the server that the .le should be closed. However,when aclient crashes, the server never receives a close(), and thus has to notice the client has crashed in order to close the .le. 
For these reasons, the designers of NFS decided to pursue a state­less approach: each client operation contains all the information needed to complete the request. No fancy crash recovery is needed; the server just starts running again, and a client, at worst, might have to retry a request. 
ASIDE:WHY SERVERS CRASH 
Before getting into the details of the NFSv2 protocol, you might be wondering: why do servers crash? Well, as you might guess, there are plenty of reasons. Servers may simply suffer from a power out­age (temporarily); only when power is restored can the machines be restarted. Servers are often comprised of hundreds of thousands or even millions of lines of code; thus, they have bugs (even good soft­ware has a few bugs per hundred or thousand lines of code), and thus they eventually will trigger a bug that will cause them tocrash. They also have memory leaks; even a small memory leak will cause asystem to run outof memory and crash. And, .nally, in distributed systems, there is a network between the client and the server;if the network acts strangely (for example, if it becomes partitioned and clients and servers are working but cannot communicate), it may ap­pear as if a remote machine has crashed, but in reality it is just not currently reachable through the network. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

47.5 The NFSv2 Protocol 
We thus arrive at the NFSv2 protocol de.nition. Our problem statement is simple: 
THE CRUX:HOW TO DEFINE A STATELESS PROTOCOL 
How can we de.ne the network protocol to enable stateless op­eration? Clearly, stateful calls like open() can’t be a part of the dis­cussion (as it would require the server to track open .les); however, the client application will want to call open(), read(), write(), close() and other standard API calls to access .les and directories. Thus, as are.ned question, how do we de.ne the protocol to both be stateless and support the POSIX .le system API? 
One key to understanding the design of the NFS protocol is un­derstanding the .le handle.File handles are used to uniquely de­scribe the .le or directory a particular operation is going tooperate upon; thus, many of the protocol requests include a .le handle. 
You can think of a .le handle as having three important compo­nents: a volume identi.er,an inode number,anda generation number; together, these three items comprise a unique identi.er for a.le or directory that a client wishes to access. The volume identi.er informs the server which .le system the request refers to (an NFS server can export more than one .le system); the inode number tells the server which .le within that partition the request is accessing. Finally, the generation number is needed when reusing an inode number; by in­crementing it whenever an inode number is reused, the server en­sures that a client with an old .le handle can’t accidentally access the newly-allocated .le. 
Here is a summary of some of the important pieces of the protocol; 
the full protocol is available elsewhere (see Callaghan’s book for an 
excellent and detailed overview of NFS [Sun89]). 
We’ll brie.y highlight some of the important components of the protocol. First, the LOOKUP protocol message is used to obtain a .le handle, which is then subsequently used to access .le data. The client passes a directory .le handle and name of a .le to look up, and the handle to that .le (or directory) plus its attributes are passed back to the client from the server. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
NFSPROC_GETATTR expects: file handle returns: attributes 
NFSPROC_SETATTR expects: file handle, attributes returns: nothing
NFSPROC_LOOKUP expects: directory file handle, name of file/directory to look up returns: file handle 
NFSPROC_READ expects: file handle, offset, count returns: data, attributes 
NFSPROC_WRITE expects: file handle, offset, count, data returns: attributes 
NFSPROC_CREATE expects: directory file handle, name of file to be created, attributes returns: nothing
NFSPROC_REMOVE expects: directory file handle, name of file to be removed returns: nothing
NFSPROC_MKDIR expects: directory file handle, name of directory to be created, attributes returns: file handle 
NFSPROC_RMDIR expects: directory file handle, name of directory to be removed returns: nothing
NFSPROC_READDIR expects: directory handle, count of bytes to read, cookie returns: directory entries, cookie (which can be used to get more entries) 
Figure 47.4: Some examples of the NFS Protocol 
For example, assume the client already has a directory .le han­dle for the root directory of a .le system (/)(indeed, this would be obtained through the NFS mount protocol,which is how clients and servers .rst are connected together; we do not discuss the mount protocol here for sake of brevity). If an application runningon the client tries to open the .le /foo.txt,the client-side .le system will send a lookup request to the server, passing it the root directory’s .le handle and the name foo.txt;if successful, the .le handle for foo.txt will be returned, along with its attributes. 
In case you are wondering, attributes are just the metadata that the .le system tracks about each .le, including .elds such as .le cre­ation time, last modi.cation time, size, ownership and permissions 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
information, and so forth, i.e., the same type of informationthat you would get back if you called stat() on a .le. 
Once a .le handle is available, the client can issue READ and WRITE protocol messages on a .le to read or write the .le, respec­tively. The READ protocol message requires the protocol to pass along the .le handle of the .le along with the offset within the.le and number of bytes to read. The server then will be able to issue the read (after all, the handle tells the server which volume and which inode to read from, and the offset and count tells it which bytes of the .le to read) and return the data to the client (or an error ifthere was a failure). WRITE is handled similarly, except the data ispassed from the client to the server, and just a success code is returned. 
One last interesting protocol message is the GETATTR request; given a .le handle, it simply fetches the attributes for that .le, in­cluding the last modi.ed time of the .le. We will see why this pro­tocol request is quite important in NFSv2 below when we discuss caching (see if you can guess why). 

47.6 From Protocol to Distributed File System 
Hopefully you are now getting some sense of how this protocol is turned into a .le system across the client-side .le system and the .le server. The client-side .le system tracks open .les, and generally translates application requests into the relevant set of protocol mes­sages. The server simply responds to each protocol message, each of which has all the information needed to complete request. 
For example, let us consider the a simple application which reads a.le. In the diagram (Figure 47.5), we show whatsystem calls the ap­plication makes, and what the client-side .le system and .le server do in responding to such calls. 
Afew comments about the .gure. First, notice how the client tracks all relevant state for the .le access, including the mapping of the integer .le descriptor to an NFS .le handle as well as the current .le pointer. This enables the client to turn each read request(which you may have noticed do not specify the offset to read from explic­itly) into a properly-formatted read protocol message whichtells the server exactly which bytes from the .le to read. Upon a successful read, the client updates the current .le position; subsequent reads are issued with the same .le handle but a different offset. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
App fd = open("/foo", ...);Client Send LOOKUP (root dir file handle, "foo")Server Receive LOOKUP request Server look for "foo" in root dir Server if successful, pass back foo’s file handle/attributesClient Receive LOOKUP replyClient use attributes to do permissions check Client if OK to access file, allocate file desc. in "open file table"; Client store NFS file handle therein Client store current file position (0 to begin)Client return file descriptor to applicationApp read(fd, buffer, MAX);Client Use file descriptor to index into open file table Client thus find the NFS file handle for this file Client use the current file position as the offset to read from Client Send READ (file handle, offset=0, count=MAX)Server Receive READ request Server file handle tells us which volume/inode number we need Server may have to read the inode from disk (or cache) Server use offset to figure out what block to read, Server and inode (and related structures) to find it Server issue read to disk (or get from server memory cache) Server return data (if successful) to client Client Receive READ replyClient Update file position to current + bytes read Client set current file position = MAX Client return data and error code to applicationApp read(fd, buffer, MAX);
(Same as above, except offset=MAX and set current file position = 2*MAX)App read(fd, buffer, MAX);
(Same as above, except offset=2*MAX and set current file position = 3*MAX)App close(fd);Client Just need to clean up local structures Client Free descriptor "fd" in open file table for this process Client (No need to talk to server) 
Figure 47.5: Reading A File: Client-side and File Server Actions 
Second, you may notice where server interactions occur. When the .le is opened for the .rst time, the client-side .le systemsends a LOOKUP request message. Indeed, if a long pathname must be tra­versed (e.g., /home/remzi/foo.txt), the client would send three LOOKUPs: one to look up home in the directory /,one to look up remzi in home,and .nally one to look up foo.txt in remzi. 
Third, you may notice how each server request has all the infor­mation needed to complete the request in its entirety. This design point is critical to be able to gracefully recover from serverfailure, as we will now discuss. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
DESIGN TIP:IDEMPOTENCY 
Idempotency is a useful property when building reliable systems. 
When an operation can be issued more than once, it is much easier to 
handle failure of the operation; you can just retry it. If an operation 
is not idempotent, life becomes more dif.cult. 

47.7 Handling Server Failure with Idempotent Operations 
When a client sends a message to the server, it sometimes does not receive a reply. There are many possible reasons for this failure to re­spond. In some cases, the message may be dropped by the network; networks do lose messages, and thus either the request or the reply could be lost and thus the client would never receive a response. 
It is also possible that the server has crashed, and thus is notcur­rently responding to messages. After a bit, the server will bere­booted and start running again, but in the meanwhile all requests have been lost. In all of these cases, clients are left with a question: what should they do when the server does not reply in a timely man­ner? 
In NFSv2, a client handles all of these failures in a single, uniform, and elegant way: it simply retries the request. Speci.cally, after send­ing the request, the client sets a timer to go off after a speci.ed time period. If a reply is received before the timer goes off, the timer is canceled and all is well. If, however, the timer goes off before any re­ply is received, the client assumes the request has not been processed and resends the request. If this time the server replies, all is well and the client has neatly handled the problem. 
The key to the ability of the client to simply retry the requestre­gardless of what caused the failure is due to an important property of most NFS requests: they are idempotent.An operation is called idempotent when the effect of performing the operation multiple times is equivalent to the effect of performing the operatinga single time. For example, if you store a value to a memory location three times, it is the same as doing so once; thus “store value to memory” is an idempotent operation. If, however, you increment a counter three times, it results in a different amount than doing so just once; thus, “increment counter” is not idempotent. More generally, any 
OPERATING SYSTEMS ARPACI-DUSSEAU 
operation that just reads data is obviously idempotent; an operation that updates data must be more carefully considered to determine if it has this property. 
The key to the design of crash recovery in NFS is the idempotency of most of the common operations. LOOKUP and READ requests are trivially idempotent, as they only read information fromthe .le server and do not update it. More interestingly, WRITE requests are also idempotent. If, for example, a WRITE fails, the client can simply retry it. Note how the WRITE message contains the data, the count, and (importantly) the exact offset to write the data to. Thus,it can be repeated with the knowledge that the outcome of multiple writes is the same as the outcome of a single write. 
Case 1: Request Lost 
Client Server [send request] (no mesg) 

Case 2: Server Down 

Case 3: Reply lost on way back from Server 

Figure 47.6: The Three Types of Loss 
In this way, the client can handle all timeouts in a uni.ed way.If aWRITE request was simply lost(Case 1 above), the clientwillretry it, the server will perform the write, and all will be well. Thesame will happen if the server happened to be down while the requestwas sent, but back up and running when the second request is sent, and again all works as desired (Case 2). Finally, the server may infact 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
receive the WRITE request, issue the write to its disk, and send a reply. This reply may get lost (Case 3), again causing the client to re-send the request. When the server receives the request again,it will simply do the exact same thing: write the data to disk and replythat it has done so. If the client this time receives the reply, all is again well, and thus the client has handled both message loss and server failure in a uniform manner. Neat! 
Asmall aside: some operations are hard to make idempotent. For example, when you try to make a directory that already exists,you are informed that the mkdir request has failed. Thus, in NFS, if the .le server receives a MKDIR protocol message and executes it suc­cessfully but the reply is lost, the client may repeat it and encounter that failure when in fact the operation at .rst succeeded and then only failed on the retry. Thus, life is not perfect. 
ASIDE:SOMETIMES LIFE ISN’T PERFECT 
Even when you design a beautiful system, sometimes all the cor­ner cases don’t work out exactly as you might like. Take the mkdir example above; one could redesign mkdir to have different seman­tics, thus making it idempotent (think about how you might do so); however, why bother? The NFS design philosophy covers most of the important cases, and overall makes the system design clean and simple with regards to failure. Thus, accepting that life isn’t perfect and still building the system is a sign of good engineering. Remem­ber Ivan Sutherland’s old saying: “the perfect is the enemy ofthe good.” 

47.8 Improving Performance: Client-side Caching 
Distributed .le systems are good for a number of reasons, but sending all read and write requests across the network can lead to a big performance problem: the network generally isn’t that fast, espe­cially as compared to local memory or disk. Thus, another problem: how can we improve the performance of a distributed .le system? 
The answer, as you might guess from reading the big bold words 
in the sub-heading above, is client-side caching.The NFS client-side 
.le system caches .le data (and metadata) that it has read fromthe 
OPERATING SYSTEMS ARPACI-DUSSEAU 
server in client memory. Thus, while the .rst access is expensive (i.e., it requires network communication), subsequent accesses are serviced quite quickly out of client memory. 
The cache also serves as a temporary buffer for writes. When a client application .rst writes to a .le, the client buffers the data in client memory (in the same cache as the data it read from the .le server) before writing the data out to the server. Such write buffer­ing is useful because it decouples application write() latency from ac­tual write performance, i.e., the application’s call to write() succeeds immediately (and just puts the data in the client-side .le system’s cache); only later does the data get written out to the .le server. 
Thus, NFS clients cache data and performance is usually greatand we are done, right? Unfortunately, not quite. Adding cachinginto any sort of system with multiple client caches introduces a big and interesting challenge which we will refer to as the cache consistency problem. 

47.9 The Cache Consistency Problem 
The cache consistency problem is best illustrated with two clients and a single server. Imagine client C1 reads a .le F, and keeps acopy of the .le in its local cache. Now imagine a different client, C2, over­writes the .le F, thus changing its contents; let’s call the new version of the .le F (version 2), or F[v2] and the old version F[v1] so wecan keep the two distinct (but of course the .le has the same name, just different contents). Finally, there is a third client, C3, which has not yet accessed the .le F. 
C1 C2 C3 
cache: F[v1] cache: F[v2] cache: empty 

Server S  
disk: F[v1] at first 
 F[v2] eventually  
Figure 47.7: The Cache Consistency Problem  
You can probably see the problem that is upcoming (Figure 47.7).  
In fact, there are two subproblems. The .rst subproblem is that the  
THREE  
ARPACI-DUSSEAU  EASY PIECES  
(V0.5)  

client C2 may buffer its writes in its cache for a time before propagat­ing them to the server; in this case, while F[v2] sits in C2’s memory, any access of F from another client (say C3) will fetch the old ver­sion of the .le (F[v1]). Thus, by buffering writes at the client, other clients may get stale versions of the .le, which may be undesirable; indeed, imagine the case where you log into machine C2, updateF, and then log into C3 and try to read the .le, only to get the old copy! Certainly this could be frustrating. Thus, let us call this aspect of the cache consistency problem update visibility;when do updates from one client become visible at other clients? 
The second subproblem of cache consistency is a stale cache;in this case, C2 has .nally .ushed its writes to the .le server, and thus the server has the latest version (F[v2]). However, C1 still has F[v1] in its cache; if a program running on C1 reads .le F, it will get astale version (F[v1]) and not the most recent copy (F[v2]). Again, this may result in undesirable behavior. 
NFSv2 implementations solve these cache consistency problems 
in two ways. First, to address update visibility, clients implement 
what is sometimes called .ush-on-close consistency semantics; specif­
ically, when a .le is written to and subsequently closed by a client ap­
plication, the client .ushes all updates (i.e., dirty pages in the cache) 
to the server. With .ush-on-close consistency, NFS tries to ensure 
that an open from another node will see the latest .le version. 
Second, to address the stale-cache problem, NFSv2 clients .rst check to see whether a .le has changed before using its cached con­tents. Speci.cally, when opening a .le, the client-side .le system will issue a GETATTR request to the server to fetch the .le’s attributes. The attributes, importantly, include information as to whenthe .le was last modi.ed on the server; if the time-of-modi.cation ismore recent than the time that the .le was fetched into the client cache, the client invalidates the .le, thus removing it from the client cache and ensuring that subsequent reads will go to the server and retrieve the latest version of the .le. If, on the other hand, the client sees that it has the latest version of the .le, it will go ahead and use the cached contents, thus increasing performance. 
When the original team at Sun implemented this solution to the stale-cache problem, they realized a new problem; suddenly,the NFS server was .ooded with GETATTR requests. A good engineering principle to follow is to design for the common case,and to make it work well; here, although the common case was that a .le was 
OPERATING SYSTEMS ARPACI-DUSSEAU 
accessed only from a single client (perhaps repeatedly), theclient al­ways had to send GETATTR requests to the server to make sure no one else had changed the .le. A client thus bombards the server, constantly asking “has anyone changed this .le?”, when most of the time no one had. 
To remedy this situation (somewhat), an attribute cache was added 
to each client. A client would still validate a .le before accessing it, 
but most often would just look in the attribute cache to fetch the at­
tributes. The attributes for a particular .le were placed in the cache 
when the .le was .rst accessed, and then would timeout after a cer­
tain amount of time (say 3 seconds). Thus, during those three sec­
onds, all .le accesses would determine that it was OK to use the 
cached .le and thus do so with no network communication with the 
server. 

47.10 Assessing NFS Cache Consistency 
Afew .nal words about NFS cache consistency. The .ush-on­close behavior was added to “make sense”, but introduced a cer­tain performance problem. Speci.cally, if a temporary or short-lived .le was created on a client and then soon deleted, it would still be forced to the server. A more ideal implementation might keep such short-lived .les in memory until they are deleted and thus remove the server interaction entirely, perhaps increasing performance. 
More importantly, the addition of an attribute cache into NFS made it very hard to understand or reason about exactly what ver­sion of a .le one was getting. Sometimes you would get the latest version; sometimes you would get an old version simply because your attribute cache hadn’t yet timed out and thus the client was happy to give you what was in client memory. Although this was .ne most of the time, it would (and still does!) occasionally lead to odd behavior. 
And thus we have described the oddity that is NFS client caching. Whew! 

47.11 Implications on Server-Side Write Buffering 
Our focus so far has been on client caching, and that is where most of the interesting issues arise. However, NFS servers tend to 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
be well-equipped machines with a lot of memory too, and thus they have caching concerns as well. When data (and metadata) is read from disk, NFS servers will keep it in memory, and subsequent reads of said data (and metadata) will not have to go to disk, a potential (small) boost in performance. 
More intriguing is the case of write buffering. NFS servers abso­lutely may not return success on a WRITE protocol request until the write has been forced to stable storage (e.g., to disk or some other persistent device). While they can place a copy of the data in server memory, returning success to the client on a WRITE protocol request could result in incorrect behavior; can you .gure out why? 
The answer lies in our assumptions about how clients handle server 
failure. Imagine the following sequence of writes as issued by a 
client: 
write(fd, a_buffer, size); // fill first block with a’swrite(fd, b_buffer, size); // fill second block with b’swrite(fd, c_buffer, size); // fill third block with c’s 
These writes overwrite the three blocks of a .le with a block of a’s, then b’s, and then c’s. Thus, if the .le initially looked like this: 
xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz 
we might expect the .nal result after these writes to be like this: 
aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb cccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc 
The x’s, y’s, and z’s, would be overwritten with a’s, b’s, and c’s, respectively. 
Now let’s assume for the sake of the example that these three client writes were issued to the server as three distinct WRITE pro­tocol messages. Assume the .rst WRITE message is received by the server and issued to the disk, and the client informed of its success. Now assume the second write is just buffered in memory, and the server also reports it success to the client before forcing it to disk; un­fortunately, the server crashes before writing it to disk. The server quickly restarts and receives the third write request, whichalso suc­ceeds. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
Thus, to the client, all the requests succeeded, but we are sur­prised that the .le contents look like this: 
aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy <---oops cccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc 
Yikes! Because the server told the client that the second write was successful before committing it to disk, an old chunk is left in the .le, which, depending on the application, might result in a completely useless .le. 
To avoid this problem, NFS servers must commit each write to sta­ble (persistent) storage before informing the client of success; doing so enables the client to detect server failure during a write,and thus retry until it .nally succeeds. Doing so ensures we will neverend up with .le contents intermingled as in the above example. 
The problem that this requirement gives rise to in NFS server im­plementation is that write performance, without great care,can be the major performance bottleneck. Indeed, some companies (e.g., Network Appliance) came into existence with the simple objective of building an NFS server that can perform writes quickly; onetrick they use is to .rst put writes in a battery-backed memory, thusen­abling to quickly reply to WRITE requests without fear of losing the data and without the cost of having to write to disk right away;the second trick is to use a .le system design speci.cally designed to write to disk quickly when one .nally needs to do so [HLM94,RO91]. 

47.12 Summary 
We have seen the introduction of the NFS distributed .le system. NFS is centered around the idea of simple and fast recovery in the face of server failure, and achieves this end through carefulprotocol design. Idempotency of operations is essential; because a client can safely replay a failed operation, it is OK to do so whether or not the server has executed the request. 
We also have seen how the introduction of caching into a multiple-client, single-server system can complicate things. In particular, the system must resolve the cache consistency problem in order tobe­have reasonably; however, NFS does so in a slightly ad hoc fashion which can occasionally result in observably weird behavior.Finally, 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
we saw how caching on the server can be tricky; in particular, writes to the server must be forced to stable storage before returning success (otherwise data can be lost). 
OPERATING SYSTEMS ARPACI-DUSSEAU 

References 
[S86] “The Sun Network File System: Design, Implementation and Experience” Russel Sandberg USENIX Summer 1986 
The original NFS paper. Frankly, it is pretty poorly written and makes some of the behaviors of NFS hard to understand. 
[P+94] “NFS Version 3: Design and Implementation” 
Brian Pawlowski, Chet Juszczak, Peter Staubach, Carl Smith,Diane Lebel, Dave Hitz 
USENIX Summer 1994. 137-152 

[P+00] “The NFS version 4 protocol” 
Brian Pawlowski, David Noveck, David Robinson, Robert Thurlow 
Proceedings of the 2nd International System Administrationand Networking Confer­ence (SANE 2000). 

[4] “NFS Illustrated” Brent Callaghan Addison-Wesley Professional Computing Series, 2000 
Agreat NFS reference. 
[Sun89] “NFS: Network File System Protocol Speci.cation” 
Sun Microsystems, Inc. Request for Comments: 1094. March 1989 
Available: http://www.ietf.org/rfc/rfc1094.txt 

[O91] “The Role of Distributed State” 
John K. Ousterhout 
Available: ftp://ftp.cs.berkeley.edu/ucb/sprite/papers/state.ps 

[HLM94] “File System Design for an NFS File Server Appliance” 
Dave Hitz, James Lau, Michael Malcolm 
USENIX Winter 1994. San Francisco, California, 1994 
Hitz et al. were greatly in.uenced by previous work on log-structured .le systems. 

[RO91] “The Design and Implementation of the Log-structuredFile System” 
Mendel Rosenblum, John Ousterhout 
Symposium on Operating Systems Principles (SOSP), 1991. 

THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
48 



The Andrew File System (AFS) 
The Andrew File System was introduced by researchers at Carnegie-Mellon University (CMU) in the 1980’s [H+88]. Led by the well-known Professor M. Satyanarayanan of Carnegie-Mellon University (“Satya” for short), the main goal of this project was simple: scale. Speci.cally, how can one design a distributed .le system suchthat a server can support as many clients as possible? 
Interestingly, as we will see, there are numerous design and im­plementation components that affect scalability. Most important is the design of the protocol between clients and servers. In NFS, for example, the protocol forces clients to check with the serverperi­odically to determine if cached contents have changed; because each check uses server resources (e.g., CPU, network bandwidth, etc.), fre­quent checks like this will limit the number of clients a server can respond to and thus limit scalability. 
48.1 AFS Version 1 
We will discuss two versions of AFS [H+88,S+85]. The .rst ver­sion (which we will call AFSv1, but actually the original system was called the ITC distributed .le system [S+85]) had some of the basic design in place, but didn’t scale as desired, which led to a re-design and the .nal protocol (which we will call AFSv2, or just AFS) [H+88]. We now discuss the .rst version. 
One of the basic tenets of all versions of AFS is whole-.le caching 
on the local disk of the client machine that is accessing a .le. When 
you open() a .le, the entire .le (if it exists) is fetched from the server 
623 and stored in a .le on your local disk. Subsequent applicationread() and write() operations are redirected to the local .le systemwhere the .le is stored; thus, these operations require no network commu­nication and are fast. Finally, upon close(), the .le (if it has been modi.ed) is .ushed back to the server. Note the obvious contrasts with NFS, which caches blocks (not whole .les, although NFS could of course cache every block of an entire .le) and does so in client memory (not local disk). 
Let’s get into the details a bit more. When a client application .rst calls open(), the AFS client-side code (which the AFS designers call Venus)would send a Fetch protocol message to the server. The Fetch message would pass the entire pathname (e.g., /home/remzi/notes.txt) of the desired .le to the .le server (the group of which they called Vice), which would then traverse the pathname, .nd the desired .le, and ship the entire .le back to the client. The client-side code would then cache the .le on the local disk of the client (by writing itto local disk). As we said above, subsequent read() and write() systemcalls are strictly local in AFS (no communication with the server occurs); they are just redirected to the local copy of the .le. Because the read() and write() calls act just like calls to a local .le system, once a block is accessed, it also may be cached in client memory. Thus, AFS also uses client memory to cache copies of blocks that it has in its local disk. Finally, when .nished, the AFS client checks if the .le has been modi.ed (i.e., that it has been opened for writing); if so, it .ushes the new version back to the server with a Store protocol message, send­ing the entire .le and pathname to the server for permanent storage. 
TestAuth Test whether a file has changed (used to validate cached entries) GetFileStat Get the stat info for a file Fetch Fetch the contents of an entire file from the server Store Store this file on the server SetFileStat Set the stat info for a file ListDir List the contents of a directory 
Figure 48.1: AFSv1 Protocol Highlights 
The next time the .le is accessed, AFSv1 does so much more ef.­ciently. Speci.cally, the client-side code .rst contacts the server (us­ing the TestAuth protocol message) in order to determine whether the .le has changed. If not, the client would use the locally-cached copy, thus improving performance by avoiding a network transfer. The .gure above shows some of the protocol messages in AFSv1. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
Note that this early version of the protocol only cached .le contents; directories, for example, were only kept at the server. 

48.2 Problems with Version 1 
Afew key problems with this .rst version of AFS motivated the designers to rethink their .le system. To study the problems in de­tail, the designers of AFS spent a great deal of time measuringtheir existing prototype to .nd what was wrong. Such experimentation is a good thing; measurement is the key to understanding how sys­tems work and how to improve them. Hard data helps take intuition and make into a concrete science of deconstructing systems. In their study, the authors found two main problems with AFSv1: 
• 	
Path-traversal costs are too high:When performing a Fetch or Store, the client passes the entire .le name (e.g., the .le/home/remzi/grades.txt)to the server. The server, in or­der to access the .le, must perform a full pathname traversal, .rst looking in the root directory to .nd home,then in home to .nd remzi,and so forth, all the way down the path until .nally the desired .le is located. With many clients accessing the server at once, the designers of AFS found that the server was spending much of its time simply walking down directory paths! 

• 	
The client issues too many TestAuths to the server:Much like NFS and its overabundance of GetAttr protocol messages, AFSv1 generated a large amount of traf.c to check whether a local .le (or its stat information) was valid with the TestAuth protocol message. Thus, servers spent a great deal of time telling clients whether it was OK to used their cached copies of a .le. Most of the time, it was OK (of course), and thus the protocol was leading to high server overheads again. 


There were actually two other problems with AFSv1: load was not balanced across servers, and the server used a single distinct process per client thus inducing context switching and otherover­heads. The load imbalance problem was solved by introducing vol­umes,which an administrator could move across servers to balance load; the context-switch problem was solved in AFSv2 by building 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
the server with threads instead of processes. However, for the sake of space, we focus here on the main two protocol problems above that limited the scale of the system. 

48.3 Improving the Protocol 
The two problems above limited the scalability of AFS; the server CPU became the bottleneck of the system, and each server couldonly service 20 clients without becoming overloaded. Servers were receiv­ing too many TestAuth messages, and when they received Fetch or Store messages, were spending too much time traversing the direc­tory hierarchy. Thus, the AFS designers were faced with a problem: 
THE CRUX:HOW TO DESIGN A PROTOCOL FOR SCALABILITY 
How should one redesign the protocol to minimize the number of server interactions, i.e., how could they reduce the number of Tes­tAuth messages? Further, how could they design the protocol to make these server interactions ef.cient? By attacking both of these issues, a new protocol would result in a much more scalable version AFS. 

48.4 AFS Version 2 
AFSv2 introduced the notion of a callback to reduce the number of client/server interactions. A callback is simply a promise from the server to the client that the server will inform the client when a .le that the client is caching has been modi.ed. By adding this state to the server, the client no longer needs to contact the server to.nd out if a cached .le is still valid; rather, it assumes that the .le is valid until the server tells it otherwise. 
AFSv2 also introduced the notion of a .le handle (very similar to NFS) instead of pathnames to specify which .le a client was inter­ested in. A .le handle in AFS consisted of a volume identi.er, a.le identi.er, and a generation number. Thus, instead of sendingwhole pathnames to the server and letting the server walk the pathname to .nd the desired .le, the client would walk the pathname, one piece at a time, caching the results and thus hopefully reducing theload 
OPERATING SYSTEMS ARPACI-DUSSEAU 
on the server. 
For example, if a client accessed /home/remzi/notes.txt,and home was the AFS directory mounted onto / (in other words, / was the local root directory, but home and its children were in AFS), the client would .rst Fetch the directory contents of home,put them in the local-disk cache, and setup a callback on home.Then, the client would Fetch the directory remzi,put it in the local-disk cache, and setup a callback on the server on remzi.Finally, the client would Fetch notes.txt,cache this regular .le in the local disk, setup a callback, and .nally return a .le descriptor to the calling application. 
The key difference, however, from NFS, is that with each fetch of a directory or .le, the AFS client would establish a callback with the server, thus ensuring that the server would notify the client of a change in its cached state. The bene.t is obvious: although the .rst access to /home/remzi/notes.txt generates many client-server messages (as described above), it also establishes callbacks for all the directories as well as the .le notes.txt, and thus subsequentaccesses are entirely local and require no server interaction at all. Thus, in the common case where a .le is cached at the client, AFS behaves nearly identically to a local disk-based .le system. If one accessesa .le more than once, the second access should be just as fast as accessing a .le locally. 

48.5 Cache Consistency 
Because of callbacks and whole-.le caching, the cache consistency model provided by AFS is easy to describe and understand. Whena client (C1) opens a .le, it will fetch it from the server. Any updates it makes to the .le are entirely local, and thus only visible toother applications on that same client (C1); if an application on another client (C2) opens the .le at this point, it will just get the version that is stored at the server which does not yet re.ect the changes being made at C1. When the application at C1 .nishes updating the .le, it calls close() which .ushes the entire .le to the server. At that point, any clients caching the .le (such as C2) would be informed thattheir callbacks are broken and thus they should not use cached versions of the .le because the server has a newer version. 
In the rare case that two clients are modifying a .le at the same time, AFS naturally employs what is known as a last writer wins 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
approach. Speci.cally, whichever client calls close() lastwill update the entire .le on the server last and thus will be the winning .le, i.e., the .le that remains on the server for others to see. The resultis a .le that is either one client’s or the other client’s. Note thedifference from a block-based protocol like NFS: in such a block-based protocol, writes of individual blocks may be .ushed out to the server as each client is updating the .le, and thus the .nal .le on the server could end up as a mix of updates from both clients; in many cases, sucha mixed .le output would not make much sense (i.e., imagine a JPEG image getting modi.ed by two clients in pieces; the resultingmixof writes would hardly make much sense). 

48.6 Crash Recovery 
From the description above, you might sense that crash recovery is more involved than with NFS. You would be right. For example, imagine there is a short period of time where a server (S) is notable to contact a client (C1), for example, while the client C1 is reboot­ing. While C1 is not available, S may have tried to send it one or more callback recall messages; for example, imagine C1 had .le F cached on its local disk, and then C2 (another client) updatedF,thus causing S to send messages to all clients caching the .le to remove it from their local caches. Because C1 may miss those critical messages when it is rebooting, upon rejoining the system, C1 should treat all of its cache contents as suspect. Thus, upon the next access to.le F, C1 should .rst ask the server (with a TestAuth protocol message) whether its cached copy of .le F is still valid; if so, C1 can useit; if not, C1 should fetch the newer version from the server. 
Server recovery after a crash is more complicated. The problem that arises is that callbacks are kept in-memory; thus, when aserver reboots, it has no idea which client machine has which .les. Thus, upon server restart, each client of the server must realize that the server has crashed and treat all of their cache contents as suspect, and (as above) reestablish the validity of a .le before using it. Thus, aserver crash is abig event, as one mustensure that each client is aware of the crash in a timely manner, or risk a client accessing a stale .le. There are many ways to implement such recovery; forex­ample, by having the server send a message (saying “don’t trust your cache contents!”) to each client when it is up and running again. As 
OPERATING SYSTEMS ARPACI-DUSSEAU 
you can see, there is a cost to building a more scalable and sensible caching model; with NFS, clients hardly noticed a server crash.  
48.7  Scale of AFSv2  
With the new protocol in place, AFSv2 was measured and found to be much more scalable that the original version. Indeed, each server could support about 50 clients (instead of just 20). A further bene.t was that client-side performance often came quite close to lo­cal performance, because in the common case, all .le accesseswere local; .le reads usually went to the local disk cache (and potentially, local memory). Only when a client created a new .le or wrote to an existing one was there need to send a Store message to the server and thus update the .le with new contents.  
48.8  Other Improvements: Namespaces, Security, Etc.  
AFS added a number of other improvements beyond scale. It pro­vided a true global namespace to clients, thus ensuring that all .les were named the same way on all client machines; NFS, in contrast, al­lowed each client to mount NFS servers in any way that they pleased, and thus only by convention (and great administrative effort) would .les be named similarly across clients. AFS also took security seriously, and incorporated mechanisms to authenticate users and ensure that a set of .les could be kept private if a user so desired. NFS, in contrast, still has quite primitive support for security. Finally, AFS also included facilities for .exible user-managed ac­cess control. Thus, when using AFS, a user has a great deal of control over who exactly can access which .les. NFS, like most UNIX .le systems, has much more primitive support for this type of sharing.  
48.9  Summary  
AFS shows us how distributed .le systems can be built quite dif­ferently than what we saw with NFS. The protocol design of AFS is particularly important; by minimizing server interactions(through whole-.le caching and callbacks), each server can support many clients  
ARPACI-DUSSEAU  THREE EASY PIECES (V0.5)  

and thus reduce the number of servers needed to manage a partic­ular site. Many other features, including the single namespace, se­curity, and access-control lists, make AFS quite nice to use.Finally, the consistency model provided by AFS is simple to understandand reason about, and does not lead to the occasional weird behavior as one sometimes observes in NFS. 
Perhaps unfortunately, AFS is likely on the decline. BecauseNFS became an open standard, many different vendors supported it, and, along with CIFS (the Windows-based distributed .le system proto­col), NFS dominates the marketplace. Although one still seesAFS installations from time to time (such as in various educational insti­tutions, including Wisconsin), the only lasting in.uence will likely be from the ideas of AFS rather than the actual system itself. Indeed, NFSv4 now adds server state (e.g., an “open” protocol message), and thus bears more similarity to AFS than it used to. 

References 
[H+88] “Scale and Performance in a Distributed File System” 
John H. Howard, Michael L. Kazar, Sherri G. Menees, David A. Nichols, M. Satya­narayanan, Robert N. Sidebotham, Michael J. West. 
ACM Transactions on Computing Systems (ACM TOCS), page 51-81, Volume 6, Number 
1, February 1988. 

[S+85] “The ITC Distributed File System: Principles and Design” 

M. Satyanarayanan, J.H. Howard, D.A. Nichols, R.N. Sidebotham, A. Spector, M.J. West. SOSP ’85. pages 35-50. 
OPERATING SYSTEMS ARPACI-DUSSEAU 


Part V 
Appendices 

631 
49 


ADialogue on Virtual Machine Monitors 
Student: So now we’re stuck in the Appendix, huh? 
Professor: Yes, just when you thought things couldn’t get any worse. 
Student: Well, what are we going to talk about? 
Professor: An old topic that has been reborn: virtual machine monitors, also known as hypervisors. 
Student: Oh, like VMware? That’s cool; I’ve used that kind of software before. 
Professor: Cool indeed. We’ll learn how VMMs add yet another layer of virtualization into systems, this one beneath the OS itself! Crazy and amazing stuff, really. 
Student: Sounds neat. Why not include this in the earlier part of the book, then, on virtualization? Shouldn’t it really go there? 
Professor: That’s above our pay grade, I’m afraid. But my guess is this: there is already a lot of material there. By moving this small aside on VMMs into the appendix, a particular instructor can choose whether to include it or skip it. But I do think it should be included, because if you canunderstand how VMMs work, then you really understand virtualization quite well. 
Student: Alright then, let’s get to work! 
633 
50 


Virtual Machine Monitors 
50.1 Introduction 
Years ago, IBM sold expensive mainframes to large organizations, and a problem arose: what if the organization wanted to run different operating systems on the machine at the same time? Some applica­tions had been developed on one OS, and some on others, and thus the problem. As a solution, IBM introduced yet another level of indi­rection in the form of a virtual machine monitor (VMM)(also called a hypervisor)[G74]. 
Speci.cally, the monitor sits between one or more operating sys­tems and the hardware and gives the illusion to each running OSthat it controls the machine. Behind the scenes, however, the monitor actually is in control of the hardware, and must multiplex running OSes across the physical resources of the machine. Indeed, the VMM serves as an operating system for operating systems, but at a much lower level; the OS must still think it is interacting with thephysical hardware. Thus, transparency is a major goal of VMMs. 
Thus, we .nd ourselves in a funny position: the OS has thus far served as the master illusionist, tricking unsuspecting applications into thinking they have their own private CPU and a large virtual memory, while secretly switching between applications and sharing memory as well. Now, we have to do it again, but this time under­neath the OS, who is used to being in charge. How can the VMM create this illusion for each OS running on top of it? 
635 
THE CRUX: HOW TO VIRTUALIZE THE MACHINE UNDERNEATH THE OS 
The virtual machine monitor must transparently virtualize the 
machine underneath the OS; what are the techniques required to do 
so? 

50.2 Motivation: Why VMMs? 
Today, VMMs have become popular again for a multitude of rea­sons. Server consolidation is one such reason. In many settings, peo­ple run services on different machines which run different operating systems (or even OS versions), and yet each machine is lightlyuti­lized. In this case, virtualization enables an administrator to consol­idate multiple OSes onto fewer hardware platforms, and thus lower costs and ease administration. 
Virtualization has also become popular on desktops, as many users 
wish to run one operating system (say Linux or Mac OS X) but still 
have access to native applications on a different platform (say Win­
dows). This type of improvement in functionality is also a good 
reason. 
Another reason is testing and debugging. While developers write code on one main platform, they often want to debug and test it on the many different platforms that they deploy the software toin the .eld. Thus, virtualization makes it easy to do so, by enablinga de­veloper to run many operating system types and versions on just one machine. 
This resurgence in virtualization began in earnest the mid-to-late 1990’s, and was led by a group of researchers at Stanford headed by Professor Mendel Rosenblum. His group’s work on Disco [B+97], a virtual machine monitor for the MIPS processor, was an early effort that revived VMMs and eventually led that group to the founding of VMware [V98], now a market leader in virtualization technology. In this chapter, we will discuss the primary technology underlying Disco and through that window try to understand how virtualiza­tion works. 

50.3 Virtualizing the CPU 
OPERATING SYSTEMS ARPACI-DUSSEAU 
To run a virtual machine (e.g., an OS and its applications) on top  
of a virtual machine monitor, the basic technique that is usedis lim­ 
ited direct execution,a technique we saw before when discussing  
how the OS virtualizes the CPU. Thus, when we wish to “boot” a  
new OS on top of the VMM, we simply jump to the address of the  
.rst instruction and let the OS begin running. It is as simple as that  
(well, almost).  
Assume we are running on a single processor, and that we wish to  
multiplex between two virtual machines, that is, between twoOSes  
and their respective applications. In a manner quite similarto an  
operating system switching between running processes (a context  
switch), a virtual machine monitor must perform a machine switch  
between running virtual machines. Thus, when performing such a  
switch, the VMM must save the entire machine state of one OS (in­ 
cluding registers, PC, and unlike in a context switch, any privileged  
hardware state), restore the machine state of the to-be-run VM, and  
then jump to the PC of the to-be-run VM and thus complete the  
switch. Note that the to-be-run VM’s PC may be within the OS it­ 
self (i.e., the system was executing a system call) or it may simply be  
within a process that is running on that OS (i.e., a user-mode appli­ 
cation).  
We get into some slightly trickier issues when a running applica­ 
tion or OS tries to perform some kind of privileged operation.For  
example, on a system with a software-managed TLB, the OS will use  
special privileged instructions to update the TLB with a translation  
before restarting an instruction that suffered a TLB miss.  Ina vir­ 
tualized environment, the OS cannot be allowed to perform privi­ 
leged instructions, because then it controls the machine rather than  
the VMM beneath it. Thus, the VMM must somehow intercept at­ 
tempts to perform privileged operations and thus retain control of  
the machine.  
Asimple example of how a VMM must interpose on certain op­ 
erations arises when a running process on a given OS tries to make a system call. For example, the process may be trying to call open() on a .le, or may be calling read() to get data from it, or may be calling fork() to create a new process. In a system without virtual­ 
ization, a system call is achieved with a special instruction; on MIPS, it is a trap instruction, and on x86, it is the int (an interrupt) in­struction with the argument 0x80.Here is the open library call on  
FreeBSD [B00] (recall that your C code .rst makes a library call into  
THREE  
ARPACI-DUSSEAU  EASY PIECES  
(V0.5)  


the C library, which then executes the proper assembly sequence to actually issue the trap instruction and thus make a system call):
open:push dword mode push dword flags push dword path mov eax, 5 push eax int 80h 
On UNIX-basedsystems, open() takes three arguments: int open(char *path, int flags, mode tmode). You can see in the code above how the open() library call is implemented: .rst, the argu­ments get pushed onto the stack (mode, flags, path), then a 5 gets pushed onto the stack, and then int 80h is called, which trans­fers control to the kernel. The 5, if you were wondering, is thepre­agreed upon convention between user-mode applications and the kernel for the open() system call in FreeBSD; different system calls would place different numbers onto the stack (in the same position) before calling the trap instruction int and thus making the system 
call1. 
Process Hardware Operating System 
1. 
Execute instructions 
(add, load, etc.) 


2. 
System call: 
Trap to OS 



3. 
Switch to kernel mode; Jump to trap handler 


4. In kernel mode; Handle system call; Return from trap 
5. 
Switch to user mode; Return to user code 

6. Resume execution 
(@PC after trap) 

Table 50.1: Executing a System Call
When a trap instruction is executed, as we’ve discussed before, it usually does a number of interesting things. Most important in our example here is that it .rst transfers control (i.e., changesthe PC) to awell-de.ned trap handler within the operating system. The OS, when it is .rst starting up, establishes the address of such a routine 
1Just to make things confusing, the Intel folks use the term “interrupt” for what 
almost any sane person would call a trap instruction. As Patterson said about the Intel 
instruction set: “It’s an ISA only a mother could love.” 
OPERATING SYSTEMS ARPACI-DUSSEAU 
with the hardware (also a privileged operation) and thus uponsub­sequent traps, the hardware knows where to start running codeto handle the trap. At the same time of the trap, the hardware alsodoes one other crucial thing: it changes the mode of the processor from user mode to kernel mode.In user mode, operations are restricted, and attempts to perform privileged operations will lead to a trap and likely the termination of the offending process; in kernel mode, on the other hand, the full power of the machine is available, andthus all privileged operations can be executed. Thus, in a traditional set­ting (again, without virtualization), the .ow of control would be like what you see in Table 50.1. 
Process Operating System 
1. 
System call: 
Trap to OS 



2. OS trap handler: Decode trap and execute appropriate syscall routine; When done: return from trap 
3. 
Resume execution 
(@PC after trap) 


Table 50.2: System Call Flow Without Virtualization 
On a virtualized platform, things are a little more interesting. When an application running on an OS wishes to perform a system call, it does the exact same thing: executes a trap instruction with the ar­guments carefully placed on the stack (or in registers). However, it is the VMM that controls the machine, and thus the VMM who has installed a trap handler that will .rst get executed in kernelmode. 
So what should the VMM do to handle this system call? The VMM doesn’t really know how to handle the call; after all, it does not know the details of each OS that is running and therefore does not know what each call should do. What the VMM does know, how­ever, is where the OS’s trap handler is. It knows this because when the OS booted up, it tried to install its own trap handlers; when the OS did so, it was trying to do something privileged, and therefore trapped into the VMM; at that time, the VMM recorded the neces­sary information (i.e., where this OS’s trap handlers are in memory). Now, when the VMM receives a trap from a user process running on the given OS, it knows exactly what to do: it jumps to the OS’s trap handler and lets the OS handle the system call as it should. When the OS is .nished, it executes some kind of privileged instruction 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
to return from the trap (rett on MIPS, iret on x86), which again bounces into the VMM, which then realizes that the OS is tryingto return from the trap and thus performs a real return-from-trap and thus returns control to the user and puts the machine back in user mode. The entire process is depicted in Tables 50.2 and 50.3, both for the normal case without virtualization and the case with virtualiza­tion (we leave out the exact hardware operations from above tosave space). 
Process Operating System VMM 
1. 
System call: 
Trap to OS 



2. Process trapped: Call OS trap handler (at reduced privilege) 
3. OS trap handler: Decode trap and execute syscall; When done: issue return-from-trap 
4. OS tried return from trap: Do real return from trap 
5. 
Resume execution 
(@PC after trap) 


Table 50.3: System Call Flow with Virtualization 
As you can see from the .gures, a lot more has to happen when virtualization is going on. Certainly, because of the extra jumping around, virtualization might indeed slow down system calls and thus could hurt performance. 
You might also notice that we have one remaining question: what mode should the OS run in? It can’t run in kernel mode, because then it would have unrestricted access to the hardware. Thus, it must run in some less privileged mode than before, be able to accessits own data structures, and simultaneously prevent access to its data structures from user processes. 
In the Disco work, Rosenblum and colleagues handled this prob­lem quite neatly by taking advantage of a special mode provided by the MIPS hardware known as supervisor mode. When running in this mode, one still doesn’t have access to privileged instructions, but one can access a little more memory than when in user mode; the OS can use this extra memory for its data structures and allis well. On hardware that doesn’t have such a mode, one has to run the OS in user mode and use memory protection (page tables and TLBs) 
OPERATING SYSTEMS ARPACI-DUSSEAU 
to protect OS data structures appropriately. In other words,when switching into the OS, the monitor would have to make the memory of the OS data structures available to the OS via page-table protec­tions; when switching back to the running application, the ability to read and write the kernel would have to be removed. 
OS Page Table VMM Page Table 
VPN 0 to PFN 10 PFN 03 to MFN 06 VPN 2 to PFN 03 PFN 08 to MFN 10 VPN 3 to PFN 08 PFN 10 to MFN 05 
Virtual Address Space "Physical Memory" Machine Memory 
0 

00 1 
11 2 
22 3 
33 
44 
5 

5 6 
6 7 
7 8 
8 9 
9 10 
10 11 11 12 12 13 13 14 14 15 15 16 17 18 19 20 21 22 23 
Figure 50.1: VMM Memory Virtualization 

50.4 Virtualizing Memory 
You should now have a basic idea of how the processor is vir­tualized: the VMM acts like an OS and schedules different virtual machines to run, and some interesting interactions occur when priv­ilege levels change. But we have left out a big part of the equation: how does the VMM virtualize memory? 
Each OS normally thinks of physical memory as a linear array of pages, and assigns each page to itself or user processes. The OS itself, of course, already virtualizes memory for its running processes, such that each process has the illusion of its own private address space. Now we must add another layer of virtualization, so that multiple OSes can share the actual physical memory of the machine, and we must do so transparently. 
This extra layer of virtualization makes “physical” memory avir-
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
tualization on top of what the VMM refers to as machine memory, which is the real physical memory of the system. Thus, we now have an additional layer of indirection: each OS maps virtual-to-physical addresses via its per-process page tables; the VMM maps the result­ing physical mappings to underlying machine addresses via its per-OS page tables. Figure 50.1 depicts this extra level of indirection. 
In the .gure, there is just a single virtual address space withfour pages, three of which are valid (0, 2, and 3). The OS uses its page table to map these pages to three underlying physical frames (10, 3, and 8, respectively). Underneath the OS, the VMM performs a further level of indirection, mapping PFNs 3, 8, and 10 to machine frames 6, 10, and 5 respectively. Of course, this picture simpli.es things quite a bit; on a real system, there would be V operating sys­tems running (with V likely greater than one), and thus V VMM page tables; further, on top of each running operating systemOSi, there would be a number of processes Pi running (Pi likely in the tens or hundreds), and hence Pi (per-process) page tables within OSi. 
To understand how this works a little better, let’s recall how ad­dress translation works in a modern paged system. Speci.cally, let’s discuss what happens on a system with a software-managed TLB during address translation. Assume a user process generatesan ad­dress (for an instruction fetch or an explicit load or store);by de.ni­tion, the process generates a virtual address,as itsaddress space has been virtualized by the OS. As you know by now, it is the role of the OS, with help from the hardware, to turn this into a physical address and thus be able to fetch the desired contents from physical memory. 
Assume we have a 32-bit virtual address space and a 4-KB page size. Thus, our 32-bit address is chopped into two parts: a 20-bit virtual page number (VPN), and a 12-bit offset. The role of theOS, with help from the hardware TLB, is to translate the VPN into a valid physical page frame number (PFN) and thus produce a fully-formed physical address which can be sent to physical memory to fetchthe proper data. In the common case, we expect the TLB to handle the translation in hardware, thus making the translation fast. When a TLB miss occurs (at least, on a system with a software-managedTLB), the OS must get involved to service the miss, as depicted here in Ta­ble 50.4. 
As you can see, a TLB miss causes a trap into the OS, which han­
dles the fault by looking up the VPN in the page table and installing 
the translation in the TLB. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
Process Operating System 
1. Load from memory: 
TLB miss: Trap 

3. Resume execution 
(@PC of trapping instruction); 
Instruction is retried; 
Results in TLB hit 

2. OS TLB miss handler: 
Extract VPN from VA; 
Do page table lookup; 
If present and valid: 
get PFN, update TLB; 
Return from trap 

Table 50.4: TLB Miss Flow without Virtualization  
With a virtual machine monitor underneath the OS, however, things  
again get a little more interesting. Let’s examine the .ow of aTLB  
miss again (see Table 50.5 for a summary). When a process makesa  
virtual memory reference and misses in the TLB, it is not the OSTLB  
miss handler that runs; rather, it is the VMM TLB miss handler,as  
the VMM is the true privileged owner of the machine. However, in  
the normal case, the VMM TLB handler doesn’t know how to handle  
the TLB miss, so it immediately jumps into the OS TLB miss handler;  
the VMM knows the location of this handler because the OS, during  
“boot”, tried to install its own trap handlers. The OS TLB misshan­ 
dler then runs, does a page table lookup for the VPN in question, and  
tries to install the VPN-to-PFN mapping in the TLB. However, doing  
so is a privileged operation, and thus causes another trap into the  
VMM (the VMM gets noti.ed when any non-privileged code tries to  
do something that is privileged, of course). At this point, the VMM  
plays its trick: instead of installing the OS’s VPN-to-PFN mapping,  
the VMM installs its desired VPN-to-MFN mapping. After doingso,  
the system eventually gets back to the user-level code, whichretries  
the instruction, and results in a TLB hit, fetching the data from the  
machine frame where the data resides.  
This set of actions also hints at how a VMM must manage the vir­ 
tualization of physical memory for each running OS; just likethe OS  
has a page table for each process, the VMM must track the physical- 
to-machine mappings for each virtual machine it is running. These  
per-machine page tables need to be consulted in the VMM TLB miss  
handler in order to determine which machine page a particular“phys­ 
ical” page maps to, and even, for example, if it is present in machine  
memory at the current time (i.e., the VMM could have swapped itto  
THREE  
ARPACI-DUSSEAU  EASY PIECES  
(V0.5)  

Process Operating System Virtual Machine Monitor 
1. 
Load from memory 
TLB miss: Trap 



2. VMM TLB miss handler: Call into OS TLB handler (reducing privilege) 
3. 
OS TLB miss handler: 
Extract VPN from VA; 
Do page table lookup; 
If present and valid, 
get PFN, update TLB 



4. Trap handler: 
Unprivileged code trying to 
update the TLB; 
OS is trying to install 
VPN-to-PFN mapping; 
Update TLB instead with 
VPN-to-MFN (privileged); 
Jump back to OS 
(reducing privilege) 

5. 
Return from trap 

6. Trap handler: Unprivileged code trying to return from a trap; Return from trap 
7. 
Resume execution 
(@PC of instruction); 
Instruction is retried; 
Results in TLB hit 


Table 50.5: TLB Miss Flow with Virtualization 
disk to reduce memory pressure). 
ASIDE:HARDWARE-MANAGED TLBS 
Our discussion has centered around software-managed TLBs and the work that needs to be done when a miss occurs. But you might be wondering: how does the virtual machine monitor get involvedwith ahardware-managed TLB? In those systems, the hardware walks the page table on each TLB miss and updates the TLB as need be, and thus the VMM doesn’t have a chance to run on each TLB miss to sneak its translation into the system. Instead, the VMM must closely monitor changes the OS makes to each page table (which, in a hardware-managed system, is pointed to by a page-table base 
OPERATING SYSTEMS ARPACI-DUSSEAU 
register of some kind), and keep a shadow page table that instead maps the virtual addresses of each process to the VMM’s desired machine pages [AA06]. The VMM installs a process’s shadow page table whenever the OS tries to install the process’s OS-levelpage ta­ble, and thus the hardware chugs along, translating virtual addresses to machine addresses using the shadow table, without the OS even noticing. 
Finally, as you might notice from this sequence of operations, TLB misses on a virtualized system become quite a bit more expensive than in a non-virtualized system. To reduce this cost, the designers of Disco added a VMM-level “software TLB”. The idea behind this data structure is simple. The VMM records every virtual-to-physical mapping that it sees the OS try to install; then, on a TLB miss, the VMM .rst consults its software TLB to see if it has seen this virtual-to-physical mapping before, and what the VMM’s desired virtual-to-machine mapping should be. If the VMM .nds the translationin its software TLB, it simply installs the virtual-to-machinemapping directly into the hardware TLB, and thus skips all the back andforth in the control .ow above [B+97]. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 

50.5 The Information Gap 
Just like the OS doesn’t know too much about what application programs really want, and thus must often make general policies that hopefully work for all programs, the VMM often doesn’t know too much about what the OS is doing or wanting; this lack of knowledge, sometimes called the information gap between the VMM and the OS, can lead to various inef.ciencies [B+97]. For example, anOS, when it has nothing else to run, will sometimes go into an idle loop just spinning and waiting for the next interrupt to occur: 
while (1)
;// theidle loop 

It makes sense to spin like this if the OS in charge of the entirema­chine and thus knows there is nothing else that needs to run. How­ever, when a VMM is running underneath two different OSes, onein the idle loop and one usefully running user processes, it would be useful for the VMM to know that one OS is idle so it can give more CPU time to the OS doing useful work. 
Another example arises with demand zeroing of pages. Most op­erating systems zero a physical frame before mapping it into apro­cess’s address space. The reason for doing so is simple: security. If the OS gave one process a page that another had been using with­out zeroing it, an information leak across processes could occur, thus potentially leaking sensitive information. Unfortunately, the VMM must zero pages that it gives to each OS, for the same reason, and thus many times a page will be zeroed twice, once by the VMM when assigning it to an OS, and once by the OS when assigning it to a pro­cess. The authors of Disco had no great solution to this problem: they simply changed the OS (IRIX) to not zero pages that it knew had been zeroed by the underlying VMM [B+97]. 
There are many other similar problems to these described here. One solution is for the VMM to use inference (a form of implicit information)to overcome the problem. For example, a VMM can detect the idle loop by noticing that the OS switched to a low-power mode. A different approach, seen in para-virtualized systems, re­quires the OS to be changed a little bit, and perhaps use some inter­faces provided by the VMM to more seamlessly work together. This more explicit approach, while harder to deploy, can be quite effec­tive. 
OPERATING SYSTEMS ARPACI-DUSSEAU ASIDE:PARA-VIRTUALIZATION 
In many situations, it is good to assume that the OS cannot be modi­
.ed in order to work better with virtual machine monitors (forex­
ample, because you are running your VMM under an unfriendly 
competitor’s operating system). However, this is not alwaysthe 
case, and when the OS can be modi.ed (as we saw in the example 
with demand-zeroing of pages), it may run more ef.ciently on top 
of a VMM. Running a modi.ed OS to run on a VMM is generally 
called para-virtualization [WSG02], as the virtualization provided 
by the VMM isn’t a complete one, but rather a partial one requiring 
OS changes to operate effectively. Research shows that a properly-
designed para-virtualized system, with just the right OS changes, can 
be made to be nearly as ef.cient a system without a VMM [BD+03]. 

50.6 Summary 
Virtualization is in a renaissance. For a multitude of reasons, users and administrators want to run multiple OSes on the same machine at the same time. The key is that VMMs generally provide this ser­vice transparently;the OS above has little clue that it is not actu­ally controlling the hardware of the machine. The key method that VMMs use to do so is to extend the notion of limited direct execution; by setting up the hardware to enable the VMM to interpose on key events (such as traps), the VMM can completely control how ma­chine resources are allocated while preserving the illusionthat the OS requires. 
You might have noticed some similarities between what the OS does for processes and what the VMM does for OSes. They both virtualize the hardware after all, and hence do some of the same things. However, there is one key difference: with the OS virtual­ization, a number of new abstractions and nice interfaces arepro­vided; with VMM-level virtualization, the abstraction is identical to the hardware (and thus not very nice). While both the OS and VMM virtualize hardware, they do so by providing completely different interfaces; VMMs, unlike the OS, are not particularly meant to make the hardware easier to use. 
There are many other topics to study if you wish to learn more 
about virtualization. For example, we didn’t even discuss what hap­
pens with I/O, a topic that has its own new and interesting issues 
ARPACI-DUSSEAU 
THREE EASY PIECES (V0.5) 
DESIGN TIP:USE IMPLICIT INFORMATION 
Implicit information can be a powerful tool in layered systems where 
it is hard to change the interfaces between systems, but more infor­
mation about a different layer of the system is needed. For example, 
ablock-based disk device mightlike to know more about how a .le 
system above it is using it; Similarly, an application might want to 
know what pages are currently in the .le-system page cache, but the 
OS provides no API to access this information. In both these cases, re­
searchers have developed powerful inferencing techniques to gather 
the needed information implicitly, without requiring an explicit inter­
face between layers [AD01,S+03]. Such techniques are quite useful 
in a virtual machine monitor, which would like to learn more about 
the OSes running above it without requiring an explicit API between 
the two layers. 
when it comes to virtualized platforms. We also didn’t discuss how virtualization works when running “on the side” with your OS in what is sometimes called a “hosted” con.guration. Read more about both of these topics if you’re interested [SVL01]. Finally, we didn’t discuss what happens when a collection of operating systems run­ning on a VMM uses too much memory. As it turns out, memory overload in virtualized systems can be quite dif.cult to handle, and thus it is the topic of our next chapter. 
OPERATING SYSTEMS ARPACI-DUSSEAU 

References 
[AA06] “A Comparison of Software and Hardware Techniques for x86 Virtualization” Keith Adams and Ole Agesen ASPLOS ’06, San Jose, California 
Aterri.c paper from two VMware engineers about the surprisingly small bene.ts of having hardware support for virtualization. Also an excellent general discussion about virtualization in VMware, including the crazy binary-translation tricks theyhave to playin order to virtualize the dif.cult-to-virtualize x86 platform. 
[AD+01] “Information and Control in Gray-box Systems” Andrea C. Arpaci-Dusseau and Remzi H. Arpaci-Dusseau SOSP ’01, Banff, Canada 
Our own work on how to infer information and even exert controlover the OS from application level, without any change to the OS. The best example therein:determining which .le blocks are cached in the OS using a probabilistic probe-based technique; doing so allows applications to better utilize the cache, by .rst scheduling work that will result in hits. 
[B00] “FreeBSD Developers’ Handbook: 
Chapter 11 x86 Assembly Language Programming” 
http://www.freebsd.org/doc/en/books/developers-handbook/ 

Anice tutorial on system calls and such in the BSD developers handbook. 
[BD+03] “Xen and the Art of Virtualization” Paul Barham, Boris Dragovic, Keir Fraser, Steven Hand, Tim Harris, Alex Ho, Rolf Neugebauer, Ian Pratt, Andrew War.eld SOSP ’03, Bolton Landing, New York 
The paper that shows that with para-virtualized systems, theoverheads of virtualized systems can be made to be incredibly low. So successful was this paper on the Xen virtual machine monitor that it launched a company. 
[B+97] “Disco: Running Commodity Operating Systems on Scalable Multiprocessors” Edouard Bugnion, Scott Devine, Kinshuk Govil, Mendel Rosenblum SOSP ’97 
The paper that reintroduced the systems community to virtualmachine research; well, perhaps this is unfair as Bressoud and Schneider [BS95] also did, but here we began to understand why virtualization was going to come back. What made it even clearer, however, is when this group of excellent researchers started VMware and made some billionsof dollars. 
THREE EASY ARPACI-DUSSEAU 
PIECES (V0.5) 
[BS95] “Hypervisor-based Fault-tolerance” 
Thomas C. Bressoud, Fred B. Schneider 
SOSP ’95 

One the earliest papers to bring back the hypervisor,which is just another term for a virtual machine monitor. In this work, however, such hypervisors areused to improvesystem tolerance of hardware faults, which is perhaps less useful than some of the more practical scenarios discussed in this chapter; however, still quite an intriguing paper in its own right. 
[G74] “Survey of Virtual Machine Research” 
R.P. Goldberg 
IEEE Computer, Volume 7, Number 6 

Aterri.c survey of a lot of old virtual machine research. 
[SVL01] “Virtualizing I/O Devices on VMware Workstation’s Hosted Virtual Machine Monitor” Jeremy Sugerman, Ganesh Venkitachalam and Beng-Hong Lim USENIX ’01, Boston, Massachusetts 
Provides a good overview of how I/O works in VMware using a hosted architecture which exploits many native OS features to avoid reimplementing them within the VMM. 
[V98] VMware corporation. 
Available: http://www.vmware.com/ 

This may be the most useless reference in this book, as you can clearly look this up yourself. Anyhow, the company was founded in 1998 and is a leader in the .eld of virtualization. 
[S+03] “Semantically-Smart Disk Systems” 
Muthian Sivathanu, Vijayan Prabhakaran, Florentina I. Popovici, Timothy E. Denehy, 
Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau 
FAST ’03, San Francisco, California, March 2003 

Our work again, this time showing how a dumb block-based device can infer much about what the .le system above it is doing, such as deleting a .le. The technology used therein enables interesting new functionality within a block device, such assecure delete, ormore reliablestorage. 
[WSG02] “Scale and Performance in the Denali Isolation Kernel” Andrew Whitaker, Marianne Shaw, and Steven D. Gribble OSDI ’02, Boston, Massachusetts 
The paper that introduces the term para-virtualization. Although one can argue that Bugnion et al. [B+97] introduce the idea of para-virtualization in the Disco paper, Whitaker et al. take it further and show how the idea can be more general than what was thought before. 
OPERATING SYSTEMS ARPACI-DUSSEAU 
51 



VMM Memory Management(INCOMPLETE) 
51.1 Introduction 
References 
[B+97] “Disco: Running Commodity Operating Systems on Scalable Multi­processors” 
Edouard Bugnion, Scott Devine, Kinshuk Govil, Mendel Rosenblum. 
SOSP ’97. 

[W02] “Memory Resource Management in VMware ESX Server” 
Carl A. Waldspurger 
OSDI ’02, Boston, Massachusetts 

The paper to read about memory management in VMMs. Also, the subject of the next chapter. 
651 
absolute pathname, 484 abstraction, iv, 121, 425 abstractions, 17 access methods, 506 access path, 515 accessed bit, 197 accounting, 86 ack, 588 acknowledgment, 588 acquired, 328 additive parity, 469 address, 9 address space, 11, 31, 119, 140, 297, 433 address space identi.er, 213 address translation, 138, 143, 147, 642 address translations, 192 address-based ordering, 187 address-space identi.er, 212 address-translation cache, 207 admission control, 270 advice, 89 allocate, 518 allocation structures, 508 AMAT, 256 




